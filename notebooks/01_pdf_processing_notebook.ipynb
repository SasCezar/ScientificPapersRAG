{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "757f0f5a",
   "metadata": {},
   "source": [
    "# PDF Processing: Load, Chunk, and Prepare for Vector DB\n",
    "\n",
    "This notebook demonstrates a step-by-step approach to loading, enriching,\n",
    "chunking, and storing scientific paper PDFs into a vector database.\n",
    "It compares different outputs at each stage interactively with helpful visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33f71e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from IPython.display import display\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3412558",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DOCS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06472b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 288 PDF files in ../data/arxiv/pdfs\n"
     ]
    }
   ],
   "source": [
    "# ## Step 1: Load Multiple PDF Documents\n",
    "\n",
    "pdf_path = Path(\"../data/arxiv/pdfs/\")  # change to your path\n",
    "pdf_files = list(pdf_path.glob(\"*.pdf\"))\n",
    "metadata_path = Path(\"../data/arxiv/metadata/\")  # change to your path\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "print(f\"Found {len(pdf_files)} PDF files in {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f85bbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading PDFs: 100%|██████████| 10/10 [00:21<00:00,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loaded 10 documents ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for pdf_file in tqdm(pdf_files[:MAX_DOCS], desc=\"Loading PDFs\"):\n",
    "    try:\n",
    "        loader = UnstructuredPDFLoader(file_path=pdf_file, mode=\"single\")\n",
    "        doc = loader.load()[0]\n",
    "\n",
    "        metadata_file = pdf_file.with_suffix(\".json\")\n",
    "        if metadata_file.exists():\n",
    "            with open(metadata_file) as f:\n",
    "                metadata = json.load(f)\n",
    "            doc.metadata |= metadata\n",
    "\n",
    "        all_docs.append(doc)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {pdf_file}: {e}\")\n",
    "\n",
    "print(f\"--- Loaded {len(all_docs)} documents ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1af2b131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (762 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Chunk Stats ---\n",
      "Total chunks: 916\n",
      "Avg size: 199.72, Max: 299, Min: 3\n"
     ]
    }
   ],
   "source": [
    "# ## Step 2: Recursive Chunking by Tokens\n",
    "\n",
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer,\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=30,\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "final_chunks = []\n",
    "chunk_by_doc = {}\n",
    "for doc in all_docs:\n",
    "    doc_chunks = []\n",
    "    for i, chunk in enumerate(splitter.split_text(doc.page_content)):\n",
    "        metadata = doc.metadata.copy()\n",
    "        metadata[\"chunk_index\"] = i\n",
    "        doc_chunks.append(Document(page_content=chunk, metadata=metadata))\n",
    "    chunk_by_doc[doc.metadata.get(\"title\", doc.metadata.get(\"source\", \"Document\"))] = (\n",
    "        doc_chunks\n",
    "    )\n",
    "    final_chunks.extend(doc_chunks)\n",
    "\n",
    "chunk_lengths = [\n",
    "    len(tokenizer.encode(chunk.page_content, truncation=False))\n",
    "    for chunk in final_chunks\n",
    "]\n",
    "\n",
    "print(f\"\\n--- Final Chunk Stats ---\")\n",
    "print(f\"Total chunks: {len(final_chunks)}\")\n",
    "print(\n",
    "    f\"Avg size: {np.mean(chunk_lengths):.2f}, Max: {np.max(chunk_lengths)}, Min: {np.min(chunk_lengths)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8d673ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram of Chunk Sizes\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(chunk_lengths, bins=20, color=\"skyblue\")\n",
    "plt.title(\"Token Count Distribution per Chunk\")\n",
    "plt.xlabel(\"Token Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56e0c0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1de58ee07041b18bbbaa74d14f451d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Document:', layout=Layout(width='50%'), options=('../data/arxiv/pdfs/2410…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc_selector = widgets.Dropdown(\n",
    "    options=list(chunk_by_doc.keys()),\n",
    "    description=\"Document:\",\n",
    "    layout=widgets.Layout(width=\"50%\"),\n",
    ")\n",
    "\n",
    "chunk_slider = widgets.IntSlider(min=0, max=1, step=1, description=\"Chunk:\")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "\n",
    "def update_slider(*args):\n",
    "    selected_doc = doc_selector.value\n",
    "    chunk_slider.max = len(chunk_by_doc[selected_doc]) - 1\n",
    "    chunk_slider.value = 0\n",
    "    show_chunk(0)\n",
    "\n",
    "\n",
    "def show_chunk(i):\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        selected_doc = doc_selector.value\n",
    "        chunk = chunk_by_doc[selected_doc][i]\n",
    "        print(chunk.metadata)\n",
    "        print(\"\\n\" + chunk.page_content[:1000])\n",
    "\n",
    "\n",
    "chunk_slider.observe(lambda change: show_chunk(change[\"new\"]), names=\"value\")\n",
    "doc_selector.observe(update_slider, names=\"value\")\n",
    "\n",
    "display(widgets.VBox([doc_selector, chunk_slider, output_area]))\n",
    "update_slider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e1840ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stored in Qdrant Vector DB ---\n",
      "Collection: papers_demo\n"
     ]
    }
   ],
   "source": [
    "# ## Step 3: Store in Qdrant Vector DB\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=model_id)\n",
    "\n",
    "db = Qdrant.from_documents(\n",
    "    documents=final_chunks,\n",
    "    embedding=embedding,\n",
    "    location=\"localhost:6333\",\n",
    "    collection_name=\"papers_demo\",\n",
    "    prefer_grpc=False,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Stored in Qdrant Vector DB ---\")\n",
    "print(f\"Collection: {db.collection_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fcbfda3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '../data/arxiv/pdfs/2410.05915v2.pdf', 'chunk_index': 0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chunks[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee2a9a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA\n",
    "texts = [doc.page_content for doc in final_chunks]\n",
    "embeddings = embedding.embed_documents(texts)\n",
    "pca = PCA(n_components=2)\n",
    "points = pca.fit_transform(embeddings)\n",
    "\n",
    "# Create color group\n",
    "doc_names = [\n",
    "    doc.metadata.get(\"title\", doc.metadata.get(\"source\", \"Doc \" + str(i)))\n",
    "    for i, doc in enumerate(final_chunks)\n",
    "]\n",
    "unique_doc_ids = {name: i for i, name in enumerate(set(doc_names))}\n",
    "colors = [unique_doc_ids[name] for name in doc_names]\n",
    "\n",
    "# DataFrame for plot\n",
    "df = pd.DataFrame({\n",
    "    \"x\": points[:, 0],\n",
    "    \"y\": points[:, 1],\n",
    "    \"document\": doc_names,\n",
    "    \"chunk\": [doc.metadata.get(\"chunk_index\", 0) for doc in final_chunks],\n",
    "    \"preview\": [doc.page_content[:100].replace(\"\\n\", \" \") for doc in final_chunks],\n",
    "    \"color_id\": colors\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01da3d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           0,
           "4 2 0 2  v o N 9  ] L C . s c [  2 v 5 1 9 5 0 . 0 1 4 2 : v i X r a  Give me a hint: Can LLMs take "
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           1,
           "Abstract  While state-of-the-art LLMs have shown poor logical and basic mathematical rea- soning, re"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           2,
           "1  Introduction  The ability to reason and logically solve complex mathematical problems is essentia"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           3,
           "We further examine the robustness of these models to adversarial prompts, misleading hints, and clue"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           4,
           "However, these methods have not been extensively explored in the context of solving more compli- cat"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           5,
           "To test the adversarial robustness of these models to hints, we provided either an adversarial misle"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           6,
           "3 Experiments and Results  3.1 Setup  We evaluate a set of 11 models for our experiments, using open"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           7,
           "Model OutputThe answer is 4Q: When you divide a binary number...Step 1 :...Step 2: ...Q:If P_b × P_b"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           8,
           "...A: ...Q:If P_b × P_b = 31_b, where P and brepresent two distinct digits 0-9 and Pis one less than"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           9,
           "Figure 1: A comparison of various prompting techniques  2  e r o c S e g a r e v A  0.6  0.5  0.4  0"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           10,
           "3.2 Evaluating Hint based prompting  We observe that hinting improves the performance of models, as "
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           11,
           "3.3 Evaluating Adversarial Hinting  We find that giving Adversarial hints drastically reduces the mo"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           12,
           "Model Input  Model OutputThe answer is 128Q:Number of ways to place 2 itemson a 8*8 board such that "
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           13,
           "0.4815 - 0.4761 0.5073  Table 1: Comparison of various prompting techniques with adversarial hinting"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           14,
           "3.4 Model-wise performance  While comparing the performance of different models, we observe that clo"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           15,
           "4 Conclusion  In this work, we have evaluated the mathematical reasoning abilities of various models"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           16,
           "4  5 Limitations and Future Work  Our work mainly focuses on prompting LLMs with problems as textual"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           17,
           "References  [1] Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to sol"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           18,
           "sonability in math problems. arXiv preprint arXiv:2403.19346, 2024.  [3] Tom Brown, Benjamin Mann, N"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           19,
           "[4] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, "
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           20,
           "of context, 2024.  [8] Ankit Satpute, Noah Gießing, André Greiner-Petter, Moritz Schubotz, Olaf Tesc"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           21,
           "5  [11] Hongwei Liu, Zilong Zheng, Yuxuan Qiao, Haodong Duan, Zhiwei Fei, Fengzhe Zhou, Wenwei Zhang"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           22,
           "[15] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li,"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           23,
           "[18] Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye,"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           24,
           "[21] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher D"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           25,
           "[25] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Yu Wu, an"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           26,
           "A.1 Base Prompting techniques  In our baseline approach, we provide the models with the target quest"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           27,
           "Baseline  Q: If f(x) is an even function and g(x) isan odd function ...A: ??  Chain Of ThoughtQ: Whe"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           28,
           "A: ??  Figure 4: Examples of Base Prompting techniques  A.2 Hinting  For our baseline approach, we o"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           29,
           "One-shot Hinting  Hinting  Q: If f(x) is an even function and g(x) isan odd function ...A: ??  Few-s"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           30,
           "We experiment with two different types of adversarial prompting:  Case-I - Random : The model is pro"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           31,
           "One-shot-Random-Hinting: Giving one example question and random hint (Case-I) pair (Difficulty level"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           32,
           "Chain-Of-Thought-Random: Giving one example QnA pair with random (Case-II) step- by-step-solution (D"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           33,
           "Chain-Of-Thought RandomQ: What is the sum of all values of y forwhich the expression (y+6)/(y^2 − 5y"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           34,
           "Q: A sphere is inscribed inside ahemisphere ...A: ??  Q: If f(x) is an even function and g(x) isan o"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           35,
           "A: ??  One-shot Random Hinting  Chain-Of-Thought AdversarialQ1: ...Adv Hint 1: ...Q2: ...Adv Hint 2:"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           36,
           "Figure 6: Examples of Adversarial Prompting techniques  B Dataset  We evaluate on a subset of the MA"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           37,
           "C Experiments  All the prompting techniques were tested on a set of models, chosen in a manner to en"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           38,
           "C.1 Experiments on VLMs  We evaluated the mathematical visual question answering ability of multi-mo"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           39,
           "Model  Prompting method Overall  Gemini 1.5  Baseline Hinting Adversarial Hinting Random Hinting  0."
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           40,
           "Table 3: Comparison of Visual Mathematical Question and Answering abilities of Multimodal models.  2"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           41,
           "10  Hinting  Image:  Hint: Use supplementary angles andisosceles triangle properties.A: ??  Image: A"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           42,
           "Random Hint  Random Hint: Since AC = CX, you canuse the properties of isosceles trianglesto find the"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           43,
           "0.46  0.47  0.48  0.48  0.51  0.51  0.5  0.52  0.49  0.47  0.5  0.47  0.47  0.49  0.46  0.44  0.42  "
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           44,
           "11  0.48  Model Name  Baseline  Qwen2-Math-7B-Instruct Gemma-2-2b-it Meta-Llama-3.1-8B-Instruct Meta"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           45,
           "Adversarial-Hint  Qwen2-Math-7B-Instruct Gemma-2-2B-it Meta-Llama-3.1-8B-Instruct Meta-Llama-3.1-8B "
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           46,
           "Chain of Thought  Qwen2-Math-7B-Instruct Gemma-2-2B-it Meta-Llama-3.1-8B-Instruct Meta-Llama-3.1-8B "
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           47,
           "CoT Random  Qwen2-Math-7B-Instruct Gemma-2-2B-it Meta-Llama-3.1-8B Mistral-7B-Instruct-v0.3 Qwen2-7B"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           48,
           "0.68 0.2 0.58 0.59 0.22 0.48 0.34 0.56 0.58 0.44 0.44  0.85 0.4 0.46 0.41 0.12 0.48 0.46 0.63 0.48 0"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           49,
           "0.66 0.3 0.56 0.49 0.16 0.52 0.52 0.78 0.51 0.56 0.58  0.75 0.36 0.56 0.46 0.32 0.42 0.42 0.52 0.49 "
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           50,
           "0.72 0.26 0.44 0.36 0.18 0.46 0.42 0.90 0.46 0.38 0.70  0.62 0.28 0.52 0.46 0.10 0.44 0.40 0.76 0.34"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           51,
           "0.88 0.38 0.62 0.24 0.20 0.62 0.38 0.68 0.66 0.88 0.96  0.72 0.14 0.50 0.24 0.20 0.44 0.32 0.48 0.38"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           52,
           "0.7314 0.2314 0.5314 0.1971 0.1571 0.4629 0.3771 0.5457 0.4714 0.6371 0.7486  0.88 0.42 0.82 0.26 0."
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           53,
           "0.58 0.18 0.28 0.14 0.10 0.24 0.54 0.28 0.30 0.56 0.68  0.7514 0.3029 0.4743 0.2771 0.2143 0.3000 0."
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           54,
           "0.88 0.40 0.42 0.28 0.60 0.66 0.36 0.74 0.66 0.82 0.92  0.62 0.16 0.10 0.10 0.58 0.30 0.28 0.30 0.42"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           55,
           "0.84 0.32 0.34 0.16 0.50 0.60 0.38 0.58 0.56 0.54 0.52  0.82 0.38 0.32 0.42 0.70 0.65 0.34 0.76 0.66"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           56,
           "Few-shot Hint  Qwen2-Math-7B-Instruct Gemma-2-2b-it Meta-Llama-3.1-8B-Instruct Meta-Llama-3.1-8B Mis"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           57,
           "Few-Shot Adversarial Hint  Qwen2-Math-7B-Instruct.json Gemma-2-2b-it.json Mathstral-7B-v0.1.json Met"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           58,
           "One-Shot  Qwen2-Math-7B-Instruct Mathstral-7B-v0.1 Gemma-2-2b-it Meta-Llama-3.1-8B-Instruct Meta-Lla"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           59,
           "0.72 0.26 0.51 0.32 0.26 0.3 0.5 0.46 0.5 0.84 0.92  0.62 0.14 0.52 0.8 0.16 0.26 0.58 0.48 0.44 0.7"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           60,
           "0.86 0.42 0.72 0.72 0.22 0.2 0.32 0.56 0.65 0.82 0.92  0.76 0.32 0.5 0.48 0.2 0.24 0.3 0.56 0.48 0.6"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           61,
           "0.56 0.48 0.12 0.50 0.60 0.06 0.30 0.42 0.50 0.74 0.80  0.80 0.65 0.54 0.68 0.78 0.14 0.28 0.48 0.42"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           62,
           "0.7 0.14 0.5 0.56 0.06 0.38 0.34 0.34 0.32 0.7 0.64  0.7571 0.2886 0.5429 0.5743 0.16 0.3143 0.4486 "
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           63,
           "0.68 0.20 0.52 0.72 0.24 0.26 0.34 0.40 0.40 0.42 0.56  0.84 0.36 0.52 0.12 0.16 0.28 0.50 0.52 0.64"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           64,
           "0.56 0.08 0.44 0.42 0.36 0.14 0.24 0.52 0.4 0.52 0.68  0.7314 0.28 0.56 0.5657 0.4571 0.1943 0.2457 "
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           65,
           "0.72 0.50 0.18 0.50 0.40 0.12 0.34 0.50 0.36 0.42 0.54  0.74 0.58 0.44 0.58 0.24 0.16 0.51 0.52 0.40"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           66,
           "0.52 0.40 0.14 0.36 0.34 0.14 0.18 0.42 0.32 0.46 0.68  0.73 0.53 0.30 0.48 0.55 0.16 0.21 0.38 0.45"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           67,
           "0.62 0.46 0.20 0.44 0.18 0.12 0.46 0.50 0.44 0.64 0.78  0.66 0.50 0.26 0.58 0.26 0.16 0.48 0.28 0.34"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           68,
           "One-Shot Adversarial Hint  Qwen2-Math-7B-Instruct Mathstral-7B-v0.1 Deepseek-Math-7B-Instruct Gemma-"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           69,
           "0.82 0.64 0.48 0.32 0.51 0.02 0.22 0.46 0.46 0.42 0.56  0.82 0.78 0.60 0.42 0.74 0.12 0.32 0.48 0.60"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           70,
           "0.88 0.68 0.51 0.60 0.06 0.22 0.51 0.44 0.70 0.72 0.98  0.78 0.62 0.24 0.58 0.20 0.26 0.34 0.44 0.36"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           71,
           "E Tables  Table 4: Few-Shot Prompting Examples for All Math Categories used for our experimentation "
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           72,
           "14  Category  Counting and Probability  Geometry  Few shot  Answer: 13  1. Example Problem: \"What is"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           73,
           "(cid:1). You are told that (cid:0)15  6  (cid:1) = 5005 and  1. Example Problem: \"Square ABCD has it"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           74,
           "4. Example Problem: \"Triangle ABC with vertices A(−2,0), B(1,4) and C(−3,2) is reflected over the y-"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           75,
           "fp − w = 10000  If f = 5 and w = 5 + 125i, what is p?\"\"Let a and b be nonzero real numbers such that"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           76,
           "i=0  bi ci  −  2005 (cid:88)  i=0  ai bi  .  Answer: 1/2  1. Example Problem: \"If AAA4 can be expres"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           77,
           "75 + 76 + 77 + 78 + 79 + 80 + 81 + 82  is divided by 16.\" Answer: 4  5. Example Problem: \"When the b"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           78,
           "4. Example Problem: \"What is 2 Answer: 2/15  5 divided by 3?\"  5. Example Problem: \"Twenty gremlins "
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           79,
           "A3 + pA2 + qA + rI = 0,  where I and 0 are the 3 × 3 identity matrix and zero matrix, respectively. "
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           80,
           "Category  COT  Algebra  Example Problem: \"What is the sum of all values of y for which the expressio"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           81,
           "Example Problem: \"What is the coefficient of x8 in the expansion of (x − 1)9?\"  Counting and Probabi"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           82,
           "(cid:19)  x9−k(−1)k  For x8, 9 − k = 8, so k = 1. * Step 5: Calculate the Coefficient: Substitute k "
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           83,
           "× base area × height  For this pyramid, the base area is 82. Thus, the volume formula becomes:  V = "
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           84,
           "= 3  Step 6: Conclusion: Therefore, the height h of the pyramid is 3 .  Example Problem: \"Let a and "
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           85,
           "Step 5: Solve for a  b: Rearrange the equation to solve for a:  2a = −7b  Divide by b:  a b  = −  7 "
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           86,
           "2kbk + 2k−1bk−1 + ··· + 4b2 + 2b1 + b0  Step 2: Find the Remainder When Dividing by 4: To find the r"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           87,
           "3r − 3 · 7 = 4 · 2 − 4 · 2r + 4  Step 2: Calculate the Products: Compute each product separately:  3"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           88,
           "r =  33 11  = 3  Step 7: Conclusion: Therefore, the value of r is 3 .  Example Problem: \"If cosx + c"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           89,
           "Step 4: Calculate sinx+sin2x+sin3x: For x = 2nπ, we have sinx = sin2nπ = 0. Similarly, sin2x = sin4n"
          ],
          [
           "../data/arxiv/pdfs/2410.05915v2.pdf",
           90,
           "Answer: 3  Example Problem: \"Let a and b be nonzero real numbers such that  Intermediate algebra  (2"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           0,
           "4 2 0 2  v o N 2 1  ] I  A . s c [  2 v 8 7 1 0 2 . 0 1 4 2 : v i X r a  LLMs Can Evolve Continually"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           1,
           "Multimodal Large Language Models (MLLMs) have gained significant attention due to their impressive c"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           2,
           ". Additionally, an MoE- based gating module is applied between two types of adapters to further enha"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           3,
           "1  Introduction  With recent advances in artificial intelligence, Large Language Models (LLMs) have "
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           4,
           "∗Corresponding author.  Preprint. Under review.  Modality IncrementalAnswering  IncrementalMLLMs  Qu"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           5,
           "To address this issue, recent approaches [2, 3, 19, 1] extend the architecture and training strategi"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           6,
           "In this paper, we propose PathWeave , a flexible and scalable framework with modal-path switching an"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           7,
           ". Additionally, an MoE-based gating module is implemented between uni-modal and cross-modal adapters"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           8,
           "To evaluate the proposed PathWeave, we establish a challenging benchmark, namely Continual Learning "
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           9,
           "expand on multiple modalities, without the need for joint-modal pretraining.  2  We introduce a nove"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           10,
           "Multimodal Large Language Models. In recent years, researchers have been exploring the potential of "
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           11,
           ". OneLLM [3] explores parameter unification by introducing a unified encoder and projection module f"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           12,
           "Continual Learning in Foundational Models. Continual Learning (CL) has been applied to large foundat"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           13,
           ". This inspires us to eliminate joint-modal pertaining from MLLMs by developing an efficient, scalab"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           14,
           "Transfer learning. In the realm of Natural Language Progressing (NLP), fine-tuning large-scale model"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           15,
           "<latexit sha1_base64=\"uljar/DsQdH5yIV6vl3NtvSabfI=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIqMuqG5cV7APa"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           16,
           "sha1_base64=\"uljar/DsQdH5yIV6vl3NtvSabfI=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIqMuqG5cV7APasWTSTBuaS"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           17,
           "sha1_base64=\"iy6i/P2a/MYANjLfMsAb2Wb1fWM=\">AAAB/HicbVDLSsNA"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           18,
           "DLSsNAFJ3UV62vaJduBovgqiQi6rLqxmUF+4AmlpvppB06eTAzEUKIv+LGhS"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           19,
           "v+LGhSJu/RB3/o2TNgttPTBwOOde7pnjxZxJZVnfRmVldW19o7pZ29re2d0z"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           20,
           "re2d0z9w+6MkoEoR0S8Uj0PZCUs5B2FFOc9mNBIfA47XnTm8LvPVIhWRTeqz"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           21,
           "WRTeqzSmbgDjkPmMgNLS0Kw7E1CZE4CaEODZVZ4/2EOzYTWtGfAysUvSQCXa"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           22,
           "vSQCXaQ/PLGUUkCWioCAcpB7YVKzcDoRjhNK85iaQxkCmM6UDTEAIq3WwWPs"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           23,
           "3WwWPsfHWhlhPxL6hQrP1N8bGQRSpoGnJ4uUctErxP+8QaL8SzdjYZwoGpL5"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           24,
           "woGpL5IT/hWEW4aAKPmKBE8VQTIILprJhMQABRuq+aLsFe/PIy6Z427fPm2d"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           25,
           "7fPm2d1Zo3Vd1lFFh+gInSAbXaAWukVt1EEEpegZvaI348l4Md6Nj/loxSh3"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           26,
           "loxSh36ugPjM8f/9qVAQ==</latexit>ˆA1"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           27,
           "Q  ➕  VideoEncoder"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           28,
           "<latexit sha1_base64=\"SQdEQn930ZHkFugauVlnAk2B92c=\">AAAB9HicbVDLSsNAFL2pr1pfUZduBovgqiSlqMuqG5cV7APa"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           29,
           "sha1_base64=\"SQdEQn930ZHkFugauVlnAk2B92c=\">AAAB9HicbVDLSsNAFL2pr1pfUZduBovgqiSlqMuqG5cV7APaWCbTSTt0M"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           30,
           "FrozenTrainable"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           31,
           "<latexit sha1_base64=\"uljar/DsQdH5yIV6vl3NtvSabfI=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIqMuqG5cV7APa"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           32,
           "sha1_base64=\"uljar/DsQdH5yIV6vl3NtvSabfI=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIqMuqG5cV7APasWTSTBuaS"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           33,
           "sha1_base64=\"iy6i/P2a/MYANjLfMsAb2Wb1fWM=\">AAAB/HicbVDLSsNA"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           34,
           "DLSsNAFJ3UV62vaJduBovgqiQi6rLqxmUF+4AmlpvppB06eTAzEUKIv+LGhS"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           35,
           "v+LGhSJu/RB3/o2TNgttPTBwOOde7pnjxZxJZVnfRmVldW19o7pZ29re2d0z"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           36,
           "re2d0z9w+6MkoEoR0S8Uj0PZCUs5B2FFOc9mNBIfA47XnTm8LvPVIhWRTeqz"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           37,
           "WRTeqzSmbgDjkPmMgNLS0Kw7E1CZE4CaEODZVZ4/2EOzYTWtGfAysUvSQCXa"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           38,
           "vSQCXaQ/PLGUUkCWioCAcpB7YVKzcDoRjhNK85iaQxkCmM6UDTEAIq3WwWPs"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           39,
           "3WwWPsfHWhlhPxL6hQrP1N8bGQRSpoGnJ4uUctErxP+8QaL8SzdjYZwoGpL5"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           40,
           "woGpL5IT/hWEW4aAKPmKBE8VQTIILprJhMQABRuq+aLsFe/PIy6Z427fPm2d"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           41,
           "7fPm2d1Zo3Vd1lFFh+gInSAbXaAWukVt1EEEpegZvaI348l4Md6Nj/loxSh3"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           42,
           "loxSh36ugPjM8f/9qVAQ==</latexit>ˆA1"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           43,
           "<latexit sha1_base64=\"SYNJJzyUlrpEe3ba3t6pLh33md4=\">AAAB9HicbVDLSsNAFL2pr1pfVZduBovgqiRa1GXRhS4r2Ae0"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           44,
           "Video  …  Modality Incremental  AnA  \"  “In the depth image, a group of young men are playing a game"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           45,
           "LLM  <latexit sha1_base64=\"+0YNkND47TiSwBUjpQJAYsUodDo=\">AAAB9HicbVDLSsNAFL2pr1pfUZduBovgqiSlqMuiC11"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           46,
           "\"<latexit sha1_base64=\"kevMbpI8jG1EJPRZms7WiHROBC8=\">AAAB+nicbVDLSsNAFL3xWesr1aWbYBFclUSLuiwK4rKCfUA"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           47,
           "sha1_base64=\"SQdEQn930ZHkFugauVlnAk2B92c=\">AAAB9HicbVDLSsNAFL2pr1pfUZduBovgqiSlqMuqG5cV7APaWCbTSTt0M"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           48,
           "sha1_base64=\"SQdEQn930ZHkFugauVlnAk2B92c=\">AAAB9HicbVDLSsNAFL2pr1pfUZduBovgqiSlqMuqG5cV7APaWCbTSTt0M"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           49,
           "sha1_base64=\"K088E6xBnRnqRDpA9w0OgJaJBaM=\">AAAB+nicbVDLSsNA"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           50,
           "DLSsNAFL2pr1pfqS7dDBbBVUlqUZdFQVxWsA9oY5hMp+3QyYOZiVJiPsWNC0"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           51,
           "PsWNC0Xc+iXu/BsnbRbaemDgcM693DPHiziTyrK+jcLK6tr6RnGztLW9s7tn"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           52,
           "W9s7tnlvfbMowFoS0S8lB0PSwpZwFtKaY47UaCYt/jtONNrjK/80CFZGFwp6"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           53,
           "ZGFwp6YRdXw8CtiQEay05Jrlvo/VmGCeXKduUkvvT12zYlWtGdAysXNSgRxN"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           54,
           "NSgRxN1/zqD0IS+zRQhGMpe7YVKSfBQjHCaVrqx5JGmEzwiPY0DbBPpZPMoq"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           55,
           "pZPMoqfoWCsDNAyFfoFCM/X3RoJ9Kae+pyezoHLRy8T/vF6shhdOwoIoVjQg"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           56,
           "IoVjQg80PDmCMVoqwHNGCCEsWnmmAimM6KyBgLTJRuq6RLsBe/vEzatap9Vq"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           57,
           "tap9Vq3f1iuNy7yOIhzCEZyADefQgBtoQgsIPMIzvMKb8WS8GO/Gx3y0YOQ7"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           58,
           "y0YOQ7B/AHxucPINaT7A==</latexit>F32<latexit"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           59,
           "sha1_base64=\"CVO06vIcHh4cw1HSGSkzuY1hVG4=\">AAAB+nicbVDLSsNAFL3xWesr1aWbYBFclaQUdVkUxGUF+4A2hsl00g6dT"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           60,
           "sha1_base64=\"0hklqRIff465pyiO0VwqXuelTNs=\">AAAB6nicbVDLSgNB"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           61,
           "DLSgNBEOyNrxhfUY9eBoPgKeyKqMegF48RzQOSJcxOZpMh81hnZoWw5BO8eF"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           62,
           "5BO8eFDEq1/kzb9xkuxBEwsaiqpuuruihDNjff/bK6ysrq1vFDdLW9s7u3vl"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           63,
           "s7u3vl/YOmUakmtEEUV7odYUM5k7RhmeW0nWiKRcRpKxrdTP3WE9WGKflgxw"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           64,
           "KflgxwkNBR5IFjOCrZPuH3uiV674VX8GtEyCnFQgR71X/ur2FUkFlZZwbEwn"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           65,
           "ZwbEwn8BMbZlhbRjidlLqpoQkmIzygHUclFtSE2ezUCTpxSh/FSruSFs3U3x"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           66,
           "Fs3U3xMZFsaMReQ6BbZDs+hNxf+8TmrjqzBjMkktlWS+KE45sgpN/0Z9pimx"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           67,
           "Z9pimxfOwIJpq5WxEZYo2JdemUXAjB4svLpHlWDS6q53fnldp1HkcRjuAYTi"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           68,
           "juAYTiGAS6jBLdShAQQG8Ayv8OZx78V79z7mrQUvnzmEP/A+fwBe5I3e</la"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           69,
           "3e</latexit>qm<latexit"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           70,
           "sha1_base64=\"fLPInqOjRNOcWEJSgk/ecdZa558=\">AAAB6nicbVDLSgNB"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           71,
           "DLSgNBEOyNrxhfUY9eBoPgKeyKqMegF71FNA9IljA7mU2GzGOZmRXCkk/w4k"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           72,
           "kk/w4kERr36RN//GSbIHTSxoKKq66e6KEs6M9f1vr7Cyura+UdwsbW3v7O6V"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           73,
           "3v7O6V9w+aRqWa0AZRXOl2hA3lTNKGZZbTdqIpFhGnrWh0M/VbT1QbpuSjHS"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           74,
           "puSjHSc0FHggWcwItk56uOuJXrniV/0Z0DIJclKBHPVe+avbVyQVVFrCsTGd"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           75,
           "rCsTGdwE9smGFtGeF0UuqmhiaYjPCAdhyVWFATZrNTJ+jEKX0UK+1KWjRTf0"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           76,
           "WjRTf09kWBgzFpHrFNgOzaI3Ff/zOqmNr8KMySS1VJL5ojjlyCo0/Rv1mabE"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           77,
           "v1mabE8rEjmGjmbkVkiDUm1qVTciEEiy8vk+ZZNbiont+fV2rXeRxFOIJjOI"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           78,
           "OIJjOIUALqEGt1CHBhAYwDO8wpvHvRfv3fuYtxa8fOYQ/sD7/AEh9I22</la"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           79,
           "22</latexit>Im"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           80,
           "PointEncode  Image  AnA  AnA  “Please describe this depth in detail.”  AnAQ-Former  Self Attention  "
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           81,
           "<latexit"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           82,
           "sha1_base64=\"wL2aphdn8lJ9u6FfD1RO2liB3ac=\">AAAB/HicbVDLSsNA"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           83,
           "DLSsNAFL2pr1pf0S7dBIvgqiSlqMuqG5cV7AOaWCbTSTt0MgkzE6GE+CtuXC"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           84,
           "+CtuXCji1g9x5984abPQ1gMDh3Pu5Z45fsyoVLb9bZTW1jc2t8rblZ3dvf0D"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           85,
           "3dvf0D8/CoK6NEYNLBEYtE30eSMMpJR1HFSD8WBIU+Iz1/epP7vUciJI34vZ"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           86,
           "JI34vZrFxAvRmNOAYqS0NDSr7gSp1A2RmmDE0qsse2gMzZpdt+ewVolTkBoU"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           87,
           "lTkBoUaA/NL3cU4SQkXGGGpBw4dqy8FAlFMSNZxU0kiRGeojEZaMpRSKSXzs"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           88,
           "SKSXzsNn1qlWRlYQCf24subq740UhVLOQl9P5inlspeL/3mDRAWXXkp5nCjC"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           89,
           "p5nCjC8eJQkDBLRVbehDWigmDFZpogLKjOauEJEggr3VdFl+Asf3mVdBt157"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           90,
           "dBt157zevGvWWtdFHWU4hhM4AwcuoAW30IYOYJjBM7zCm/FkvBjvxsditGQU"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           91,
           "ditGQUO1X4A+PzBwFtlQI=</latexit>ˆA2<latexit"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           92,
           "sha1_base64=\"GLmDe3k2yet6tnT7/0pFIpE18RU=\">AAAB9HicbVC7TsMwFL0pr1JeBUYWiwqJqUqgAsYCC2OR6ENqQ+W4TmvVc"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           93,
           "sha1_base64=\"GLmDe3k2yet6tnT7/0pFIpE18RU=\">AAAB9HicbVC7TsMwFL0pr1JeBUYWiwqJqUqgAsYCC2OR6ENqQ+W4TmvVc"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           94,
           "Image Encoder  …  Figure 2: Overall framework of PathWeave. We start from a pretrained vision LLM [2"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           95,
           "3 PathWeave  3.1 Preliminaries  Continual learning can empower large-scale foundation modals to cons"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           96,
           "m =  i ,om i }  M  3.2 Framework Overview  This work presents PathWeave, an efficient and extensible"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           97,
           "4  0 is predefined multimodal alignment on a frozen LLM. It is worth noting that the initial modalit"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           98,
           "M  A  m) are constructed by inserting a set of in-adapters (  m i }  {F  3.3 Adapter-in-Adapter  X-I"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           99,
           "m in m are inserted into different linear m can be expressed as:  Uni-modal Adapters. Given the curr"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           100,
           "Fu and  A  F  F  (1)  Cross-modal Adapters. The uni-modal adapters are effective at preserving the u"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           101,
           "A  l (xm m  l ),  (2)  where ˆ 1] represents the cross-modal adapters for current A i, which is a mo"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           102,
           "F  ∈  −  M  A  MoE-based Gating. Cross-modal adapters rely on in-adapters to effectively leverage hi"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           103,
           "m  {P  G  l = Ql(xm ym  l ) +  m (cid:88)  i=1  i Pi(xm W m l ),  NE i=1 represents the gating weigh"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           104,
           "G  l = Qm ym  l (xm  l ) +  m−1 (cid:88)  i=1  W i ˆ A  i(xm  l ) + W m  A  m(xm  l ).  4 Continual "
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           105,
           "MCL Metrics. We formulate the metrics from two aspects to evaluate the proposed MCL strategy on mult"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           106,
           "F Ni  m,i =  1 Ni  Ni(cid:88)  n=1  max 0≤j<m  (Sn  j,i)  −  Sn  m,i.  In addition, we define the fo"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           107,
           "Nm(cid:88)  n=1  Sn  m,m.  And the performance on learned modalities ˆT n ˆT n i = Sn i,i.  i for th"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           108,
           "Audio T3 ↑ 93.55 67.60 70.00 70.75 83.35  →  Depth F3 ↓ 68.19 10.94 4.27 0.00 0.00  Depth  T4 ↑ 149."
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           109,
           "] 4 4 [  t s e T s p a C o i d u A  ] 4 4 [  A Q s p a C o i d u A  ] 6 3 [  M 3 C C  ] 6 1 [  K 0 5"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           110,
           "104.4 84.9 87.4 86.1 96.5  82.7 50.3 52.6 55.4 70.2  41.7 4.2 3.2 4.9 39.3  108.2 5.3 10.3 10.6 107."
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           111,
           "90.4 8.5 0.6 0.0 0.0  49.5 3.00 0.0 0.0 0.0  - - - -  - - - -  59.63 9.29 3.00 0.00 0.00(-3.00)  Tab"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           112,
           "5 Experiments  5.1  Implementation Details  Our method is built on the LAVIS library’s framework [55"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           113,
           "Transfer Learning on New Modality. As shown in Table 1 and 2, we conduct experiments on existing tra"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           114,
           "Alleviate Forgetting of Previous Knowledge. We also present the average forgetting rate Fm of histor"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           115,
           "29.4 29.1 - 26.2 28.6  62.8 - - - 59.5  Table 3: Comparison with state-of-the-art methods on trainin"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           116,
           "75.75 76.40 79.50 83.35  49.10 49.80 50.25 52.20  68.00 69.50 71.35 73.45  51.05 49.70 52.60 53.70  "
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           117,
           "Comparison with Existing MLLMs. Table 3 shows the comparison between our approach and state-of-the-a"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           118,
           "5.3 Ablation Study  Ablation Study of the In-Adapter and MoE-based gating. We conduct detailed ablat"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           119,
           "Analysis of the Benefit from Previous Modalities. Figure 3 presents the ablation study on the abilit"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           120,
           "s p a C o h t o l C  ] 6 3 [  M 3 C C  ] 6 1 [  K 0 5 A V A L L  ] 7 3 [  2 v U Y N  ] 8 3 [  D B G "
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           121,
           "65.3 67.1 69.5 72.6  36.8 34.1 34.7 36.9  30.2 28.7 31.2 33.5  28.8 27.5 27.0 28.6  90.7 92.5 93.9 9"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           122,
           "ESC50OpenClothoAQAClothoCaps  Based on I-V-A-DBased on I-V(b) The performance of depth(a) The perfor"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           123,
           "5.4 Qualitative Analysis  Figure 4 shows the qualitative results of our method for inference on each"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           124,
           "When I hear this audio, I feel like I am walking along the beach, with the sound of the waves crashi"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           125,
           "A limitation of this paper is that we only explored the extension of five modalities and do not cove"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           126,
           "[2] A. Panagopoulou, L. Xue, N. Yu, J. Li, D. Li, S. Joty, R. Xu, S. Savarese, C. Xiong, and J. C. N"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           127,
           "[5] Z. Li and D. Hoiem, “Learning without forgetting,” IEEE transactions on pattern analysis and mac"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           128,
           "clip model,” arXiv preprint arXiv:2207.09248, 2022.  [8] M. Wortsman, G. Ilharco, J. W. Kim, M. Li, "
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           129,
           "adaptation of large language models,” arXiv preprint arXiv:2106.09685, 2021.  [10] T. Srinivasan, T."
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           130,
           "[13] Z. Qian, X. Wang, X. Duan, P. Qin, Y. Li, and W. Zhu, “Decouple before interact: Multi-modal pr"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           131,
           "systems, vol. 36, 2024.  [17] J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           132,
           "multimodal large language models,” arXiv preprint arXiv:2309.10313, 2023.  [21] J. Li, D. Li, C. Xio"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           133,
           "and ground anything anywhere at any granularity,” arXiv preprint arXiv:2310.07704, 2023.  [25] J. Xu"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           134,
           "[28] Y. Fang, W. Wang, B. Xie, Q. Sun, L. Wu, X. Wang, T. Huang, X. Wang, and Y. Cao, “Eva: Explorin"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           135,
           "(WI’07).  IEEE, 2007, pp. 184–190.  [32] S. Changpinyo, P. Sharma, N. Ding, and R. Soricut, “Concept"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           136,
           "[36] P. Sharma, N. Ding, S. Goodman, and R. Soricut, “Conceptual captions: A cleaned, hypernymed, im"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           137,
           "international conference on Multimedia, 2015, pp. 1015–1018.  [40] S. Lipping, P. Sudarsanam, K. Dro"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           138,
           "customization of text-to-image diffusion with c-lora,” arXiv preprint arXiv:2304.06027, 2023.  [44] "
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           139,
           "[47] L. Zhu, Z. Zhu, C. Zhang, Y. Xu, and X. Kong, “Multimodal sentiment analysis based on fusion me"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           140,
           "[51] Y. Hong, H. Zhen, P. Chen, S. Zheng, Y. Du, Z. Chen, and C. Gan, “3d-llm: Injecting the 3d worl"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           141,
           "understanding and reasoning,” arXiv preprint arXiv:2403.11401, 2024.  [54] D. Li, X. Liu, B. Xing, B"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           142,
           "arXiv preprint arXiv:2209.09019, 2022.  [56] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang,"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           143,
           "[59] H. Diao, B. Wan, Y. Zhang, X. Jia, H. Lu, and L. Chen, “Unipt: Universal parallel tuning for tr"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           144,
           "[63] Z. Zhang, M. Fang, L. Chen, and M.-R. Namazi-Rad, “Citb: A benchmark for continual instruction "
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           145,
           "IEEE, 2024, pp. 1126–1130.  [67] D. Liu, X. Huang, Y. Hou, Z. Wang, Z. Yin, Y. Gong, P. Gao, and W. "
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           146,
           "[70] J. He, Y. Wang, L. Wang, H. Lu, J.-Y. He, J.-P. Lan, B. Luo, and X. Xie, “Multi-modal instructi"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           147,
           "[73] M. De Lange, R. Aljundi, M. Masana, S. Parisot, X. Jia, A. Leonardis, G. Slabaugh, and T. Tuyte"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           148,
           "information processing systems, vol. 30, 2017.  [77] O. Ostapenko, T. Lesort, P. Rodríguez, M. R. Ar"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           149,
           "conference on machine learning. PMLR, 2017, pp. 3987–3995.  [80] J. Kirkpatrick, R. Pascanu, N. Rabi"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           150,
           "[83] S. Hou, X. Pan, C. C. Loy, Z. Wang, and D. Lin, “Learning a unified classifier incrementally vi"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           151,
           "[87] J. Zhu, Z.-Q. Cheng, J.-Y. He, C. Li, B. Luo, H. Lu, Y. Geng, and X. Xie, “Tracking with human-"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           152,
           "acoustic tokenizers,” arXiv preprint arXiv:2212.09058, 2022.  [91] X. Mei, C. Meng, H. Liu, Q. Kong,"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           153,
           "[93] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick,"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           154,
           "[96] D. Schwenk, A. Khandelwal, C. Clark, K. Marino, and R. Mottaghi, “A-okvqa: A benchmark for visu"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           155,
           "[99] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh, “Making the v in vqa matter: Eleva"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           156,
           "LLaVA data includes multiple rounds of dialogue. To align with our training process, we randomly sel"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           157,
           "Cap3D [45], Cap3D-QA  Depth*  0.5M  CC3M [36], LLaVA-50K [16]  Total  23.2M+  All Datasets  Table A6"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           158,
           "A.2 Training & Evaluation Details  Table A7 records the detailed hyper-parameters we used during the"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           159,
           "Modality  Iteration Batch Size (Train/Val)  Learning Rate  Video Audio Depth 3D  15K 65K 35K 65K  16"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           160,
           ".  5 2  8  3 7 . 4  5 2 . 1  5 1 . 0  2 8 . 2  6 7 . 1  3 4 . 1  2 8 . 7  2 7 . 5  9 2 . 5  6 0 . 3 "
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           161,
           ".  1 3 8 2  .  4 8 2  .  4 8 2  .  4 8 2  .  1 0 6 9  .  1 0 6 9  .  1 0 6  .  9  5 2  8  5 2  8  5 "
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           162,
           "2 8 . 7    2 7 . 5  2 7 . 5    9 2 . 5      6 0 . 3      5 8 . 4      4 1 . 2                       "
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           163,
           "P o i n t  4 8 1  .  1 3 7 7  .  1 3 8 2  .  4 8 7  .  1 2 5 6  .  5 5  1  4 0 . 1  1 7 . 7  1 0 . 0"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           164,
           "4 8 1  .  1 3 7  7  1 3 7 7  .  1 3 7 7  .  1 3 8  2  1 3 8 2  .  1 3 8 2  .  4 8  .  7  4 8 7  .  4"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           165,
           "2 5 . 3  2 5 . 3    1 2 . 9  1 2 . 9    2 . 8  2 . 8    1 8 . 6  1 8 . 6    9 . 3  9 . 3    8 6 . 1 "
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           166,
           "L2 Reg + WE  P o i n t  D e p t h  A u d i o  V i d e o  I  m a g e  4 8 1  .  4 7 9  .  4 7 9  .  4"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           167,
           "5  1 3 8 4  .  1 3 8 1  .  1 3 8  2  4 5 0  .  4 2  .  1  4 5 1  .  4 7 1  .    8 2 5  .  8 7 5  .  "
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           168,
           "2 . 8  3 . 3  3 . 4      1 3 . 0  1 2 . 1  4 . 2      1 . 7  1 . 6  2 . 0      0 . 1  0 . 1  0 . 8  "
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           169,
           "4 3 . 4        1 . 8          0 . 5          3 . 2          1 0 . 3          WiSE-FT  P o i n t  D e"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           170,
           "2  1 3 0 8  .  1 3 6 8  .  3 6 5  .  4 3  .  7  4 5 7  .  4 7 1  .  6 9 3  .  7 4 2  .  9 4 2  .  8 "
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           171,
           "2 7 . 1    0 . 4  2 . 8  5 . 2    0 . 1  0 . 3  2 . 1    4 . 3  1 0 . 5  1 4 . 4    0 . 8  1 . 7  1 "
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           172,
           "5 . 3        17  I  m a g e  4 8 1  .  1 3 7  7  1 3 8  2                                        Con"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           173,
           ".  1 9  5  5 9 5  .  1 1 2 8  .  1 3 7  7  3 8 4  .  2 0  .  8  6 1 0  .  1 1 2 1  .  1 3 8  2  1 8 "
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           174,
           ".    8 . 9  1 6 . 2  1 0 . 9  4 3 . 2    3 . 2  7 . 2  6 2 . 4      2 . 6  1 0 . 4  7 4 . 7      8 ."
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           175,
           "3 3 . 2  8 2 . 7        4 4 . 8  6 2 . 1        3 1 . 3  4 4 . 6        6 2 . 5          5 0 . 4    "
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           176,
           "AudioCaps Test [44]  AudioCaps QA [44]  ESC50 Cls [39]  ESC50 Open [39]  ClothoAQA [40]  Clotho Caps"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           177,
           "r e c o r d s  o f  a l l  c o m p a r e d C L m e t h o d s  i n  a l l  m o d a l i t i e s .  A.4"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           178,
           "→  Video F1 ↓ 16.30 1.00 0.40 0.00 0.00  Video T2 ↑ 33.75 5.88 4.45 10.9 42.9  →  Audio F2 ↓ 34.63 0"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           179,
           "] 4 3 [  A Q D V S M  ] 3 3 [  p a C D V S M  ] 9 3 [  s l C 0 5 C S E  ] 9 3 [  n e p O 0 5 C S E  "
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           180,
           "- - - -  50.7 45.7 47.1 48.7 48.2  136.5 94.2 100.4 125.6 106.9  66.2 5.2 2.0 12.9 72.6  18.4 2.1 0."
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           181,
           "36.5 5.1 3.1 0.0 0.0  96.1 33.4 8.7 0.0 0.0  46.9 3.6 0.4 0.0 0.0  7.15 1.9 0.7 0.0 0.0  20.7 7.0 1."
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           182,
           "i of transfer learning capability to adapt the new modality.  We quantitatively compare the results "
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           183,
           "62.8 - - 59.5  46.7 - - 47.9  Method  COCOvalCOCOtest MSRVTT MSRVTTQAAudioCapsvalAudioCapstestAudioC"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           184,
           "18  Please describe this image in detail.  The image shows a blue double-decker bus on a city street"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           185,
           "An audio clip from the forest  The image shows two large teddy bears positioned side by side, with a"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           186,
           "1.  1. 1.  1  10 10  10  80 80  Video  MSVD QA [34]  MSVD Cap [33] MSRVTT [35]  MSRVTT QA [35]  base"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           187,
           "1.  1. 1.  1.  10 10  1  1 10  1  80 80  10  80 80  10  Clotho Caps [41]  1.  10  80  Depth  CC3M [3"
          ],
          [
           "../data/arxiv/pdfs/2410.20178v2.pdf",
           188,
           "Modelnet Cls [42]  Modelnet Open [42]  Cap3D QA [45]  Cap3D Cap [45]  describe the 3d model. based o"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           0,
           "5 2 0 2  n a J  2  ]  V C . s c [  3 v 0 6 1 5 0 . 0 1 4 2 : v i X r a  Manuscript  VLM2VEC: TRAININ"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           1,
           "Embedding models have been crucial in enabling various downstream tasks such as semantic similarity,"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           2,
           ". Unlike previous models such as CLIP or BLIP, which encodes text or images independently without an"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           3,
           "1  INTRODUCTION"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           4,
           "Embeddings, or distributed representations, text or images) as Since the advent of fixed-dimensional"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           5,
           "., 2014), automatic evaluation (Zhang et al., 2020; Sellam et al., 2020), prompt retrieval for in-co"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           6,
           "., 2024; Springer et al., 2024; BehnamGhader et al., 2024) have demonstrated promising results on th"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           7,
           "encode inputs (whether  ∗Work done during an internship at University of Waterloo in collaboration w"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           8,
           "Foods  Instruction: Represent the given news image with the following caption for domain classificat"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           9,
           "Current research in multimodal embeddings faces two primary limitations: (1) existing studies typ- i"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           10,
           "MMEB: We introduce a novel benchmark, MMEB (Massive Multimodal Embedding Benchmark), which includes "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           11,
           "., 2023) and MagicLens (Zhang et al., 2024), which rely on late fusion of CLIP (Radford et al., 2021"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           12,
           "Manuscript"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           13,
           "Following extensive contrastive training, VLM2VEC can handle any combination of images and text, pro"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           14,
           ". Compared to the best baseline model with fine-tuning, our model achieves a 15.7 point improvement "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           15,
           "2 MMEB: A BENCHMARK FOR MULTIMODAL EMBEDDINGS  2.1 DATASET OVERVIEW  We present MMEB (Massive Multim"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           16,
           "The embedding models are supposed to compress the query side into a vector and the target can- didat"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           17,
           "MMEB offers a wide range of tasks from various domains, such as common, news, Wikipedia, web, and fa"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           18,
           "Information Retrieval Both the query and target sides can involve a combination of text, images, and"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           19,
           "✓ ✓ ✓ ✓ ✓  100K 49K 8K 8K 20K - - - - -  1000 1000 1000 1000 1000 1000 1000 1000 1000 1000  1000 24 "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           20,
           "T I + T T I T I I T I + T I + T T T  I I I T I T I I + T I + T I I + T I  ✓ ✓ ✓ ✓  123K 26K 100K 100"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           21,
           "Further details on dataset processing can be found in Section A.1.  3 VLM2VEC: TRANSFORMING LVMS TO "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           22,
           "ChartQA  ImageNet-R  Retrieval  Classification  ObjectNet  Country211  ImageNet-1K  VQA  A-OKVQA  VO"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           23,
           "FashionIQ  MSCOCO  Visual7W-Pointing  HatefulMemes  Figure 2: An overview of the tasks and datasets "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           24,
           "ϕ(hqinst,ht+) +  (ϕ(hqinst,ht−))  t−∈N  where N denotes the set of all negatives, and ϕ(hq,ht) is a "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           25,
           "A bottleneck lies in the GPU memory that limits us from increasing the batch size and the number of "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           26,
           "(cid:88)  ˆQj∈Q  (cid:88)  qi∈ ˆQj  ∂L ∂f(qi)  ∂f(qi) ∂Θ  =  (cid:88)  ˆQj∈Q  (cid:88)  qi∈ ˆQj  ui "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           27,
           "Image EncoderVLM2Vec(Target)  Contrastive Loss  Manuscript  Figure 3: VLM2VEC uses a VLM as the back"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           28,
           "For the 20 training datasets, if a dataset contains more than 50K samples, we randomly select 50K fo"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           29,
           "4.1 BASELINES  Four groups of baselines are reported in this study.  CLIP-family: We utilize vision/"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           30,
           "MagicLens: MagicLens (Zhang et al., 2024) is a self-supervised image retrieval model capable of hand"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           31,
           "6  Manuscript  For all our baselines, we first use their original versions. Additionally, we have fi"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           32,
           "IND  OOD Overall  # of datasets →  10  10  12  4  20  16  36  Baseline Models (No Fine-tuning on MME"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           33,
           "51.8 47.0 59.5 53.3 62.2 65.3 19.0 26.0  37.1 25.3 32.3 39.3 44.7 47.1 14.9 31.0  38.7 25.1 38.0 40."
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           34,
           "42.8 43.1  45.4 47.2  Ours (VLM2VEC)  Phi-3.5-V, FFT (bs=1024) Phi-3.5-V, LoRA (bs=1024) LLaVA-1.6, "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           35,
           "From Table 2, the best variant of VLM2VEC leverages LLaVA-1.6, is trained with LoRA, and pro- cesses"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           36,
           "Compared to other baseline models, with or without fine-tuning on MMEB training data, our model demo"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           37,
           "7  Manuscript  4.3 RESULT ANALYSIS  To train an effective and generalizable multimodal embedding, va"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           38,
           "Model  Meta-Task Average Score  Average Score  Classification VQA Retrieval Grounding  IND OOD Overa"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           39,
           "52.0 58.4 58.2 50.8 53.4  4.3.2 TRAINING PARAMETERS  During our experiments, we identified three key"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           40,
           "600  52  48  50  800  53  5  54Performance (%)  Batch Size Influence on Performance  6000  8000Step "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           41,
           "8  Manuscript  4.3.3 META-TASK GENERALIZATION  We have demonstrated that VLM2VEC has the potential t"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           42,
           "Figure 5 illustrates the generalizability of these three models on unseen meta-tasks. We could ob- s"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           43,
           "40  0  VLM2VecCLS  50  60  20  13.033.013.835.1VLM2VecVQA vs VLM2VecCLS  0  10  14.251.312.635.1VLM2"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           44,
           "4.3.4  IMPACT OF INSTRUCTIONS  Previous studies have shown the influence of instructions on addressi"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           45,
           "5 RELATED WORK  5.1 TEXT EMBEDDING  Text embeddings have demonstrated significant potential in power"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           46,
           "10  10  12  4  20  16  36  CILP  w/o instruction w/ instruction ∆  42.8 17.4 -59.3%  9.1 8.0  53.0 4"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           47,
           "embeddings for specific tasks. With the rise of pretrained language models, efforts have shifted to-"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           48,
           "5.2 MULTIMODAL EMBEDDINGS  Multimodal embeddings have long been a significant research challenge. Ea"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           49,
           "Most research on universal multimodal embeddings involves fine-tuning models like CLIP or BLIP, typi"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           50,
           "For multimodal retrieval, several benchmarks have been introduced to evaluate model performance acro"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           51,
           "Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. SemEval-2012 task 6: A pilot on sema"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           52,
           "Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenba"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           53,
           "Daniel Cer, Mona Diab, Eneko Agirre, I˜nigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1: Sem"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           54,
           "similarity through ranking. Journal of Machine Learning Research, 11(3), 2010.  Mehdi Cherti, Romain"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           55,
           "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi- e"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           56,
           "under memory limited setup. arXiv preprint arXiv:2101.06983, 2021a.  Tianyu Gao, Xingcheng Yao, and "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           57,
           "Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyl"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           58,
           "Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandelwal, Mandar Joshi, Kenton Lee, Kristina Toutanova, an"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           59,
           "Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           60,
           "13  Manuscript  Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik R"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           61,
           "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti,"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           62,
           "Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-nex"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           63,
           "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr In Compu"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           64,
           "search over multimodal web content. arXiv preprint arXiv:2305.13631, 2023.  Zheyuan Liu, Cristian Ro"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           65,
           "retrieval via document screenshot embedding. arXiv preprint arXiv:2406.11251, 2024a.  Xueguang Ma, L"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           66,
           "Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual In Proceedin"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           67,
           "Rui Meng, Ye Liu, Shafiq Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. Sfr-embedding-2: Advance"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           68,
           "bedding benchmark. Association for Computational Linguistics, pp. 2014–2037, 2023.  Tri Nguyen, Mir "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           69,
           "Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svet- lana La"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           70,
           "Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learni"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           71,
           "Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig, and Aditi Raghunathan. Rep-  etit"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           72,
           "Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Ma- jumder, and"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           73,
           "ple visual language model pretraining with weak supervision. Learning Representations, 2022b.  Cong "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           74,
           "Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           75,
           "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluat- ing"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           76,
           "A.1.1 CLASSIFICATION  There are a total of 10 datasets for classification tasks.  ImageNet-1K (Deng "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           77,
           "N24News (Wang et al., 2021) The dataset is sourced from the New York Times and consists of 24 catego"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           78,
           "Country-211 (Radford et al., 2021) The dataset is designed to assess the geolocation capability of v"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           79,
           "InfographicsVQA (Mathew et al., 2022) The dataset comprises a diverse collection of infographics acc"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           80,
           "VizWiz (Gurari et al., 2018) The dataset originates from a natural visual question answering sce- na"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           81,
           "A.1.3 RETRIEVAL  There are a total of 12 datasets for retrieval tasks.  VisDial (Das et al., 2017) T"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           82,
           "VisualNews (Liu et al., 2020) The dataset contains publicly available news image paired with cap- ti"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           83,
           "19  Manuscript  NIGHTS (Fu et al., 2023) The dataset contains human similarity judgments on image pa"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           84,
           "Wiki-SS-NQ (Ma et al., 2024a) The dataset is another retrieval-based VQA dataset. Unlike the origina"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           85,
           "RefCOCO (Kazemzadeh et al., 2014) The dataset includes an object detection task that requires more r"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           86,
           "Visual7W-pointing (Zhu et al., 2016) The dataset establishes a semantic link between textual de- scr"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           87,
           "20  Manuscript  Table 5: We compare the performance of VLM2VEC using different numbers of candidates"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           88,
           "76.6 66.9 60.1 55.9 49.5  21  Manuscript  Table 6: The detailed results of the baselines and our VLM"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           89,
           "45.4 13.9 47.2 64.3 39.6 20.0 42.6 75.0 40.3 14.2 40.3  10.3 36.0 49.6 52.1 34.5 21.5 3.2 39.7 20.6 "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           90,
           "11.5 3.3 5.3 4.6 1.5 2.6 10.2 6.6 52.5 10.9 10.9  2.4 1.5 4.2 2.7 3.0 1.2 7.9 2.3 57.5 1.0 8.4  8.7 "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           91,
           "30.7 12.6 78.9 79.6 59.5 57.7 60.4 67.5 11.4 55.0 41.1 81.0 53.0  25.4 15.4 74.0 78.0 63.6 62.1 66.1"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           92,
           "Visual Grounding (4 tasks) MSCOCO RefCOCO RefCOCO-matching Visual7W-pointing All Visual Grounding  3"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           93,
           "13.3 14.9 11.5  44.7 47.1 41.7  22  74.5 80.3 67.9 91.5 75.8 44.0 43.6 79.8 39.6 14.7 61.2  69.0 54."
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           94,
           "Represent the given image for classification  Italian greyhound    ImageNet-A (Hendrycks et al., 202"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           95,
           "firing range indoor    ObjectNet 2019)  (Barbu et al.,  Identify the object shown in the image.  mug"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           96,
           "Target Text  Target Image  OK-VQA (Marino et al., 2019)  Represent the given image with the followin"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           97,
           "Represent the given image with the following question. Which of these states is farthest north?  Sou"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           98,
           "Query Image  Target Text  Target Image  VisDial (Das et al., 2017)  Represent the given dialogue abo"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           99,
           "Represent the given image.  Retrieval  WebQA (Chang et al., 2022)  Find a Wikipedia image-passage pa"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           100,
           "Represent screenshot.  the given document  VisualNews i2t (Liu et al., 2020)  Find a caption for the"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           101,
           "Find an image to match the fashion image and style note. Is shiny and silver with shorter sleeves an"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           102,
           "a Wikipedia that  pair  Represent the given Wikipedia im- age with related text information. Titisee"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           103,
           "Select the portion of the image that answers the given question. Which door is behind a person sit- "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           104,
           "Model  image retrieval  text retrieval  R@1 R@5 R@10 R@1 R@5 R@10  OpenAI CLIP-B/16 Open CLIP-B/16 E"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           0,
           "4 2 0 2  c e D 1 3  ]  G L . s c [  1 v 7 5 4 0 0 . 1 0 5 2 : v i X r a  Differentiable Prompt Learn"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           1,
           "Prompt learning is an effective way to exploit the potential of large-scale pre-trained foundational"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           2,
           ". The performance on the downstream tasks exhibits the superiority of the automatic design: our meth"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           3,
           "et al., 2020; Lester et al., 2021; Schick and Sch¨utze, 2020b; Shin et al., 2020; Gao et al., 2020] "
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           4,
           "Deep continuous prompts add prompts to multiple lay- ers. This method has shown superior performance"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           5,
           ". This line of work indi- cates that there is a subset of layers in the pre-trained model, depending"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           6,
           "1 Introduction Parameter-Efficient fine-tuning (PEFT) offers a paradigm to adapt pretrained model to"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           7,
           "In summary, our DPL method has the following contribu-  tions:  The DPL method automates the continu"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           8,
           "2 Background and Related Work Prompt Learning Prompt learning offers an efficient way to adapt pre-t"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           9,
           ". PLOT [Chen et al., 2022] uses the computationally costly iterative algorithm to compute the transp"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           10,
           "Neural Architecture Search Neural architecture search (NAS) automatically determines the architectur"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           11,
           "¯o(i,j)(x) =  (cid:88)  o∈O  (cid:80)  exp(α(i,j) ) o′ exp(α(i,j)  o  o′  )  o(x) ,  where α(i,j) ar"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           12,
           "Here we denote l-th transformer block as f(l). Different op- tions for adding continuous prompts can"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           13,
           "(2)  Text/ImageTransformer Block  Text/ImageTransformer Block  Search StageTrain Stagea)b)c) DepthMa"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           14,
           "into transformer blocks f(·). w.l.o.g, for the l-th layer, there are t options for adding continuous"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           15,
           "k [E(l)  v [E(l)  q x, K = wT  ,x(l−1)], V = wT  ,x(l−1)] . (4)  i  i  Cross Attention = Softmax(  Q"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           16,
           "i=1 The Equation 6 indicates that the output of each transformer block is a combination of all possi"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           17,
           "Lval(E∗(α),α) min α s.t. E∗(α) = argmin  Ltrain(E,α).  E  Algorithm 1 shows the searching stage. The"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           18,
           "5: end while 6: for i = 1 to ℓ do 7:  Aα im, k determines the context length of continuous prompts f"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           19,
           "(7)  (8)  (9)  Similar to the differentiable NAS where the supernet in- corporates all candidate ope"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           20,
           "txt for the α matrix of the text branch and Aα  In the image branch, the input x ∈ RC×H×W is patchif"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           21,
           "ˆy =  (cid:80)  exp(sim(˜f, ˜g)/τ) c∈C exp(sim(˜f, ˜gc)/τ)  ,  where τ is the temperature parameter."
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           22,
           "The total loss Ltotal is computed by:  Ltotal = Lde + λLkl ,  (10)  (11)  (12)  (13)  where λ is a h"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           23,
           "where λ is a hypereparameter. The gradient descent of con- tinuous prompts uses Ltotal to update par"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           24,
           "Baselines We compare the proposed method with CoCoOp [Zhou et al., 2022a], PLOT [Chen et al., 2022],"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           25,
           "Experiment Details We use the pretrained ViT-B/16 CLIP model [Radford et al., 2021] in the few-shot "
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           26,
           "4.2 Determining Context Lengths of Continuous  Prompts  We use the few-shot learning setting in the "
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           27,
           "ij ≫ Aα  a)b)c)  Figure 2: (a) α matrices for the text branch. (b) α matrices for the image branch. "
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           28,
           "i j ik = max m  where Aα  (Aα  im) .  ij) − Softmax(Aα  s.t. ϵ > 0, ϵ ≪ |Aα  im|, ϵ ≪ |Aα  in|, |Aα "
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           29,
           "Apparently, single-dominant α matrix has a high α difference value. Figure 2 (c) shows the variation"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           30,
           "Method  ZS CLIP Linear Probe CoCoOp PLOT ProGrad MaPLe DPL DPL + KD  Caltech101 DTD EuroSAT Aircraft"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           31,
           "Cars 55.63 70.13 72.30 68.10 81.23 74.87 82.70 82.83  UCF 61.45 73.70 76.97 72.23 81.60 80.37 84.10 "
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           32,
           "The ideal converged α matrix has the number of dominants # Dominants = ℓ(t − 1).  4.3 Prompt Learnin"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           33,
           "CoCoOpPLOTProGradMaPLeDPL  Figure 3: Original image and Grad-CAM visualization [Selvaraju et al., 20"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           34,
           "Our proposed method shows a pronounced advantage com- It boosts the average accuracy pared to baseli"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           35,
           "We visualize the attention map on the downstream datasets using different prompt learning methods as"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           36,
           "respectively. The equivalent prompt depth is 1. The way of adding context prompts is illustrated in "
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           37,
           "ImageNet Average  1.73  3.40  5 Discussion  By using bi-level optimization, the DPL method is able t"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           38,
           "Existing deep prompt learning works hardly consider a granular level of adding continuous prompts: e"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           39,
           "If some layer parameters are close to optimal in down- stream datasets, we believe there is no need "
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           40,
           "6 Limitation  The searching stage of the DPL method, similar to NAS, is computationally costly due t"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           41,
           "7 Conclusion  In this work, we automate the prompt learning design by re- laxing the categorical sel"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           42,
           "[Bar et al., 2022] Amir Bar, Yossi Gandelsman, Trevor Dar- rell, Amir Globerson, and Alexei Efros. V"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           43,
           "[Boschini et al., 2022] Matteo Boschini, Lorenzo Bonicelli, Angelo Porrello, Giovanni Bellitto, Matt"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           44,
           "[Chen et al., 2019] Xinyang Chen, Sinan Wang, Bo Fu, Mingsheng Long, and Jianmin Wang. Catastrophic "
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           45,
           "Chu,  [Cimpoi et al., 2014] Mircea Cimpoi, Subhransu Maji, Ia- sonas Kokkinos, Sammy Mohamed, and An"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           46,
           "[Dong and Yang, 2019] Xuanyi Dong and Yi Yang. Search- ing for a robust neural architecture in four "
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           47,
           "[Gao et al., 2020] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           48,
           "[Hendrycks and Dietterich, 2019] Dan  and Thomas Dietterich. Benchmarking neural network ro- bustnes"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           49,
           "[Jia et al., 2022] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath H"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           50,
           "[Koh et al., 2021] Pang Wei Koh, Shiori Sagawa, Hen- rik Marklund, Sang Michael Xie, Marvin Zhang, A"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           51,
           "Constant. prompt tuning. arXiv preprint arXiv:2104.08691, 2021. [Li and Liang, 2021] Xiang Lisa Li a"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           52,
           "[Liu et al., 2023] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie "
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           53,
           "[Mukherjee and Awadallah, 2019] Subhabrata Mukherjee and Ahmed Hassan Awadallah. Distilling bert int"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           54,
           "[Parkhi et al., 2012] Omkar M Parkhi, Andrea Vedaldi, An- drew Zisserman, and CV Jawahar. Cats and d"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           55,
           "[Recht et al., 2019] Benjamin Recht, Rebecca Roelofs, Lud- wig Schmidt, and Vaishaal Shankar. Do ima"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           56,
           "[Saxena and Verbeek, 2016] Shreyas Saxena and Jakob Ver- beek. Convolutional neural fabrics. Advance"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           57,
           "Schick  [Selvaraju et al., 2017] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishn"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           58,
           "[Shtedritski et al., 2023] Aleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi. What doe"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           59,
           "perception. Advances in Neural Information Processing Systems, 36, 2024.  [Vaswani et al., 2017] Ash"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           60,
           "[Wang et al., 2022b] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolo"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           61,
           "[Xu et al., 2019] Yuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Guo-Jun Qi, Qi Tian, and Hongkai X"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           62,
           "Chen Huang, and Chen Change Loy. sion and language prompt arXiv:2210.07225, 2022.  learning.  [Zela "
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           63,
           "Zhou,  [Zhu et al., 2023a] Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Hanwang Zhang. Prompt-alig"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           64,
           "We note that the α matrix at the last epoch varies for the different datasets. It indicates the data"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           65,
           "Supprompt  Subprompt  Confidence  adding continuous prompts. For an α matrix Aα ∈ Rℓ×t. The search s"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           66,
           "in| > T, where T  A.2 Terminology We summarize the terminology in Table 3. Supprompt is analogous to"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           67,
           "tion. We use Confidence to indicate the quality of the search- ing stage. When the confidence level "
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           68,
           "The number of parameters in the training stage is in the range (0,0.12] M ((0,0.096]% of the origina"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           69,
           "Avg Test Acc  75.37 79.14 75.50 78.47  81.71  A.4 Performance on Downstream Tasks The standard devia"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           70,
           "Caltech101 87.20 90.43 ± 0.21 95.10 ± 0.08 93.70 ± 0.10 95.63 ± 0.39 95.10 ± 0.16 95.80 ± 0.24 95.73"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           71,
           "DTD 42.34 64.03 ± 0.82 63.63 ± 0.88 70.90 ± 0.54 66.27 ± 0.73 67.27 ± 0.61 71.50 ± 0.97 70.90 ± 0.24"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           72,
           "EuroSAT 37.57 82.70 ± 1.06 74.10 ± 0.57 84.03 ± 0.59 82.03 ± 1.52 86.40 ± 1.47 92.30 ± 0.18 92.47 ± "
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           73,
           "shots is 16. Search space per depth is {0,2,4,6}. We do not observe a large variation of the perform"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           74,
           "After determining the subprompt, we examine the perfor- mance in the few-shot learning setup (16/8/4"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           75,
           "0.0350# Parameters (M)  OxfordPets  FGVCAircraft  0.0275  Figure 5: The computational complexity of "
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           76,
           "60  60  2 Shots (Text)  20  40  FGVCAircraft  40  8 Shots (Text)  40  4 Shots (Text)  10  UCF101  16"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           77,
           "85Test accuracy  4  1  92.5  70  16Shots per class  75  16Shots per class  4  1  80Test accuracy  PL"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           78,
           "2  60  90Test accuracy  8  4  8  DTD  80  4  2  16Shots per class  8  4  2  Cars  2  MaPLe  4  2  Fo"
          ],
          [
           "../data/arxiv/pdfs/2501.00457v1.pdf",
           79,
           "4  4  16Shots per class  8  2  50  16Shots per class  70Test accuracy  1  8  1  ProGrad  2  40Test a"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           0,
           "5 2 0 2  n a J  4 2  ]  G L . s c [  1 v 8 7 3 6 1 . 1 0 5 2 : v i X r a  Internal Activation Revisi"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           1,
           "Warning: This paper contains offensive content that may disturb some readers. Vision-language models"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           2,
           ".94%, 34.34%, 43.92%, and 52.98% on SafeBench, Safe- Unsafe, Unsafe, and MM-SafetyBench, respectivel"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           3,
           "Introduction Large language models (LLMs) have been further enhanced by adopting visual instruction "
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           4,
           "ical standards (Ouyang et al. 2022; Rafailov et al. 2023; Ji et al. 2023). The earlier work, AdaShie"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           5,
           "To investigate this area, we examine the vulnerabilities of VLMs by analyzing the differences in int"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           6,
           "Figure 1: Computation flow at the transformer layer l, with head-level revision after head attention"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           7,
           "We propose the internal activation revision approach to enhance the safety of VLMs. This method outp"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           8,
           "internal states for a deeper understanding of its behav- ior (Zhu et al. 2021, 2024; Song et al. 202"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           9,
           ". 2023). Activation addition (Turner et al. 2023) involves adding a fixed vector to the activations "
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           10,
           "Why VLMs Are More Vulnerable?  Preliminary Before explaining the methodology, we briefly introduce t"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           11,
           "process continues through all subsequent layers, forming a sequence of vectors x0,...,xn. Finally, t"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           12,
           "Dataset To our best knowledge, existing unimodal and multimodal instruction datasets containing both"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           13,
           ". 2024) consists of 250 positive instructions across ten categories and 200 negative instruc- tions "
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           14,
           "(1)  Figure 2: 2D t-SNE visualization of internal activations from the 5th, 15th, and 31st layers. T"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           15,
           "Activation Visualization with t-SNE Figure 2 illustrates the distribution of activations of the last"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           16,
           "Figure 5: An example of Multi-Response.  our classification dataset denoted as {((x′)l,h,y)i}M i=1. "
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           17,
           "We randomly divide the dataset TextSetA into training and validation sets with a 4:1 ratio. A binary"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           18,
           ". The accuracy differ- ence indicates that the safety alignments of LLMs embedded within VLMs are no"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           19,
           "Activation Revision for Safety Enhancement Based on the above observations, we propose an activation"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           20,
           "l  In head-level revision, we only intervene on some of the heads in one layer l so as to be minimal"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           21,
           "Revision Vectors Extraction As illustrated in Equations (3) and (4), we need to calculate the revisi"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           22,
           "i=1,  i=1.  Similarly, we collect head activations at the last token to col- lect a dataset {((x′)l,"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           23,
           "MM-Safety ASR (%) ↓  ScienceQA ACC (%) ↑  GQA ACC (%) ↑  B 7 - 5 . 1 V A V a L L    Vanilla Adashiel"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           24,
           "76.54 35.93 30.14 35.14 40.16 38.08 35.76 36.10 32.33 30.49 25.53 23.10  70.78 65.31 63.75 61.28 59."
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           25,
           "61.54 36.09 32.87 36.11 36.01 34.41 31.91 30.46 29.74 28.97 26.10 25.41  72.83 38.32 35.74 30.56 45."
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           26,
           "t c u r t s n I - B 7 - L V - 2 n e w Q  Vanilla Adashield MLLM-Protector Fine-tuning Text-Response "
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           27,
           "73.16 33.75 30.60 31.97 38.14 35.26 31.87 29.38 27.11 26.09 24.27 24.44  71.21 67.01 64.29 62.42 58."
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           28,
           "Construction of contrastive samples Our method relies on a small set of positive and negative sample"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           29,
           "Evaluation We evaluate our proposed method from two perspectives. For safety, we measure the attack "
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           30,
           "ASRvanilla i  − ASRrevised  i  (cid:17)  (cid:125)  + λ  1 ∥Dsafety∥  (cid:88)  j∈{Dsafety}  (cid:16"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           31,
           "Analysis and Discussion  Defense Effectiveness Table 2 presents a comprehensive comparison of our fr"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           32,
           "8.590 15.005 19.505 17.930 15.083 6.408 1.570  11.800 25.628 20.200 18.995 5.570 -1.083 -11.280  12."
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           33,
           "(b) Head-Level Revision  Table 3: Composite scores of layer-level and head-level re- vision. l and α"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           34,
           "(1) The head-level activation revision method using Multi-Response with MMS achieves the best perfor"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           35,
           "(2) Head-level revision achieves a better balance between safety and helpfulness than layer-level re"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           36,
           "(4) MMS outperforms PWD. When using Multi- Response, MMS outperforms PWD in both head-level and laye"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           37,
           "(1) With the same α, deeper layers lead to a greater im- pact across various tasks. ASR on SafeBench"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           38,
           "(3) Determination of optimal parameters. From Fig- ure 6, it is evident that revising activations at"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           39,
           "23.013 20.192 25.934  20.151 22.633 28.120  Table 4: Composite Score on MiniGPT-V2 and InternVL2- 8B"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           40,
           "(4) Transferability. We apply the head-level revision vectors that we extract from LLaVA-V1.5-7B to "
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           41,
           "Conclusion We dive into the research question of why VLMs are more vulnerable by analyzing from the "
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           42,
           "Acknowledgments We thank the anonymous reviewers for their valuable com- ments and constructive feed"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           43,
           "References Azaria, A.; and Mitchell, T. 2023. The Internal State of an LLM Knows When It’s Lying. In"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           44,
           ". Chen, J.; Zhu, D.; Shen, X.; Li, X.; Liu, Z.; Zhang, P.; Kr- ishnamoorthi, R.; Chandra, V.; Xiong,"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           45,
           ". ternvl: Scaling up vision foundation models and aligning In Proceedings of the for generic visual-"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           46,
           ". Computer Vision Foundation / IEEE. Ji, J.; Qiu, T.; Chen, B.; Zhang, B.; Lou, H.; Wang, K.; Duan, "
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           47,
           "Kim, B.; Wattenberg, M.; Gilmer, J.; Cai, C. J.; Wexler, J.; Vi´egas, F. B.; and Sayres, R. 2018. In"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           48,
           ". In Thirty-seventh Confer- swers from a Language Model. ence on Neural Information Processing Syste"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           49,
           ". Liu, H.; Li, C.; Wu, Q.; and Lee, Y. J. 2023a. Visual In- struction Tuning. In Oh, A.; Naumann, T."
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           50,
           ". Springer. Liu, X.; Zhu, Y.; Lan, Y.; Yang, C.; and Qiao, Y. 2023b. Query-relevant images jailbreak"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           51,
           ".; Fraser, K.; and Kiritchenko, S. 2022. Improv- ing Generalizability in Implicitly Abusive Language"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           52,
           ". F.; Leike, J.; and Lowe, R. 2022. Training language models to follow in- structions with human fee"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           53,
           "break Attacks. In Dinkar, T.; Attanasio, G.; Curry, A. C.; Konstas, I.; Hovy, D.; and Rieser, V., ed"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           54,
           ".; and Chen, Y.-N., eds., Proceed- ings of the 2024 Conference on Empirical Methods in Nat- ural Lan"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           55,
           ". Red-teaming language models via activa- tion engineering. https://github.com/nrimsky/LM-exp/. Rims"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           56,
           ".; Bianchi, F.; and Hovy, D. 2024. XSTest: A Test Suite for Identifying Exaggerated Safety Behaviour"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           57,
           ".; Zhang, T.; Dubois, Y.; Li, X.; Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stanford Alpac"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           58,
           ". 2023. Activation addition: Steer- ing language models without optimization. ArXiv preprint, abs/23"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           59,
           "V. N.; and Garnett, R., eds., Advances in Neural Informa- tion Processing Systems 30: Annual Confere"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           60,
           ". Zhang, Y.; Chen, L.; Zheng, G.; Gao, Y.; Zheng, R.; Fu, J.; Yin, Z.; Jin, S.; Qiao, Y.; Huang, X.;"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           61,
           ".; Gomez, H.; and Bethard, S., eds., Findings of the Association for Computational Linguistics: NAAC"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           62,
           ". ArXiv preprint, abs/2402.02207."
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           63,
           "Appendix  Visulization of layer activations We use t-SNE to visualize the extracted layer activation"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           64,
           "Figure 7: 2D layer activations using t-SNE. The blue and red dots represent positive and negative sa"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           65,
           "The impact of head utilization ratio on model perfor- mance. Based on the observations in Figure 8, "
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           66,
           "Figure 10: Detailed information of ASR on Safe-Unsafe and Unsafe, and accuracy on GQA.  Figure 11: D"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           0,
           "4 2 0 2  t c O 2  ]  V C . s c [  1 v 2 1 9 1 0 . 0 1 4 2 : v i X r a  DnD-Transformer  A SPARK OF V"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           1,
           "😠？√  😊  Figure 1: Generations from DnD-Transformers trained on class-conditional ImageNet256×256 (a."
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           2,
           "1  DnD-Transformer  ABSTRACT  This work tackles the information loss bottleneck of vector-quantizati"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           3,
           "1  INTRODUCTION  The field of autoregressive (AR) image generation is experiencing a resurgence of i"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           4,
           "A review of the development history of AR image generation approaches reveals significant efforts fo"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           5,
           "However, despite advancements in AR image generation, VQ-based autoregressive methods face two persi"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           6,
           "a downsampling factor (f) of 8 and 4 channels, with fp32 tensor precision (log N = 4 × 32).  2  DnD-"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           7,
           "1  O  Final Transformer Block  Transformer Block O-2  +  2  O  1  …  DnD-Transformer (This Work)  28"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           8,
           "We draw inspiration from the Residual Quantization method (Lee et al., 2022b), which provides a new "
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           9,
           "The remaining problem is how to predict the d times more tokens effectively. We propose the DnD- Tra"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           10,
           "4. A spark of vision-language intelligence for the first time, enabling unconditional rich- text ima"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           11,
           "=  logN 24f2  A typical configuration (N=8192, f=16) results in 0.21% ICR. This ICR is significantly"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           12,
           "2.2  IMAGES’ 2D DECOMPOSITION AND QUANTIZATION  As pointed out by Equation 1, the information compre"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           13,
           "r0 = v  Consequently, the sum of the residual codes (cid:80)d i=1 qi is expected to approximate more"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           14,
           "ICR(N,f,d) = d ×  (H/f) × (W/f) × logN H × W × 3 × log256  = d ×  logN 24f2  4  (1)  (2)  (3)  DnD-T"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           15,
           "Depth  ImageNet 256×256  Depth  Text256 Text512 arXiv512  rFID↓ L2 Loss↓ Code Usage↑  rOCR↑  1 2 4 8"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           16,
           "(b) Reconstruction OCR Performance. † indicates zero-shot tokenizer trained on ImageNet.  Table 1: A"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           17,
           "2.3 RECONSTRUCTION PERFORMANCE  We evaluate the reconstruction performance of our trained visual tok"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           18,
           "5  DnD-Transformer  Max Depth (d)  1  0.6  2  0.2  0.4  8  12345678Codebook Depth  0.0  Layerwise Co"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           19,
           "125  0.25  (a) Layerwise code usage of visual tokenizers.  (b) Code Norm Distribution for Tokeniers "
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           20,
           "rOCR - A New Metric. We proposes rOCR, a novel metric for evaluating rich-text image re- constructio"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           21,
           "3 THE DND-TRANSFORMER  Prior section showed DnD visual tokenizers effectively reconstruct fine detai"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           22,
           "+  …………  Transformer Block O-1  Final Transformer Block  28  OOutput HeadAdd  26  28-(N-1)  2  +  28"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           23,
           "3.2  IMPLEMENTATION DETAILS  As shown in the left part of Figure 5, the increment of DnD-Transformer"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           24,
           "4 EXPERIMENTS AND FINDINGS  4.1 TASKS AND DATASETS  Class-Conditional Image Generation. We conduct s"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           25,
           "1. Pure Text Images (Text-Image). The dataset is automatically rendered from a portion of English wi"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           26,
           "Figure 6: Data examples in of the collected Text-Image and arXiv-Image image datasets.  Model, Qwen2"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           27,
           "DnD-Transformer. We train two size of DnD-Transformers across our experiment, namely DnD- Transforme"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           28,
           "Implemented Baselines for Rich-Text Image Generation. We select multiple diffusion models as the bas"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           29,
           "4.3 RESULTS OF CLASS-CONDITIONAL IMAGE GENERATION  As demonstrated in Table 2, our DnD-Transformer s"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           30,
           "FID↓  IS↑  Precision↑ Recall↑  Diffusion-Reported  ADM (Dhariwal & Nichol, 2021) CDM (Ho et al., 202"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           31,
           "1.4B 3.8B 1.4B 1.4B 3.1B 3.1B  5.20 7.55 3.64 2.52 4.21 2.81  280.3 134.0 296.5 295.4 325.2 311.6  −"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           32,
           "1.4B 1.4B 1.4B 1.4B 1.4B 1.4B 2.5B 2.5B 2.5B 2.5B  7.67 4.12 6.55 2.58 2.78 2.96 6.48 2.77 2.21 2.52"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           33,
           "DnD-Transformer-XXLComparison of FIDs along Training on ImageNet  24681078910  LlamaGen-XXL  FIDEpoc"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           34,
           "Figure 7: Curves during training.  4.4 RESULTS OF RICH-TEXT IMAGE GENERATION  Generation Results on "
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           35,
           "Generation Results on arXiv-Image. An 8-layer visual tokenizer and corresponding DnD- Transformer tr"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           36,
           "(a) Training Loss for DnD-Transformer trained with different number of prediction heads.  (b) Traini"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           37,
           "4.6 AR TRAINING LOSS FOR DIFFERENT DOMAINS ALIGN WITH INNER RANDOMNESS  Training loss for the same D"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           38,
           "Image Generation with VQVAE. The vector quantization (VQ) method has been pivotal in the development"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           39,
           "., 2024; Sun et al., 2024). Recent research has also focused on developing multimodal foundation mod"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           40,
           "Rich-Text Image Generation. Despite recent significant progress in image generation, the task of ric"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           41,
           ". (2023a;b); Balaji et al. (2023); Saharia et al. (2022a) in image generation, often limited to gene"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           42,
           "6 CONCLUSION  This paper investigated the limitations of autoregressive (AR) image generation method"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           43,
           "12  DnD-Transformer  REFERENCES  Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuill"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           44,
           "James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuan"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           45,
           "Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin "
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           46,
           "in neural information processing systems, 34:8780–8794, 2021.  Patrick Esser, Robin Rombach, and Bjo"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           47,
           "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in  ne"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           48,
           "Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jos´e Lezama, Jonathan Huang, Grant Schindler, Rachel Hor- nung,"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           49,
           "Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive im- age generation"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           50,
           "language, and multi-modal tasks.  Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khos"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           51,
           "Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantiza-  tio"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           52,
           "Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language under-  st"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           53,
           "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kam- yar Seye"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           54,
           "Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregres"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           55,
           "Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, and Xuansong Xie. Anytext: Multi- lingual visu"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           56,
           "Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           57,
           "Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang- Chieh Chen. "
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           58,
           "Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexand"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           59,
           "A PRELIMINARY: AUTOREGRESSIVE IMAGE GENERATION  In this section, we introduce the fundamentals of au"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           60,
           "A.1 STEP1: TRAIN THE VISUAL TOKENIZER AND TOKENIZE THE IMAGES  Images initially exist in the pixel-l"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           61,
           "A.2 STEP2: LEARN THE PRIOR DISTRIBUTION OF IMAGE TOKENS  Having tokenized the source images into dis"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           62,
           "During training, the training objective is the same as GPT’s next token prediction task (Radford et "
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           63,
           "Classifier-Free Guidance As a technique to enhance the visual quality and text-image alignment, clas"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           64,
           "(4)  DnD-Transformer  B TRAINING DETAILS OF VISUAL TOKENIZERS  We follow (Lee et al., 2022b) to trai"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           65,
           "IS  Precison Recall  1D 2D Parallel 2D Vertical DnD-Transformer  1.4B 1.4B 1.4B 1.4B  4.12 6.32 3.18"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           66,
           "All transformer models were trained using settings similar to LlamaGen (Sun et al., 2024): a base le"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           67,
           "24 32  Table 4: Model sizes and architecture configurations  F GENERATION RESULTS OF DND-TRANSFORMER"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           68,
           "DnD-Transformer  DDPMSD3DnD-Transformer  SDXL  Figure 15: Comparison of Unconditional Rich-Text Imag"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           69,
           "24  DnD-Transformer  Figure 20: Unconditional Generation examples of DnD-Transformer on arXiv data w"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           0,
           "4 2 0 2  t c O 8 2  ]  G L . s c [  1 v 0 8 4 1 2 . 0 1 4 2 : v i X r a  Preprint. Under review.  AI"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           1,
           "bhr54@cornell.edu, anmol@cs.cornell.edu, felipe.pacheco@cornell.edu {leg86,jy6,amf272}@cornell.edu, "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           2,
           "Trust and interpretability are crucial for the use of Artificial Intelligence (AI) in scientific res"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           3,
           ". Each inference produces both a prediction and a natural language transcript detailing the reasonin"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           4,
           "1  Preprint. Under review.  1  INTRODUCTION  Until recently, meaningful interactions with AI models "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           5,
           "However, while this transformation has enabled AI to serve as a general-purpose assistant across a w"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           6,
           "Fortunately, the large context windows of LMMs allow for flexible specialization via in-context lear"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           7,
           "To address these challenges, we introduce AISciVision, a framework that adapts general-purpose LMMs "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           8,
           "We evaluate our method on three real-world scientific image classification datasets: detecting aquac"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           9,
           "🤖Tooling InterfaceZoomOutTool : PanRightTool :  SupervisedModelTool :  LMM: I am now conﬁdent in my "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           10,
           "LMM: Comparing the input to the positives, I see similar features like … but also differences like …"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           11,
           "ence transcripts in a ‘Chat-GPT’ style and can ask clarifying questions and provide corrections/feed"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           12,
           "3. We have deployed AISciVision as a web application that ecologists and scientists use to classify "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           13,
           "Multimodal models in low-labeled data regimes Large Multimodal Models (LMMs) like CLIP (Rad- ford et"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           14,
           ". (2022); Moor et al. (2023); Wang et al. (2023) demonstrate that these capabilities are key for adv"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           15,
           ". AISciVision allows an LMM to predict after multiple rounds of tool use, thus going beyond classica"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           16,
           "Retrieval-Augmented Generation (RAG) Since Large Language Models (LLMs) suffer from halluci- nations"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           17,
           "Interactive AI Agents and tool-use In recent years, Large Language Models have enabled users to en- "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           18,
           ". When environments do not return natural language feedback, e.g. when the feedback is scalar or bin"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           19,
           "For scientific research in physical, life, and climate sciences, not only is it important to obtain "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           20,
           "3.1 RETRIEVING RELEVANT IMAGES WITH VISUAL RAG (VISRAG)  To specialize a general-purpose LMM for sci"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           21,
           "On inference, we embed an input test image xtest to get embedding etest = ϕ(xtest). We then retrieve"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           22,
           "etest · ei ∥etest∥∥ei∥  We then provide the images x+ sim to the LMM in its prompt. This enables the"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           23,
           "Generally, we define a tool T as a function on images X, with outputs as images X or a real-valued s"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           24,
           "For each image classification task, we define a set of tools T = {T1,...,TK} and provide their descr"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           25,
           "We design the initial system prompt to reflect the specified domain (see Appendix C for example tran"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           26,
           "4 EXPERIMENTS  We extensively evaluate our AISciVision framework on three image datasets from scient"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           27,
           "2. Eelgrass Wasting Disease Detection. Eelgrass (Zostera marina), essential for coastal ecosystems, "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           28,
           "7  Preprint. Under review.  AISciVision method We use the GPT-4o as the framework’s LMM, and attach "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           29,
           "F1  AUC Acc  k-NN CLIP-ZeroShot CLIP+MLP AISciVision  0.84 0.82 0.85 0.90  0.65 0.00 0.67 0.78  0.83"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           30,
           "0.91 0.65 1.00 1.00  0.95 0.63 0.99 0.97  Table 1: Our AISciVision framework (GPT4o + VisRAG + Tools"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           31,
           "Baselines A key component of AISciVision is VisRAG, which retrieves the most similar positive and ne"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           32,
           "Ablation studies and tool efficacy We conduct ablation studies on the two components of AISciVision:"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           33,
           "2GPT4o generations are inherently random even after setting a seed, system fingerprint, the temperat"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           34,
           "0.67 0.68 0.74 0.78  0.85 0.89 0.88 0.95  0.85 0.85 0.90 0.92  0.62 0.67 0.76 0.81  0.86 0.90 0.91 0"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           35,
           "0.99 0.99 1.00 1.00  0.91 0.97 0.96 0.97  Table 2: We conduct ablation studies on each component of "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           36,
           "5 DEPLOYED APPLICATION  AISciVision is not just a conceptual framework, we have deployed it as a ful"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           37,
           "75  0.25  0.00  Solar  Brightness  Zoomout  100  Sharpen  25  Sharpen  0.25  Zoomin  50  0  Histogra"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           38,
           "Edge  Panleft  0.25  75  Eelgrass  Figure 3: We analyze the frequency of tool use in AISciVision for"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           39,
           "0.99 0.99 1.00 1.00  Preprint. Under review.  Figure 4: Screenshot of the AISciVision web applicatio"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           40,
           "6 DISCUSSION  Our AISciVision framework combines robust prediction capabilities, transparency, and a"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           41,
           "Limitations and Future Work While AISciVision offers significant benefits in providing transparent r"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           42,
           "REFERENCES  Abdelrahman Abdelhamed, Mahmoud Afifi, and Alec Go. What Do You See? Enhancing Zero-Shot"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           43,
           "Medicine, 28(9):1773–1784, 2022.  Seongsu Bae, Daeun Kyung, Jaehee Ryu, Eunbyeol Cho, Gyubok Lee, Su"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           44,
           "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           45,
           "Di Chen, Yiwei Bai, Sebastian Ament, Wenting Zhao, Dan Guevarra, Lan Zhou, Bart Selman, R Bruce van "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           46,
           "Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan W"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           47,
           "org/abs/2312.11805.  Laura Greenstreet, Joshua Fan, Felipe Siqueira Pacheco, Yiwei Bai, Marta Eichem"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           48,
           "Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization throug"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           49,
           "Alexandre Lacoste, Nils Lehmann, Pau Rodriguez, Evan David Sherwin, Hannah Kerner, Bj¨orn L¨utjens, "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           50,
           "12  Preprint. Under review.  9459–9474. Curran Associates, Inc., 2020. URL https://proceedings.neuri"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           51,
           "Visual Instruction Tuning.  Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           52,
           "Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan M Krumholz, Jure Leskovec, Eric J T"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           53,
           "nected with Massive APIs. arXiv preprint arXiv:2305.15334, 2023.  Alec Radford, Jong Wook Kim, Chris"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           54,
           "13  Preprint. Under review.  Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lo"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           55,
           "David Soong, Sriram Sridhar, Han Si, Jan-Samuel Wagner, Ana Caroline Costa S´a, Christina Y Yu, Kubr"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           56,
           "Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima A"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           57,
           "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           58,
           "Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Richard James, Jure Leskovec, Percy Liang, Mike Le"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           59,
           "14  Preprint. Under review.  Cyril Zakka, Rohan Shad, Akash Chaurasia, Alex R. Dalal, Jennifer L. Ki"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           60,
           "15  Preprint. Under review.  A DOMAIN-SPECIFIC TOOLS  The tools developed for each dataset are desig"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           61,
           "In contrast, the aquaculture pond detection task presents a unique challenge where the presence of w"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           62,
           "PanUpToolRelative: Pans the view upwards relative to the last image seen. • PanUpToolAbsolute: Pans "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           63,
           "A.2 TOOLS FOR EELGRASS DATASET  AdjustBrightnessTool: Adjusts the brightness of the image by 50%. Th"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           64,
           "PredictEelgrassWastingDiseaseTool: Predicts the probability of eelgrass wasting disease in the image"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           65,
           "IncreaseContrastTool: Increases the contrast of the image by 50%. This tool can be helpful when the "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           66,
           "20%  100%  Methods  Prec Rec  Prec Rec  Prec Rec  Prec Rec  Prec Rec  Prec Rec  k-NN 0.68 CLIP-ZeroS"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           67,
           "0.78 0.10 0.78 0.86 0.92 0.86 0.89  1.00 0.25 1.00 0.63 0.80 0.75 0.92  0.67 0.47 0.83 1.00 1.00 1.0"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           68,
           "18  Preprint. Under review.  C.1 AQUACULTURE"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           69,
           "True Class: 1, LMM Prediction: 1 (Probability: 0.80), Supervised Prediction: 0 (Probability: 25.91)S"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           70,
           ". Location: Aquaculture ponds are commonly found in coastal areas, estuaries,or near rivers and lake"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           71,
           "Assistant:In the provided satellite image, several characteristics identify the presence of an aquac"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           72,
           "Assistant:In the provided satellite image, there are no visible aquaculture ponds. Here are the obse"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           73,
           "20  Preprint. Under review."
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           74,
           "Assistant:In the provided satellite image, we need to determine whether there are aquaculture ponds "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           75,
           ".6.Texture: The water surfaces in the rectangular areas appear smooth, which is typical of controlle"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           76,
           "Here is the satellite image we need to classify as having an aquaculture pond or not. Describe what "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           77,
           "Conclusion:Based on the geometric patterns, water color, surrounding features, and comparison with k"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           78,
           ". - PanUpToolAbsolute: Pans the view upwards relative tothe original starting image. - PanDownToolRe"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           79,
           ". - ZoomOutToolAbsolute: Zooms out from the original view relative tothe original starting image.You"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           80,
           ". Note that this is not your final prediction.Then you will be asked again if you want to use a tool"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           81,
           "22  Preprint. Under review.  Assistant:[PanDownToolRelative]{yes:60,no:40}I am choosing to pan down "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           82,
           "You have 3 rounds left. You may now choose another tool or indicate you're finished. Respond with ON"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           83,
           ". New zoom level: 20You have 2 rounds left. You may now choose another tool or indicate you're finis"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           84,
           "23  Preprint. Under review.  The view has been panned down, revealing more of the area down of the p"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           85,
           "Assistant:[ZoomInToolRelative]{yes:80,no:20}I am choosing to zoom in further on the current view to "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           86,
           "User:Based on your analysis and the tools used, please provide your final classification regarding t"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           87,
           ". Here arethe key observations and tool results that influenced my decision:1.Geometric Patterns: Th"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           88,
           ".Given these observations and the additional details obtained through the tools, I am confident in c"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           89,
           "25  Preprint. Under review.  C.2 EELGRASS"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           90,
           "True Class: 1, LMM Prediction: 1 (Probability: 0.70), Supervised Prediction: 0 (Probability: 34.51)S"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           91,
           ". 4. Overall Decline: Heavily infected eelgrass beds often exhibit widespread dieback, with leaves a"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           92,
           "Assistant:This image shows eelgrass blades with dark, necrotic lesions. The lesions appear as irregu"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           93,
           "27  Preprint. Under review."
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           94,
           "Based on the presence of the dark, necrotic lesion on one of the blades in this image, it is likely "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           95,
           ". I reccomend you usethis tool. - AdjustBrightnessTool: Tool Name: 'AdjustBrightnessTool' Descriptio"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           96,
           ".You have 4 opportunities to use these tools. You are encouraged to use at least 3 tools to gather s"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           97,
           ". Note that this is not your final prediction.Then you will be asked again if you want to use a tool"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           98,
           "28  Preprint. Under review.  Assistant:  Assistant:{yes:80,no:20}The increased contrast has made the"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           99,
           "{yes:85,no:15}The sharpening has further clarified the edges and details of the dark lesion, making "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           100,
           ". If you want to reference the last toolyou used, simply write its name without brackets. After choo"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           101,
           ". Do not include multiple tool names or any other text in brackets. If you want to reference the las"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           102,
           "30  Preprint. Under review."
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           103,
           "User:Based on your analysis and the tools used, please provide your final classification regarding t"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           104,
           ".Initial Observation: The image initially showed a dark, necrotic lesion on one of the eelgrass blad"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           105,
           ".Final Classification: [Yes:70,No:30]This indicates a 70% certainty of eelgrass wasting disease pres"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           106,
           "31  Preprint. Under review.  C.3 SOLAR  Assistant:In the provided satellite image, several character"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           107,
           "True Class: 1, LMM Prediction: 1 (Probability: 0.99), Supervised Prediction: 1 (Probability: 98.95)S"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           108,
           "32  Preprint. Under review."
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           109,
           "Assistant:In the provided satellite image, several observations indicate the absence of solar panels"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           110,
           ". The rooftop instead features other types of equipment or structures.User:Here is the satellite ima"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           111,
           "User:This is an example of a satellite image without solar panels. Describe what you see, noting the"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           112,
           "Assistant:In the provided satellite image, several characteristics suggest the presence of solar pan"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           113,
           ".Second Example: Irregular color patterns without the uniform contrast typical of solar panels.Locat"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           114,
           "34  Preprint. Under review."
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           115,
           "Based on these observations, I classify the current image as containing solar panels.User:You have a"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           116,
           ". This tool is useful for making subtle features more prominent, which can help inidentifying solar "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           117,
           ".You have 4 opportunities to use these tools. You are encouraged to use at least 3 tools to gather s"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           118,
           ". Note that this is not your final prediction.Then you will be asked again if you want to use a tool"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           119,
           ". Then, explain why you are picking this tool or finishing, and"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           120,
           "35  Preprint. Under review.  how it relates to your current assessment of the image.Assistant:[Sharp"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           121,
           "Assistant:[EdgeDetectionTool] {yes:98.95,no:1.05}User:Edge detection has been applied to the image.Y"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           122,
           "36  Preprint. Under review.  User:Based on your analysis and the tools used, please provide your fin"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           123,
           "Assistant:[HistogramEqualizationTool] {yes:98.95,no:1.05}User:Histogram equalization has been applie"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           124,
           "percentages must sum to 100%. To break ties, always assign at least 51% to one category. For example"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           125,
           ".Size and Scale: The panels cover a significant portion of the rooftop, indicating a substantial ins"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           126,
           "38  Preprint. Under review.  D ABLATION FAILURE CASE  In our analysis of the Eelgrass dataset, we no"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           127,
           "Preprint. Under review.  D.1 GPT4O + VISRAG (CORRECT PREDICTION)  Assistant:This image shows eelgras"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           128,
           "True Class: 1, LMM Prediction: 1 (Probability: 0.85), Supervised Prediction: 0 (Probability: 7.13)Sy"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           129,
           ". 4. Overall Decline: Heavily infected eelgrass beds often exhibit widespread dieback, with leaves a"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           130,
           "40  Preprint. Under review.  Assistant:In this image, the eelgrass blades exhibit some dark spots an"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           131,
           "User:Based on your analysis and the tools used, please provide your final classification regarding t"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           132,
           ". The presence of these lesions, even though not as extensive as in the first example, suggests the "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           133,
           "42  Preprint. Under review.  D.2 GPT4O + TOOLS (INCORRECT PREDICTION)"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           134,
           "True Class: 1, LMM Prediction: 0 (Probability: 0.20), Supervised Prediction: 0 (Probability: 7.13)Sy"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           135,
           ". 4. Overall Decline: Heavily infected eelgrass beds often exhibit widespread dieback, with leaves a"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           136,
           "Assistant:To classify this image for eelgrass wasting disease, I will follow these steps:Identify Da"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           137,
           "Blotching: The dark streak on the left blade does not appear to have merged into larger blotches, bu"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           138,
           ". -DecreaseContrastTool: Tool Name: 'DecreaseContrastTool' Description: Decreases the contrast of th"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           139,
           ". This can help inidentifying lesions or patterns associated with eelgrass wasting disease. - Histog"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           140,
           ".After each tool use, end your thought with brackets in the format {Yes:yesPercent,No:noPercent} to "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           141,
           "44  Preprint. Under review.  [IncreaseContrastTool]User:Image contrast has been increased by 50%.You"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           142,
           "Assistant:[SharpenTool]{yes:60,no:40}I am using the SharpenTool to enhance the edges and details of "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           143,
           "Assistant:[PredictEelgrassWastingDiseaseTool]{yes:65,no:35}I am using the PredictEelgrassWastingDise"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           144,
           ". Then, explain why you are picking this tool orfinishing, and how it relates to your current assess"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           145,
           "46  Preprint. Under review.  You have 0 rounds left. You may now choose another tool or indicate you"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           146,
           "User:Based on your analysis and the tools used, please provide your final classification regarding t"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           147,
           ".2.IncreaseContrastTool: Enhanced the visibility of the dark streak, making it more prominent but di"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           148,
           "47  Preprint. Under review.  The combination of visual inspection and tool results suggests that whi"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           149,
           "True Class: 1, LMM Prediction: 0 (Probability: 0.40), Supervised Prediction: 0 (Probability: 7.13)Sy"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           150,
           ". 4. Overall Decline: Heavily infected eelgrass beds often exhibit widespread dieback, with leaves a"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           151,
           "49  Preprint. Under review.  Assistant:This image shows eelgrass blades that appear healthy, with no"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           152,
           "Based on the presence of these dark, necrotic lesions and the pattern of spotting and streaking, I c"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           153,
           ". I reccomend you usethis tool. - AdjustBrightnessTool: Tool Name: 'AdjustBrightnessTool' Descriptio"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           154,
           ".You have 4 opportunities to use these tools. You are encouraged to use at least 3 tools to gather s"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           155,
           ". Note that this is not your final prediction.Then you will be asked again if you want to use a tool"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           156,
           "51  Preprint. Under review.  Assistant:[SharpenTool]{Yes:75,No:25}I am using the SharpenTool to enha"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           157,
           "Assistant:[PredictEelgrassWastingDiseaseTool]{Yes:80,No:20}I am using the PredictEelgrassWastingDise"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           158,
           ". Then, explain why you are picking this tool orfinishing, and how it relates to your current assess"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           159,
           "53  Preprint. Under review."
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           160,
           "User:Based on your analysis and the tools used, please provide your final classification regarding t"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           161,
           ".IncreaseContrastTool: This tool enhanced the visibility of subtle details, making the dark spots an"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           162,
           ".Final Classification: [Yes:40,No:60]This indicates a 40% probability of eelgrass wasting disease pr"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           163,
           "54"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           0,
           "4 2 0 2  v o N 8  ] L C . s c [  1 v 3 2 4 5 0 . 1 1 4 2 : v i X r a  Proceedings of Machine Learnin"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           1,
           "Editor: Jeongwoo Lee, Kwangsuk Park, Jihyeon Park  Abstract Generating accurate and consistent visua"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           2,
           "1. Introduction  Visual aids are critical components of mathematics education, enabling students to "
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           3,
           "J. Lee, K. Park, J. Park  creating problems requiring precise visual representations is both time-co"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           4,
           "Generative AI, particularly LLMs, holds significant promise for addressing challenges in mathematics"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           5,
           "In this paper, we introduce a multi-agent framework for generating and visualizing math problems, le"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           6,
           "This framework introduces a novel workflow where LLMs generate both mathemati- cal problems and thei"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           7,
           "We utilized LLMs to provide detailed, step-by-step explanations for these problems, helping educator"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           8,
           "In this study, we utilized Claude 3.5 Sonnet as the core model for our framework. A critical aspect "
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           9,
           "3  J. Lee, K. Park, J. Park  Figure 1: Overview of the multi-agent system for generating math proble"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           10,
           "4  VISTA: Visual Integrated System for Tailored Automation in Math Problem Generation Using LLM  cul"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           11,
           "Function Validator. The Function Validator plays a complementary role by analyzing and verifying geo"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           12,
           "Code Executor and Summarizer. The Code Executor is responsible for executing the code generated by t"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           13,
           "5  J. Lee, K. Park, J. Park  Math Question Generator. The Math Question Generator is an agent design"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           14,
           "Within our framework, Claude 3.5 Sonnet was used to generate complex figures and math- ematical func"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           15,
           ". While visual similarity is important, it alone is not sufficient to comprehensively evalu- ate mat"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           16,
           "2. https://huggingface.co/spaces/opencompass/open_vlm_leaderboard  6  VISTA: Visual Integrated Syste"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           17,
           "To address the shortcomings of conventional visual similarity metrics, we developed an enhanced eval"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           18,
           "4. Results  4.1. Geometry  In Figure 2, the performance of VISTA and the baseline system is evaluate"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           19,
           "In the Length and Applied subtypes, the pattern remains consistent, with VISTA demon- strating stron"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           20,
           "4.2. Function  In Figure 3, the evaluation of function-related problems, which includes the subtypes"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           21,
           "In the Coordinate and Applied subtypes, VISTA continues to outperform the baseline in coherence and "
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           22,
           "4.3. Overall Analysis  The comparison between VISTA and the baseline system for Geometry and Functio"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           23,
           "ric concepts more effectively. Similarly, in the Function section, it excels in coherence and releva"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           24,
           "5. Discussion  This study has demonstrated the potential of LLMs as powerful tools for generating vi"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           25,
           "11  J. Lee, K. Park, J. Park  stance, when certain critical details were omitted from a problem, the"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           26,
           "While our system has shown considerable potential, it is not yet perfect, and several avenues for im"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           27,
           "Journal of Research & Practice, 25(5-6):355–360, 2001.  Arya Bulusu, Brandon Man, Ashish Jagmohan, A"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           28,
           "12  VISTA: Visual Integrated System for Tailored Automation in Math Problem Generation Using LLM  Qi"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           29,
           "Angle Length Area Applied Analytic  193 158 47 69 41  Total  508  Table 2: Function Problem Subtypes"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           30,
           "Figure 14: Prompt for Evaluation of Consistency  17  J. Lee, K. Park, J. Park  Figure 15: Prompt for"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           0,
           "5 2 0 2  b e F 4  ] L C . s c [  1 v 9 5 3 4 0 . 2 0 5 2 : v i X r a  Exploring Spatial Language Gro"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           1,
           "Abstract  Spatial Reasoning is an important component of human cognition and is an area in which the"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           2,
           "ther to analyze the comparative performance of these models for spatial categories that represent di"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           3,
           "We test two popular VLMs - LLaVA [Liu et al., 2024] and Grounding DINO [Liu et al., 2023b]. We also "
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           4,
           "1 Introduction Vision-Language model (VLM) research has boomed in the recent past, owing to the enha"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           5,
           "Some of our important findings are as follows:  (1) Spatial relations contribute to more accurate gr"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           6,
           "(a) The white napkin that is wrapped around the hot dog  (b) The white box that is around the mirror"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           7,
           "ing tasks such as Spatial Reasoning, Multimodal conversa- tion, etc. Works such as [Tian et al., 202"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           8,
           "[R¨osch and Libovick`y, 2023; Subramanian et al., 2022] focus solely on spatial analysis of VLMs. [W"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           9,
           ". [Lewis et al., 2022] ana- lyze if the models are erring in recognizing objects, relations, or both"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           10,
           "Other closely aligned work includes Embodied Spatial Analysis which focuses on the effects of differ"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           11,
           "3 Dataset Table 1 shows the key characteristics of some popular REC datasets. We chose CopsRef over "
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           12,
           "10.6 8.2 6.5 17.4  59 72 4 51  Table 1: Statistics of Popular Referring Expression Comprehension dat"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           13,
           "Unallocated  56  Table 2: Category-wise relation split and number of referring expressions in the Co"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           14,
           "Category  No. of relations No. of expressions  To answer these questions, we explain our research  N"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           15,
           "[Zheng et al., 2020] is an REC task-specific MGA-Net. model whose compositional learning architectur"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           16,
           "limit our training to ten epochs due to computational con- straints. [Liu et al., 2023b] It is an op"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           17,
           ". The model is trained end-to-end which involves visual instruction tuning for align- ing the vision"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           18,
           ". Model Differences. A key difference in the three main models can be seen in their input format. Wh"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           19,
           "4.2 Experimental Setting and Evaluation We create the following dataset test splits for evaluation a"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           20,
           "Visual Complexity Split To observe the effect of visual complexity on model perfor- mance, we split "
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           21,
           "5 Results  Model  Accuracy (%)  MGA-Net (Partial CNN) Grounding DINO LLaVA - Short Prompt MGA-Net (F"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           22,
           "MGA-Net (Acc %) Rank Grounding DINO (Acc %) Rank LLaVA (Acc %) Rank  Absolute Adjacency Directional "
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           23,
           "2 4 12 8 3 1 5 6 7 10 9 11  44.64 ± 0 50 ± 0 27.59 ± 0 36.19 ± 0.08 46.84 ± 0.22 48.51 ± 0.09 35.71 "
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           24,
           "5.1 Evaluation on Referring Expressions From Table 4, we can observe that Grounding DINO and MGA-Net"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           25,
           "Model 1  Model 2  Coeff, Z-score  2-tailed test (Correlated)  MGA-Net GDINO 0.18, 0.51 0.09, 0.26 MG"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           26,
           "Since we trained/tested each model for three runs, we re- port the average accuracy of the three run"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           27,
           "Among spatial categories of MGA-Net and VLMs, the ma- jor difference occurs with the Proximity and P"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           28,
           "73.88 ± 0 73.94 ± 0 69 ± 0 67.78 ± 0  42.89 ± 0.17 40.33 ± 0.01 31.05 ± 0.06 30.19 ± 0.26  67.8 ± 0 "
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           29,
           "An interesting observation is that the performance of the baseline considerably drops for the ‘Two’ "
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           30,
           "Table 8: Results for accuracy in different visual complexity settings.  and multiple instances (‘Acc"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           31,
           "Now, to answer RQ3, we observe in Table 5 that among the seven categories of single spatial relation"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           32,
           "73 Total Total failure 59 Grounding DINO 58 29 LLaVA 35 MGA-Net  36 24 23 17 20  Table 9: Results fo"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           33,
           "Out of 12586 test data points, we found that in the images of 4730 data points, there are multiple i"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           34,
           "6 Qualitative Analysis Here, we provide a qualitative analysis of certain issues faced by the models"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           35,
           "6.2 Projective and Proximity Relations Figure 1c shows an example of Projective relations (‘to the l"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           36,
           "the model might pay attention to only one spatial clause. Consequently, it returns an object satisfy"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           37,
           "7 Conclusion  Spatial reasoning and understanding is an area in which the latest VLMs have shown sig"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           38,
           "8 Future Directions  We observed that MGA-Net handles expressions with vary- ing spatial complexity "
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           39,
           "Another issue to address is the VLMs’ inability to com- prehend negations. MGA-Net’s improved perfor"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           40,
           "Irena Gao,  [Chen et al., 2020] Zhenfang Chen, Peng Wang, Lin Ma, Kwan-Yee K Wong, and Qi Wu. Cops-R"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           41,
           "[Fu et al., 2025] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           42,
           "[Islam et al., 2022] Md Mofijul Islam, Reza Mirzaiee, Alexi Gladstone, Haley Green, and Tariq Iqbal."
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           43,
           "[Kirillov et al., 2023] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Lau"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           44,
           "[Lewis et al., 2022] Martha Lewis, Nihal V Nayak, Peilin Yu, Qinan Yu, Jack Merullo, Stephen H Bach,"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           45,
           "[Liu et al., 2022] Xiao Liu, Da Yin, Yansong Feng, and Dongyan Zhao. Things not Written in Text: Exp"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           46,
           "[Liu et al., 2024] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual Instruction Tunin"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           47,
           "[Mirzaee et al., 2021] Roshanak Mirzaee, Hossein Ra- jaby Faghihi, Qiang Ning, and Parisa Kordjmashi"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           48,
           "[Qiu et al., 2021] Linlu Qiu, Hexiang Hu, Bowen Zhang, Pe- ter Shaw, and Fei Sha. Systematic General"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           49,
           "[R¨osch and Libovick`y, 2023] Philipp J R¨osch and Jindˇrich Infor- arXiv preprint  Libovick`y. mati"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           50,
           "[Team et al., 2023] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, "
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           51,
           "[Yu et al., 2018] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L "
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           52,
           "1. Absolute: Consists of relations that describe the loca- tion of an object in an absolute manner a"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           53,
           "6. Proximity: Consists of relations that indicate that two objects are near each other without givin"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           54,
           "InstructBLIP For InstructBLIP, we designed three prompts for the REC task. They are as follows:  1. "
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           55,
           "total length/width of the image according to the position of the coordinate.  Unfortunately, none of"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           56,
           "Prompt: Provide the bounding box coordinates for: ”The large poster that is leaning against the wall"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           57,
           "Prompt 2:  Example output format: <image>Expression: Ref- exp; Correct Bounding Box:[Bounding box co"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           58,
           "Work  Bench mark  Evaluation Tasks Models  Text Source  for Rows data entries [Wang et al., 2024]  N"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           59,
           "New: VSR  New: MM- Bench  Image-caption agreement Multi-Choice VQA  New: SEED- Bench  Multi-Choice V"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           60,
           "CLIP,  Human annotation with constraints  GPT-4-generated ques- tions and answers for the conversati"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           61,
           "Image-caption agreement  Multi-choice Im- age Captioning  Text-to-image generation  Multi-choice Im-"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           62,
           "Work  Image Type  Image Source  Spatial Complexity  No. of Spatial Relations  Spatial Deeper Relatio"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           63,
           "80  37  66  No  No  Yes  No  No  Yes  2023a]  [Liu et al.,  2023c]  [Li et al.,  2023]  [Yu et al., "
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           64,
           "Captions - Simple  Captions - Simple      86  15  28  6  4  No  No  No  Yes  Yes  Yes  No  No  No  N"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           65,
           "CopsRef Dataset  Captions - Simple  Referring expres- sions - Complex  8 + their superla- tives 51  "
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           0,
           "4 2 0 2  t c O 5 1  ]  V C . s c [  1 v 5 6 6 1 1 . 0 1 4 2 : v i X r a  VisualRWKV-HD and UHD: Adva"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           1,
           "Accurately understanding complex visual infor- mation is crucial for visual language models (VLMs). "
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           2,
           "1  Introduction  advancements With al., in large 2024)(LLMs)(Achiam et al., 2023), visual language m"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           3,
           "the  significant language models(Akyürek et  recent  et al., 2024) and VL-Mamba(Qiao et al., 2024), "
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           4,
           "1. VisualRWKV-HD with Ensemble of En- coders: We introduce an ensemble of en- coders, where the inpu"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           5,
           "2. VisualRWKV-UHD: In VisualRWKV-UHD, the image is divided into four segments and then recombined, a"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           6,
           "4. High-Resolution and Low-Resolution Align- ment: To align high-resolution vision en- coders with l"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           7,
           "2 Related Works  2.1 Linear Visual Language Models  In the rapidly evolving field of visual language"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           8,
           "2  tails are preserved. Together, these models not only signify important advancements in the VLM la"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           9,
           "2.3 Comparative Analysis  To elucidate the innovations in VisualRWKV-HD and VisualRWKV-UHD, we compa"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           10,
           "3  3 Method  3.1 VisualRWKV-HD  3.1.1 Ensemble of Encoders In previous versions of VisualRWKV, we ut"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           11,
           "3.1.2 Lossless DownSampler SigLip and DINOv2, as encoders from the previ- ous generation of VisualRW"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           12,
           "This formula illustrates how the new channel dimension is created by combining the lower- resolution"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           13,
           "element-wise multiplication. This mechanism op- timizes the model’s performance by dynamically adjus"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           14,
           "y = σ(Wgx + bg) ⊙ x  In this formula, y represents the output, Wg is the gating weight matrix, bg is"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           15,
           "4.1 Baselines  In the Baselines section, we compare the per- formance of the proposed VisualRWKV-HD "
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           16,
           "Resolution SQA 59.05 336 53.35 384 57.02 384 58.55 448 54.39 448 58.75 448 56.97 448  TextVQA GQA Vi"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           17,
           "Table 1: Scaling results of VisualRWKV-HD/UHD models and their performance metrics across different "
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           18,
           "Table 2: Scaling results of VisualRWKV-HD/UHD models on Text-rich tasks  These tasks often require r"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           19,
           "4.2 Benchmarks  evaluations of extensive We VisualRWKV-HD VisualRWKV-UHD and using eight diverse ben"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           20,
           "world context with noisy and incomplete visual data, a particularly challenging task designed to ass"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           21,
           ". Lastly, ChartQA assesses our model’s ability to interpret various chart types, revealing excellent"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           22,
           "5  The results from modalities and languages. these benchmarks highlight the significant perfor- man"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           23,
           "In the Quantitative Evaluation, we assess the per- formance of our proposed VisualRWKV-HD and Visual"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           24,
           ".7B with a score of 86.0 versus 84.5, and UHD also exceeds with 85.3. In the MMB bench- mark, our mo"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           25,
           ".31, Mini-Gemini: 59.8). This underscores the efficiency of our models in visual processing. Lastly,"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           26,
           "6  parameter counterparts in complex visual bench- marks.  4.4 Ablation Study  In the Experiment sec"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           27,
           "4.4.1 Ablation on Vision Decoder  In this section, we compared visual encoders, specifically Siglip "
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           28,
           "4.4.2 Ablation on Resolution  In this experiment, we conducted further research based on the introdu"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           29,
           "- - 43.6 54.71 56.31  56.1 - 57.5 55.2 60.84 59.52  - - - 54.97 49.88  MME 1196.2/- 1341/312 - 1204."
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           30,
           "43.57 41.08 48.70  55.23 56.55 58.23  MME 1204.90/245.00 1273.67/213.92 1250.50/213.21  POPE MMB/MMB"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           31,
           "4.4.3 Ablation on Projection  In this experiment, we explored the impact of re- placing the linear p"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           32,
           "ciency of the VisualRWKV-HD and UHD models: VisualRWKV-HD: After introducing the HD559k dataset, com"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           33,
           "4.4.4 Ablation on Data Scaling up  4.5 Efficiency Analysis  In the experiment, we compared the perfo"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           34,
           "7  Model Vision Encoder VisualRWKV-HD SigLIP + DINOv2 + SAM-b-1024 VisualRWKV-HD SigLIP + DINOv2 + S"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           35,
           "47.75 54.71  60.96 60.84  MME 1305.38/224.64 1378.62/266.07  POPE MMB/MMB-CN 0.855 0.860  59.45/53.0"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           36,
           "MME 1378.62/266.07 1271.03/230.36 1321.33/232.14  POPE MMB/MMB-CN 0.860 0.857 0.853  60.31/55.41 57."
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           37,
           "The introduction of techniques such as lossless downsampling, segmented image representation, and th"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           38,
           "capabilities in visual language models.  Limitations Despite the significant performance improve- me"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           39,
           "References  Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Al"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           40,
           "vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966.  Jeffrey P Bigham, "
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           41,
           "Drew A. Hudson and Christopher D. Manning. 2019. Gqa: A new dataset for real-world visual reason- in"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           42,
           "Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Cla"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           43,
           "9  Antoine Miech, Ivan Laptev, and Josef Sivic. 2017. Learnable pooling with context gating for vide"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           44,
           "Yanyuan Qiao, Zheng Yu, Longteng Guo, Sihan Chen, Zijia Zhao, Mingzhen Sun, Qi Wu, and Jing Liu. 202"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           45,
           "A Model Architecture and Computing  Model Architecture: The VisualRWKV models used in our experiment"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           46,
           "Computing Budget: Training an epoch of VisualRWKV 1.6B with 8 A100 GPUs takes 6.7 hours, equivalent "
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           47,
           "HD559k: This dataset is our custom high-resolution dataset consisting of 559,000 high-quality images"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           48,
           "Training: The models were trained using the AdamW optimizer with a learning rate of X, using NVIDIA "
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           49,
           "11  Dataset Name textocr DocReason25K sharegpt4v_instruct_61k monkey_685k_multi_round llavar_16k pdf"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           50,
           "Table 9: Overview of Datasets Used of HD667k  D Data and Hyperparameters  A. Training Data  We used "
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           51,
           "We employed various benchmarks to evaluate the model. VQA-v2 and GQA metrics are based on  12  the t"
          ],
          [
           "../data/arxiv/pdfs/2410.11665v1.pdf",
           52,
           "1.6B-Pretrain 256 1e-3 1e-5 cosine decay 0 0 2 AdamW 1  1.6B-Finetune 128 6e-5 1.5e-5 cosine decay 0"
          ]
         ],
         "hovertemplate": "x=%{x}<br>y=%{y}<br>document=%{customdata[0]}<br>chunk=%{customdata[1]}<br>preview=%{customdata[2]}<br>color_id=%{marker.color}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": {
           "bdata": "AgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICA==",
           "dtype": "i1"
          },
          "coloraxis": "coloraxis",
          "opacity": 0.6,
          "size": 6,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "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",
          "dtype": "f8"
         },
         "xaxis": "x",
         "y": {
          "bdata": "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",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "color_id"
          }
         },
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "legend": {
         "tracegroupgap": 0
        },
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Interactive PCA Projection of Chunks"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "x"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "y"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    color=\"color_id\",\n",
    "    hover_data=[\"document\", \"chunk\", \"preview\"],\n",
    "    title=\"Interactive PCA Projection of Chunks\"\n",
    ")\n",
    "fig.update_traces(marker=dict(size=6, opacity=0.6))\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4cee14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
