{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "757f0f5a",
   "metadata": {},
   "source": [
    "# PDF Processing: Load, Chunk, and Prepare for Vector DB\n",
    "\n",
    "This notebook demonstrates a step-by-step approach to loading, enriching,\n",
    "chunking, and storing scientific paper PDFs into a vector database.\n",
    "It compares different outputs at each stage interactively with helpful visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33f71e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from IPython.display import display\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3412558",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DOCS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06472b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 181 PDF files in ../data/arxiv/pdfs\n"
     ]
    }
   ],
   "source": [
    "# ## Step 1: Load Multiple PDF Documents\n",
    "\n",
    "pdf_path = Path(\"../data/arxiv/pdfs/\")  # change to your path\n",
    "pdf_files = list(pdf_path.glob(\"*.pdf\"))\n",
    "metadata_path = Path(\"../data/arxiv/metadata/\")  # change to your path\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "print(f\"Found {len(pdf_files)} PDF files in {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f85bbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading PDFs:   0%|          | 0/10 [00:00<?, ?it/s]CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "Loading PDFs:  10%|█         | 1/10 [00:06<00:57,  6.38s/it]CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "Loading PDFs:  20%|██        | 2/10 [00:07<00:24,  3.03s/it]CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "Loading PDFs:  30%|███       | 3/10 [00:09<00:20,  2.93s/it]CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "Loading PDFs:  40%|████      | 4/10 [00:11<00:13,  2.30s/it]CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "Loading PDFs:  50%|█████     | 5/10 [00:11<00:08,  1.74s/it]CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "Loading PDFs:  60%|██████    | 6/10 [00:13<00:07,  1.81s/it]CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "Loading PDFs:  70%|███████   | 7/10 [00:15<00:05,  1.85s/it]CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "Loading PDFs:  80%|████████  | 8/10 [00:18<00:04,  2.19s/it]CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "Loading PDFs:  90%|█████████ | 9/10 [00:19<00:01,  1.74s/it]CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "Loading PDFs: 100%|██████████| 10/10 [00:21<00:00,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loaded 10 documents ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for pdf_file in tqdm(pdf_files[:MAX_DOCS], desc=\"Loading PDFs\"):\n",
    "    try:\n",
    "        loader = UnstructuredPDFLoader(file_path=pdf_file, mode=\"single\")\n",
    "        doc = loader.load()[0]\n",
    "\n",
    "        metadata_file = pdf_file.with_suffix(\".json\")\n",
    "        if metadata_file.exists():\n",
    "            with open(metadata_file) as f:\n",
    "                metadata = json.load(f)\n",
    "            doc.metadata |= metadata\n",
    "\n",
    "        all_docs.append(doc)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {pdf_file}: {e}\")\n",
    "\n",
    "print(f\"--- Loaded {len(all_docs)} documents ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1af2b131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Chunk Stats ---\n",
      "Total chunks: 623\n",
      "Avg size: 236.57, Max: 302, Min: 3\n",
      "[Document(metadata={'source': '../data/arxiv/pdfs/2410.05160v3.pdf', 'chunk_index': 0}, page_content='5 2 0 2\\n\\nn a J\\n\\n2\\n\\n]\\n\\nV C . s c [\\n\\n3 v 0 6 1 5 0 . 0 1 4 2 : v i X r a\\n\\nManuscript\\n\\nVLM2VEC: TRAINING VISION-LANGUAGE MODELS FOR MASSIVE MULTIMODAL EMBEDDING TASKS\\n\\nZiyan Jiang1∗, Rui Meng2, Xinyi Yang2, Semih Yavuz2, Yingbo Zhou2, Wenhu Chen1 1University of Waterloo, 2Salesforce Research ziyanjiang528@gmail.com, ruimeng@salesforce.com, wenhuchen@uwaterloo.ca\\n\\nhttps://tiger-ai-lab.github.io/VLM2Vec/\\n\\nABSTRACT'), Document(metadata={'source': '../data/arxiv/pdfs/2410.05160v3.pdf', 'chunk_index': 1}, page_content='Embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering. Recently, there has been a surge of interest in developing universal text embedding models that can generalize across tasks (e.g., MTEB). However, progress in learning universal multimodal embedding models has been relatively slow despite its importance and practicality. In this work, we aim to explore the potential of building uni- versal multimodal embeddings capable of handling a wide range of downstream tasks. Our contributions are two fold: (1) we propose MMEB (Massive Mul- timodal Embedding Benchmark), which covers 4 meta-tasks (i.e. classification, visual question answering, multimodal retrieval, and visual grounding) and 36 datasets, including 20 training datasets and 16 evaluation datasets covering both in-distribution and out-of-distribution tasks, and (2) VLM2VEC (Vision-Language Model → Vector), a contrastive training framework that converts any vision- language model into an embedding model via contrastive training on MMEB. Unlike previous models such as CLIP or BLIP, which encodes text or images independently without any task instruction, VLM2VEC can process any combi- nation of images and text to generate a fixed-dimensional vector based on the given task instructions'), Document(metadata={'source': '../data/arxiv/pdfs/2410.05160v3.pdf', 'chunk_index': 2}, page_content='. We build a series of VLM2VEC models on SoTA VLMs like Phi-3.5-V, LLaVA-1.6 and evaluate them on MMEB’s evaluation split. With LoRA tuning, VLM2VEC can achieve an improvement of 10% to 20% over ex- isting multimodal embedding models on MMEB evaluation sets. We show that VLMs are secretly strong embedding models.'), Document(metadata={'source': '../data/arxiv/pdfs/2410.05160v3.pdf', 'chunk_index': 3}, page_content='1\\n\\nINTRODUCTION'), Document(metadata={'source': '../data/arxiv/pdfs/2410.05160v3.pdf', 'chunk_index': 4}, page_content='Embeddings, or distributed representations, text or images) as Since the advent of fixed-dimensional vectors, enabling a range of downstream tasks. Word2Vec (Mikolov, 2013) and GloVe (Pennington et al., 2014), substantial research efforts have focused on learning textual embeddings (Kiros et al., 2015; Conneau et al., 2017) and image em- beddings (Radford et al., 2021; Li et al., 2022; Jia et al., 2021; Yu et al., 2022). These embeddings facilitate a variety of applications, including textual and visual semantic similarity (Agirre et al., 2012; Marelli et al., 2014; Chechik et al., 2010; Cer et al., 2017), information retrieval (Mitra et al., 2017; Karpukhin et al., 2020; Lin et al., 2014), automatic evaluation (Zhang et al., 2020; Sellam et al., 2020), prompt retrieval for in-context learning (Liu et al., 2022; Rubin et al., 2022; Hongjin et al., 2022), and retrieval-augmented generation (Lewis et al., 2020; Guu et al., 2020; Izacard & Grave, 2020)'), Document(metadata={'source': '../data/arxiv/pdfs/2410.05160v3.pdf', 'chunk_index': 5}, page_content='., 2020; Guu et al., 2020; Izacard & Grave, 2020). A recent shift in research has focused on developing universal embeddings that can generalize across a wide range of tasks. For instance, Muennighoff et al. (2023) introduced MTEB (Massive Text Embedding Benchmark) to comprehensively assess text embeddings across tasks such as classification and clustering. MTEB has become the standard for evaluating universal text em- beddings. Recent works (Wang et al., 2022a; Su et al., 2023; Wang et al., 2024; Springer et al., 2024; BehnamGhader et al., 2024) have demonstrated promising results on the MTEB benchmark. However, progress in multimodal embeddings has been relatively slower. Despite advancements'), Document(metadata={'source': '../data/arxiv/pdfs/2410.05160v3.pdf', 'chunk_index': 6}, page_content='encode inputs (whether\\n\\n∗Work done during an internship at University of Waterloo in collaboration with Salesforce Research. Cor-\\n\\nresponding authors are Ziyan Jiang, Rui Meng and Wenhu Chen\\n\\n1\\n\\nManuscript\\n\\nThe menu is to the point: meat, above all lamb.\\n\\nVisual Groundingright sandwich, left half\\n\\nThe Titisee is a lake in the southern Black Forest in Baden-Württemberg.\\n\\nFoods\\n\\nVLM2Vec\\n\\nInstruction: Retrieve an image-description pair that provides evidence for the given question and image.\\n\\nInstruction: Represent the given image and the related question.\\n\\nClassification\\n\\nInstruction: Select the portion of the image that follows the language expressions.\\n\\nInstruction: Represent the given news image with the following caption for domain classification.\\n\\nThe napkin is under the utensil.\\n\\nRetrievalWhat is the name of this place?\\n\\nMMEB\\n\\nVisual Question AnsweringWhat is under the utensil on the left?\\n\\nFigure 1: We develop a universal multimodal embedding benchmark, MMEB, along with VLM2VEC, an embedding model adapted from vision-language models (VLMs). VLM2VEC is capa- ble of following instructions and performing various multimodal embedding tasks, accommodating any combination of image and text modalities.\\n\\nin text embeddings, the lack of both benchmarks and methodologies in the multimodal embedding domain remains a challenge.'), Document(metadata={'source': '../data/arxiv/pdfs/2410.05160v3.pdf', 'chunk_index': 7}, page_content='in text embeddings, the lack of both benchmarks and methodologies in the multimodal embedding domain remains a challenge.\\n\\nCurrent research in multimodal embeddings faces two primary limitations: (1) existing studies typ- ically evaluate visual embeddings on isolated tasks, such as ImageNet classification (Deng et al., 2009; Hendrycks et al., 2021a;b) or MSCOCO/Flickr retrieval (Lin et al., 2014; Plummer et al., 2015); (2) most existing models, such as CLIP (Radford et al., 2021), BLIP (Li et al., 2022), and SigLIP (Zhai et al., 2023), either process text and images separately or perform shallow fusion of visual and textual information (Wei et al., 2023), limiting their ability to fully capture the relation- ships between text and image modalities. Furthermore, these models exhibit limited reasoning and generalization capabilities, particularly in zero-shot scenarios for complex reasoning tasks.\\n\\nIn this paper, we attempt to build an universal multimodal embedding framework to pave road for the future research, which consists of two efforts:'), Document(metadata={'source': '../data/arxiv/pdfs/2410.05160v3.pdf', 'chunk_index': 8}, page_content='MMEB: We introduce a novel benchmark, MMEB (Massive Multimodal Embedding Benchmark), which includes 36 datasets spanning four meta-task categories: classification, visual question an- swering, retrieval, and visual grounding. MMEB provides a comprehensive framework for training and evaluating embedding models across various combinations of text and image modalities. All tasks are reformulated as ranking tasks, where the model follows instructions, processes a query, and selects the correct target from a set of candidates. The query and target can be an image, text, or a combination of both. MMEB is divided into 20 in-distribution datasets, which can be used for training, and 16 out-of-distribution datasets, reserved for evaluation. - VLM2VEC: We adopt the pre-trained vision-language models like Phi-3.5-V (Abdin et al., 2024) and LLaVA-1.6 (Li et al., 2024) as the backbone for VLM2VEC. In contrast to other multimodal embedding models like UniIR (Wei et al., 2023) and MagicLens (Zhang et al., 2024), which rely on late fusion of CLIP (Radford et al., 2021) features, our approach leverages the deep integration of vision and language features within a transformer architecture'), Document(metadata={'source': '../data/arxiv/pdfs/2410.05160v3.pdf', 'chunk_index': 9}, page_content='., 2021) features, our approach leverages the deep integration of vision and language features within a transformer architecture. There are several advantages to this approach: (1) VLMs are trained on massive multimodal datasets and can handle any combi- nation of images and text, as well as high-resolution images and long text inputs; (2) vision and language features are deeply fused in the transformer model, improving the model’s ability to cap- ture cross-modal relationships; and (3) these models are well-suited for generalizing across diverse tasks, particularly those requiring instruction-following capabilities. These factors make VLM2VEC an ideal choice for task generalization. We trained VLM2VEC on the 20 MMEB training datasets using contrastive learning and compared its performance with various baselines. 2')]\n"
     ]
    }
   ],
   "source": [
    "# ## Step 2: Recursive Chunking by Tokens\n",
    "\n",
    "model_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer,\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=30,\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "final_chunks = []\n",
    "chunk_by_doc = {}\n",
    "for doc in all_docs:\n",
    "    doc_chunks = []\n",
    "    for i, chunk in enumerate(splitter.split_text(doc.page_content)):\n",
    "        metadata = doc.metadata.copy()\n",
    "        metadata[\"chunk_index\"] = i\n",
    "        doc_chunks.append(Document(page_content=chunk, metadata=metadata))\n",
    "    chunk_by_doc[doc.metadata.get(\"title\", doc.metadata.get(\"source\", \"Document\"))] = (\n",
    "        doc_chunks\n",
    "    )\n",
    "    final_chunks.extend(doc_chunks)\n",
    "\n",
    "chunk_lengths = [\n",
    "    len(tokenizer.encode(chunk.page_content, truncation=False))\n",
    "    for chunk in final_chunks\n",
    "]\n",
    "\n",
    "print(f\"\\n--- Final Chunk Stats ---\")\n",
    "print(f\"Total chunks: {len(final_chunks)}\")\n",
    "print(\n",
    "    f\"Avg size: {np.mean(chunk_lengths):.2f}, Max: {np.max(chunk_lengths)}, Min: {np.min(chunk_lengths)}\"\n",
    ")\n",
    "print(final_chunks[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8d673ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram of Chunk Sizes\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(chunk_lengths, bins=20, color=\"skyblue\")\n",
    "plt.title(\"Token Count Distribution per Chunk\")\n",
    "plt.xlabel(\"Token Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56e0c0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4305e0363cac4c85b5ef2b15a555bd55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Document:', layout=Layout(width='50%'), options=('../data/arxiv/pdfs/2410…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc_selector = widgets.Dropdown(\n",
    "    options=list(chunk_by_doc.keys()),\n",
    "    description=\"Document:\",\n",
    "    layout=widgets.Layout(width=\"50%\"),\n",
    ")\n",
    "\n",
    "chunk_slider = widgets.IntSlider(min=0, max=1, step=1, description=\"Chunk:\")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "\n",
    "def update_slider(*args):\n",
    "    selected_doc = doc_selector.value\n",
    "    chunk_slider.max = len(chunk_by_doc[selected_doc]) - 1\n",
    "    chunk_slider.value = 0\n",
    "    show_chunk(0)\n",
    "\n",
    "\n",
    "def show_chunk(i):\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        selected_doc = doc_selector.value\n",
    "        chunk = chunk_by_doc[selected_doc][i]\n",
    "        print(chunk.metadata)\n",
    "        print(\"\\n\" + chunk.page_content[:1000])\n",
    "\n",
    "\n",
    "chunk_slider.observe(lambda change: show_chunk(change[\"new\"]), names=\"value\")\n",
    "doc_selector.observe(update_slider, names=\"value\")\n",
    "\n",
    "display(widgets.VBox([doc_selector, chunk_slider, output_area]))\n",
    "update_slider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e1840ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stored in Qdrant Vector DB ---\n",
      "Collection: papers_demo\n"
     ]
    }
   ],
   "source": [
    "# ## Step 3: Store in Qdrant Vector DB\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=model_id)\n",
    "\n",
    "db = Qdrant.from_documents(\n",
    "    documents=final_chunks,\n",
    "    embedding=embedding,\n",
    "    location=\"localhost:6333\",\n",
    "    collection_name=\"papers_demo\",\n",
    "    prefer_grpc=False,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Stored in Qdrant Vector DB ---\")\n",
    "print(f\"Collection: {db.collection_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcbfda3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '../data/arxiv/pdfs/2410.05160v3.pdf', 'chunk_index': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chunks[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee2a9a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA\n",
    "texts = [doc.page_content for doc in final_chunks]\n",
    "embeddings = embedding.embed_documents(texts)\n",
    "pca = PCA(n_components=2)\n",
    "points = pca.fit_transform(embeddings)\n",
    "\n",
    "# Create color group\n",
    "doc_names = [\n",
    "    doc.metadata.get(\"title\", doc.metadata.get(\"source\", \"Doc \" + str(i)))\n",
    "    for i, doc in enumerate(final_chunks)\n",
    "]\n",
    "unique_doc_ids = {name: i for i, name in enumerate(set(doc_names))}\n",
    "colors = [unique_doc_ids[name] for name in doc_names]\n",
    "\n",
    "# DataFrame for plot\n",
    "df = pd.DataFrame({\n",
    "    \"x\": points[:, 0],\n",
    "    \"y\": points[:, 1],\n",
    "    \"document\": doc_names,\n",
    "    \"chunk\": [doc.metadata.get(\"chunk_index\", 0) for doc in final_chunks],\n",
    "    \"preview\": [doc.page_content[:100].replace(\"\\n\", \" \") for doc in final_chunks],\n",
    "    \"color_id\": colors\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01da3d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           0,
           "5 2 0 2  n a J  2  ]  V C . s c [  3 v 0 6 1 5 0 . 0 1 4 2 : v i X r a  Manuscript  VLM2VEC: TRAININ"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           1,
           "Embedding models have been crucial in enabling various downstream tasks such as semantic similarity,"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           2,
           ". We build a series of VLM2VEC models on SoTA VLMs like Phi-3.5-V, LLaVA-1.6 and evaluate them on MM"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           3,
           "1  INTRODUCTION"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           4,
           "Embeddings, or distributed representations, text or images) as Since the advent of fixed-dimensional"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           5,
           "., 2020; Guu et al., 2020; Izacard & Grave, 2020). A recent shift in research has focused on develop"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           6,
           "encode inputs (whether  ∗Work done during an internship at University of Waterloo in collaboration w"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           7,
           "in text embeddings, the lack of both benchmarks and methodologies in the multimodal embedding domain"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           8,
           "MMEB: We introduce a novel benchmark, MMEB (Massive Multimodal Embedding Benchmark), which includes "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           9,
           "., 2021) features, our approach leverages the deep integration of vision and language features withi"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           10,
           "Manuscript"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           11,
           "Following extensive contrastive training, VLM2VEC can handle any combination of images and text, pro"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           12,
           ".0-point increase (from 43.1 to 57.1) on 16 out-of-distribution datasets for zero-shot evaluation. M"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           13,
           "2 MMEB: A BENCHMARK FOR MULTIMODAL EMBEDDINGS  2.1 DATASET OVERVIEW  We present MMEB (Massive Multim"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           14,
           "The embedding models are supposed to compress the query side into a vector and the target can- didat"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           15,
           "MMEB offers a wide range of tasks from various domains, such as common, news, Wikipedia, web, and fa"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           16,
           "Information Retrieval Both the query and target sides can involve a combination of text, images, and"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           17,
           "I + T I + T I + T I + T I + T I + T I + T I + T I + T I + T  T T T T T T T T T T  ✓ ✓ ✓ ✓  9K 17K 40"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           18,
           "I I I I + T  ✓ ✓ ✓  100K - - -  1000 1000 1000 1000  1000 1000 1000 1000  apple”) with the full imag"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           19,
           "4  (1)  Manuscript  N24News  ScienceQA  Country211  MSCOCO  RefCOCO  MSCOCO_t2i  VisDial  OK-VQA  Re"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           20,
           "Given a pretrained VLM, we feed query and target into it to obtain the query and target embeddings ("
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           21,
           "A bottleneck lies in the GPU memory that limits us from increasing the batch size and the number of "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           22,
           "=  (cid:88)  ˆQj∈Q  (cid:88)  qi∈ ˆQj  ui  ∂f(qi) ∂Θ  5  (2)  (3)  Image EncoderVLM2Vec(Query)  Proj"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           23,
           "4 EXPERIMENTS  In this section, we adopt Phi-3.5-V and LLaVA-1.6 as the backbone VLMs, with training"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           24,
           "4.1 BASELINES  Four groups of baselines are reported in this study.  CLIP-family: We utilize vision/"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           25,
           "MagicLens: MagicLens (Zhang et al., 2024) is a self-supervised image retrieval model capable of hand"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           26,
           "6  Manuscript  For all our baselines, we first use their original versions. Additionally, we have fi"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           27,
           "IND  OOD Overall  # of datasets →  10  10  12  4  20  16  36  Baseline Models (No Fine-tuning on MME"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           28,
           "38.7 25.1 38.0 40.2 40.4 41.7 11.5 23.7  37.8 25.2 34.8 39.7 42.8 44.7 13.3 27.8  Baseline Models (F"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           29,
           "72.3 79.5 64.0 86.1 +20.8 +22.0  62.8 66.5 61.0 67.5 +20.4 +17.0  47.4 52.0 47.5 57.1 +15.4 +14.0  5"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           30,
           "From Table 2, the best variant of VLM2VEC leverages LLaVA-1.6, is trained with LoRA, and pro- cesses"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           31,
           "Compared to other baseline models, with or without fine-tuning on MMEB training data, our model demo"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           32,
           "7  Manuscript  4.3 RESULT ANALYSIS  To train an effective and generalizable multimodal embedding, va"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           33,
           "Meta-Task Average Score  Average Score  Classification VQA Retrieval Grounding  IND OOD Overall  # o"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           34,
           "52.0 58.4 58.2 50.8 53.4  4.3.2 TRAINING PARAMETERS  During our experiments, we identified three key"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           35,
           "8  Manuscript  4.3.3 META-TASK GENERALIZATION  We have demonstrated that VLM2VEC has the potential t"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           36,
           "Figure 5 illustrates the generalizability of these three models on unseen meta-tasks. We could ob- s"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           37,
           "30  40  60  40  VLM2VecCLS  30  10  VLM2VecRET  Visual Grounding  10  50  10  40  Figure 5: The figu"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           38,
           "4.3.4  IMPACT OF INSTRUCTIONS  Previous studies have shown the influence of instructions on addressi"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           39,
           "9  Manuscript  Table 4: Comparison of CLIP and our VLM2VEC with and without task-specific instructio"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           40,
           "embeddings for specific tasks. With the rise of pretrained language models, efforts have shifted to-"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           41,
           "5.2 MULTIMODAL EMBEDDINGS  Multimodal embeddings have long been a significant research challenge. Ea"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           42,
           "Most research on universal multimodal embeddings involves fine-tuning models like CLIP or BLIP, typi"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           43,
           "For multimodal retrieval, several benchmarks have been introduced to evaluate model performance acro"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           44,
           "Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. SemEval-2012 task 6: A pilot on sema"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           45,
           "Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenba"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           46,
           "Yingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk. Webqa: Mul"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           47,
           "Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos´e MF Moura, Devi Parikh, an"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           48,
           "Luyu Gao, Yunyi Zhang, Jiawei Han, and Jamie Callan. Scaling deep contrastive learning batch size  u"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           49,
           "Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyl"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           50,
           "Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandelwal, Mandar Joshi, Kenton Lee, Kristina Toutanova, an"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           51,
           "Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           52,
           "Ryan Kiros, Yukun Zhu, Russ R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sa"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           53,
           "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinr"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           54,
           "Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards general te"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           55,
           "makes good in-context examples for gpt-3? DeeLIO 2022, pp. 100, 2022.  Siqi Liu, Weixi Feng, Tsu-jui"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           56,
           "Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, and Jimmy Lin. Unifying multimodal  retrieval "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           57,
           "Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual In Proceedin"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           58,
           "Rui Meng, Ye Liu, Shafiq Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. Sfr-embedding-2: Advance"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           59,
           "bedding benchmark. Association for Computational Linguistics, pp. 2014–2037, 2023.  Tri Nguyen, Mir "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           60,
           "Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svet- lana La"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           61,
           "Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learni"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           62,
           "Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig, and Aditi Raghunathan. Rep-  etit"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           63,
           "Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Ma- jumder, and"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           64,
           "ple visual language model pretraining with weak supervision. Learning Representations, 2022b.  Cong "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           65,
           "Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Con"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           66,
           "Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answer- ing i"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           67,
           "ImageNet-R (Hendrycks et al., 2021a) The dataset contains set of images labeled with ImageNet labels"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           68,
           "ObjectNet (Barbu et al., 2019) The dataset is a crowd-sourced test set of 50K images featuring objec"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           69,
           "InfographicsVQA (Mathew et al., 2022) The dataset comprises a diverse collection of infographics acc"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           70,
           "VizWiz (Gurari et al., 2018) The dataset originates from a natural visual question answering sce- na"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           71,
           "CIRR (Liu et al., 2021) The dataset is designed for the task of composed image retrieval. It consist"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           72,
           "WebQA (Chang et al., 2022) The dataset is a multihop, multimodal QA dataset that requires re- trievi"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           73,
           "EDIS (Liu et al., 2023) The dataset is a cross-modal image search in the news domain. This dataset c"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           74,
           "A.1.4 VISUAL GROUNDING  There are a total of 4 datasets for visual grounding tasks.  MSCOCO (Lin et "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           75,
           "Visual7W-pointing (Zhu et al., 2016) The dataset establishes a semantic link between textual de- scr"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           76,
           "20  Manuscript  Table 5: We compare the performance of VLM2VEC using different numbers of candidates"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           77,
           "CLIP OpenCLIP SigLIP BLIP2 MagicLens E5-V UniIR VLM2VEC  Classification (10 tasks) ImageNet-1K N24Ne"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           78,
           "VQA (10 tasks) OK-VQA A-OKVQA DocVQA InfographicsVQA ChartQA Visual7W ScienceQA VizWiz GQA TextVQA A"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           79,
           "Retrieval (12 tasks) VisDial CIRR VisualNews t2i VisualNews i2t MSCOCO t2i MSCOCO i2t NIGHTS WebQA F"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           80,
           "42.2 51.3 74.3 76.8 68.5 72.1 66.2 89.6 40.2 12.2 69.4 79.2 61.8  Visual Grounding (4 tasks) MSCOCO "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           81,
           "69.0 54.4 52.0 30.7 34.8 49.8 42.1 43.0 61.2 62.0 49.9  80.9 49.9 75.4 80.0 75.7 73.1 65.5 87.6 16.2"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           82,
           "Style    Classification  VOC2007 et al., 2014)  (Everingham  Identify the object shown in the image."
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           83,
           "DocVQA (Mathew et al., 2021)  Represent the given image with the following question. What is name of"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           84,
           "Category  Dataset  Query Text  Query Image  Target Text  Target Image  VisDial (Das et al., 2017)  R"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           85,
           "Represent the given Wikipedia im- age with related text information. Hays County Courthouse (2018), "
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           86,
           "Category  Dataset  Query Text  Query Image  Target Text  Target Image  CIRR (Liu et al., 2021)  Give"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           87,
           "image- Retrieve provides description evidence for the question of this image. What is the name of th"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           88,
           "Represent the given cropped image of the object.  Grounding  RefCOCO (Kazemzadeh et al., 2014)  Sele"
          ],
          [
           "../data/arxiv/pdfs/2410.05160v3.pdf",
           89,
           "Model  image retrieval  text retrieval  R@1 R@5 R@10 R@1 R@5 R@10  OpenAI CLIP-B/16 Open CLIP-B/16 E"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           0,
           "5 2 0 2  r a  M 6  ]  V C . s c [  1 v 7 8 9 3 0 . 3 0 5 2 : v i X r a  RetinalGPT: A Retinal Clinic"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           1,
           "Abstract. Recently, Multimodal Large Language Models (MLLMs) have gained significant attention for t"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           2,
           ". Be- yond disease diagnosis, RetinalGPT features quantitative analyses and lesion localization, rep"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           3,
           "Keywords: Foundation Model · Vision Language Model · Multimodal · Conversational AI assistants  Thes"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           4,
           "Vision-language models (VLMs) integrate visual and textual data, enabling computers to process and a"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           5,
           "However, existing medical-domain VLMs still suffer from certain limitations when it comes to handlin"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           6,
           "Title Suppressed Due to Excessive Length  Fig. 1. Top: the process of obtaining data. Bottom: The co"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           7,
           "3  4  Zhu. Wenhui et al."
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           8,
           "vessels, vessel density, branch angle, curvature tortuosity, as well as other mor- phological charac"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           9,
           ". Retinal Concepts Alignment Data. For each image, we generate single- round instruction-following d"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           10,
           "Human: Xq <STOP> Assistant: XmXd <STOP>"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           11,
           "The generated question Xq is designed to provide a concise and generic in- quiry about information o"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           12,
           ". Additionally, we designed specific prompts to guide GPT-4 [15] in generating targeted question-ans"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           13,
           "Title Suppressed Due to Excessive Length  38K retina fundus images as the Retinal Concepts Tuning (3"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           14,
           "2.2 Training Strategies  We adopted the architecture employed by LLaVA and LLaVA-Med for our MLLM wh"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           15,
           "5  6  Zhu. Wenhui et al."
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           16,
           "retinal specific domain knowledge, it is trained with the 38k RCT QA clinical preference instruction"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           17,
           "3 Experiment  3.1 Evaluation Datasets  In order to enhance the generalization of our model, we colle"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           18,
           "3.2 Multi-Disease Abnormal Detection  To evaluate the effectiveness of abnormality detection, we sel"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           19,
           "54.17  56.43  55.38  65.15  LLaVA-med  47.00  55.57  49.01  53.33  57.14  48.92  53.09  GPT-4o  54.3"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           20,
           "13.85  55.41  44.16  31.17  7  8  Zhu. Wenhui et al.  lar metrics for specific regions upon further "
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           21,
           "Title Suppressed Due to Excessive Length  4 Conclusion  In this paper, we introduce RetinalGPT, a ML"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           22,
           "2. Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann,"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           23,
           "3. Cheung, C.Y., Ran, A.R., Wang, S., Chan, V.T.T., Sham, K., Hilal, S., Venke- tasubramanian, N., C"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           24,
           ".1016/S2589-7500(22)00169-8, https://doi. org/10.1016/S2589-7500(22)00169-8"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           25,
           "4. Dong, X., Vasa, V.K., Zhu, W., Qiu, P., Chen, X., Su, Y., Xiong, Y., Yang, Z., Chen, Y., Wang, Y."
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           26,
           "6. Dumitrascu, O.M., Li, X., Zhu, W., Woodruff, B.K., Nikolova, S., Sobczak, J., Youssef, A., Saxena"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           27,
           "9. Grattafiori, A., Dubey, A., Jauhri, A., ...: The llama 3 herd of models (2024),  https://arxiv.or"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           28,
           "12. Li, N., Li, T., Hu, C., Wang, K., Kang, H.: A benchmark of ocular disease intelligent recognitio"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           29,
           "16. Pachade, S., Porwal, P., Thulkar, D., Kokare, M., Deshmukh, G., Sahasrabuddhe, V., Giancardo, L."
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           30,
           "19. Vasa, V.K., Qiu, P., Zhu, W., Xiong, Y., Dumitrascu, O., Wang, Y.: Context- aware optimal transp"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           31,
           "22. Zhang, K., Zhou, R., Adhikarla, E., Yan, Z., Liu, Y., Yu, J., Liu, Z., Chen, X., Davison, B.D., "
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           32,
           "23. Zhang, S., Xu, Y., Usuyama, N., Bagga, J., Tinn, R., Preston, S., Rao, R., Wei, M., Valluri, N.,"
          ],
          [
           "../data/arxiv/pdfs/2503.03987v1.pdf",
           33,
           "25. Zhou, Y., Wagner, S.K., Chia, M.A., Zhao, A., Xu, M., Struyven, R., Alexan- der, D.C., Keane, P."
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           0,
           "low-level actionImagination  VLM  VLM  (b) agent with world model (a) VLM-based agent(d) RIG (Ours):"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           1,
           "5 2 0 2  Abstract  r a  Reasoning before action and imagining potential outcomes (i.e., world models"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           2,
           "M 1 3  ] I  A . s c [  1 v 8 8 3 4 2 . 3 0 5 2 : v i X r a  Figure 1. Comparison between conventiona"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           3,
           "1. Introduction  To navigate the complexities of open-world environments, two quintessential human f"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           4,
           "We develop RIG by adopting a progressive data collec- tion strategy because existing datasets typica"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           5,
           "To further leverage visual imagination in reasoning to further improve the robustness of the policy,"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           6,
           "We extensively evaluate RIG in the diverse, open-world Minecraft environment. Experimental results s"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           7,
           "Embodied Agents in Minecraft. Minecraft presents a sig- nificantly open-ended and complex environmen"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           8,
           ". Learning robust world models is essential for embodied agents to effec- tively plan and act within"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           9,
           ". However, these datasets lack the interleaved action and reasoning trajectories required for traini"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           10,
           "to real-world embodied scenarios. Generalist policies like GATO [29] and RT-1 [4] demonstrate multit"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           11,
           "(Y,A,P) = F(X), X = {xIMG,xTXT}.  The model is trained in an end-to-end manner using only cross-entr"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           12,
           "(1)  (2)  3  add reasoning contents before each action (S2). The details are as below: • S0 (Refined"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           13,
           "S2 (Vision-Reasoning, 200K): To integrate reasoning in the original trajectories, we employ GPT-4o a"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           14,
           "3.3. Lookahead Reasoning  Although RIG-basic demonstrates strong baseline perfor- mance, the reasoni"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           15,
           "Reasoner  S0: Refined MineRLS1: Vision-ActionS2: Vision-ReasoningS4: Temporal Alignment  Action Text"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           16,
           "enc  enc  datasets from stages 3 and 4 produces RIG-lookahead, a model that performs reasoning condi"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           17,
           "i+1) F←− (Xi,Pi+1,Yi+1,...,Pi+n,Yi+n). (5) This lookahead mechanism enables internal review and corr"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           18,
           "We conduct comprehensive experiments to validate the ef- fectiveness of RIG across diverse tasks, fo"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           19,
           "bodied scenarios.  4.1. Inplementation Details  RIG is initialized from the pretrained Janus-1.4B [8"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           20,
           "4.2. Main Results  We first evaluate the data efficiency of RIG against prior ap- proaches (VPT [2],"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           21,
           "Dreamer: 2101h (139h + VPT), STEVE-1 and Jarvis-1: nearly 2000h). Performance in Embodied Tasks. As "
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           22,
           ". 5, RIG-lookahead demonstrates superior reasoning capabilities, achieving a Reasoning score of 7.3,"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           23,
           "4.3. Analysis of Scalability  We evaluate the scalability of RIG along three key di- mensions, train"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           24,
           "training data volume. Increasing training data from 10% to 100% dramatically improves performance, e"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           25,
           "4.4 41.8 37.8 37.9 88.7  8.4 30.3 34.6 44.1 79.6  +0.0 +21.9 +26.2 +35.7 +71.2  Tool (ID 0–4)  0 1 2"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           26,
           "+0.0 +22.7 +16.0 +80.8 +81.5  Table 1. Ablation study on embodied tasks under different capability s"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           27,
           "6.1 7.3  4  ✓  ✓  ✓  ✓  77.6  18.4  9.6  8.1  8.5  Table 2. Ablation study on Generation, Understand"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           28,
           "patterns. However, variance, illustrated by shaded areas, tends to increase with iterations, reflect"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           29,
           "7  gesting accumulated prediction errors or hallucinations be- come more prominent. For accuracy met"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           30,
           "Effect of Generation. Incorporating visual generation sig- nificantly boosts performance, especially"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           31,
           ".7 and accuracy by (1) re- +80.8. These gains arise from two key effects: ducing redundant actions, "
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           32,
           ". 3, self-reviewing enables the agent to antici- pate and reason over imagined future states, refini"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           33,
           "8  Action, Generation, Basic Reasoning and Lookahead Rea- soning leads to the most robust performanc"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           34,
           "Effect of Reasoning. Adding reasoning capabilities (ID 2 and ID 3) improves understanding and enviro"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           35,
           "Effect of Synergizing Reasoning and Imagination. By jointly optimizing generation, reasoning, and re"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           36,
           "5. Conclusion  This paper introduces RIG, an end-to-end Generalist pol- icy that integrates Reasonin"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           37,
           "Data Distribution (Appendix A.3) analyzes the diversity of embodied tasks within our datasets, and i"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           38,
           "Multi-Modal  9  21K QA benchmark, covering survival, crafting, entity understanding, and more.  Mult"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           39,
           "A.1. Inference Pipeline  As illustrated in Fig. A1, RIG follows a multimodal au- toregressive genera"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           40,
           "Dream FlowP0P1Pn  You are in a forest biome surrounded by a mix of oak and birch trees. The terrain "
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           41,
           "Figure A2. Training pipeline of RIG. S0/S1 pretrain the model by aligning real and imagined flows. S"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           42,
           "A.3. Data Distribution  Fig. A4 visualizes the task distribution across our training datasets, which"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           43,
           "10  Method  Vision Encoder  Parameters  Vision Quality (Gen.) MM Quality (Und.)  Evaluations  Autore"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           44,
           "0.68 0.68 0.68  0.15 -0.01 0.02  POPE, VQAv2 (84.5, 74.7) - -  Table A1. Comparison of various unifi"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           45,
           "6.649 4.123 6.077 4.564 12.252 -  27.21 28.93 27.06 30.84 29.91 24.16  29.62 30.28 29.81 31.73 30.69"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           46,
           "Voyager [33]  STEVE [20]  RIG (Ours)  Demos  Videos  None  Videos  None  None  None  Videos  Videos "
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           47,
           "A.5. Tokenizer and Base Model Selection  We adopt LlamaGen 16×16 VQ tokenizer and Janus-1.4B as our "
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           48,
           "Fig. A5 further compares RIG-lookahead with GPT-4o  11  rFID↓  2.865 0.502 1.113 0.261 5.953 -  &  i"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           49,
           "Resource Management: Gathering, mining, and inven- tory use.  Crafting and Construction: Recipes and"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           50,
           "12  A.9. Environment Details  We use Minecraft as the testbed for embodied agents due to its open-en"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           51,
           "For the camera action, which is originally a 2D continuous vector [a,b] representing pitch and yaw i"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           52,
           "get close to the trunk and start attacking.Prediction:• Next action:  Cracks will appear on the trun"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           53,
           "CorrectWrong  Gen IMGNext IMGCurrent IMG  Minecraft)• Player position remains unchanged•  Figure A5."
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           54,
           "Input Format:  Task: Given current task like “build a tower” • Previous Action: e.g., camera:[0,10] "
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           55,
           "{  \"from\": \"human\", \"value\": \"<image>\\n <Imagine:> Please make reasoning of action... task: build a "
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           56,
           "[2] Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Hough"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           57,
           "[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvi"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           58,
           "[8] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong R"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           59,
           "[12] Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaim"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           60,
           "[16] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains th"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           61,
           "[21] Jessy Lin, Yuqing Du, Olivia Watkins, Danijar Hafner, Pieter Abbeel, Dan Klein, and Anca Dragan"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           62,
           "[26] Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer S"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           63,
           "[30] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon S"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           64,
           "[33] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and An"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           65,
           "[36] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and se"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           66,
           "[39] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           67,
           "Zongqing Lu. with imagination for creative tasks. arXiv:2312.02519, 2023. 1  [43] Zhonghan Zhao, Wen"
          ],
          [
           "../data/arxiv/pdfs/2503.24388v1.pdf",
           68,
           "[46] Sipeng Zheng, Jiazheng Liu, Yicheng Feng, and Zongqing Steve-eye: Equipping llm-based embodied "
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           0,
           "5 2 0 2  n a J  4 2  ]  G L . s c [  1 v 8 7 3 6 1 . 1 0 5 2 : v i X r a  Internal Activation Revisi"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           1,
           "Warning: This paper contains offensive content that may disturb some readers. Vision-language models"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           2,
           ".94%, 34.34%, 43.92%, and 52.98% on SafeBench, Safe- Unsafe, Unsafe, and MM-SafetyBench, respectivel"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           3,
           "Introduction Large language models (LLMs) have been further enhanced by adopting visual instruction "
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           4,
           "These authors contributed equally. †Corresponding author.  ical standards (Ouyang et al. 2022; Rafai"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           5,
           "To investigate this area, we examine the vulnerabilities of VLMs by analyzing the differences in int"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           6,
           "Figure 1: Computation flow at the transformer layer l, with head-level revision after head attention"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           7,
           "We propose the internal activation revision approach to enhance the safety of VLMs. This method outp"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           8,
           "internal states for a deeper understanding of its behav- ior (Zhu et al. 2021, 2024; Song et al. 202"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           9,
           ". Activation addition (Turner et al. 2023) involves adding a fixed vector to the activations of spec"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           10,
           "Why VLMs Are More Vulnerable?  Preliminary Before explaining the methodology, we briefly introduce t"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           11,
           "process continues through all subsequent layers, forming a sequence of vectors x0,...,xn. Finally, t"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           12,
           "Dataset To our best knowledge, existing unimodal and multimodal instruction datasets containing both"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           13,
           ". SafeBench (Gong et al. 2023) includes 500 harmful instructions across ten topics forbidden by Ope-"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           14,
           "(1)  Figure 2: 2D t-SNE visualization of internal activations from the 5th, 15th, and 31st layers. T"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           15,
           "Activation Visualization with t-SNE Figure 2 illustrates the distribution of activations of the last"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           16,
           "Figure 5: An example of Multi-Response.  our classification dataset denoted as {((x′)l,h,y)i}M i=1. "
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           17,
           "We randomly divide the dataset TextSetA into training and validation sets with a 4:1 ratio. A binary"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           18,
           ". The accuracy differ- ence indicates that the safety alignments of LLMs embedded within VLMs are no"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           19,
           "Activation Revision for Safety Enhancement Based on the above observations, we propose an activation"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           20,
           "x′ l = xl + O  H (cid:88)  (cid:16)  Atth  l (xl) + αθl,hrhead l,h  (cid:17)  h=1  = xl + O  H (cid:"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           21,
           "and rhead  l  ′′  )l,y)i}2×N  (Al)+ = {((x (Al)− = {((x  ′′  ′′  )l,y = 1)i}N )l,y = 0)i}N  i=1,  i="
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           22,
           "ScienceQA ACC (%) ↑  GQA ACC (%) ↑  B 7 - 5 . 1 V A V a L L    Vanilla Adashield MLLM-Protector Fine"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           23,
           "70.78 65.31 63.75 61.28 59.40 61.25 62.49 63.28 62.12 61.74 60.75 63.68  75.22 74.25 74.11 74.97 74."
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           24,
           "72.83 38.32 35.74 30.56 45.79 43.01 38.32 35.16 32.41 33.27 30.94 27.16  82.33 38.54 30.17 38.41 43."
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           25,
           "73.19 27.04 22.00 25.16 43.74 40.54 37.16 33.21 32.69 30.97 26.94 23.56  53.92 30.37 25.33 23.51 35."
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           26,
           "and negative activations. While MMS calculates the average activations of positive and negative samp"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           27,
           "Evaluation We evaluate our proposed method from two perspectives. For safety, we measure the attack "
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           28,
           "− ACCvanilla  j  (cid:17) ,  (cid:124)  (cid:123)(cid:122) Helpfulness Score  (cid:125)  where Dhelp"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           29,
           "(7)  l\\α  0.5  1.0  1.5  2.0  4 9 14 19 24 29 31  4.218 3.353 4.470 2.400 3.900 1.275 0.565  8.590 1"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           30,
           "Table 3: Composite scores of layer-level and head-level re- vision. l and α represent the layer and "
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           31,
           "(1) The head-level activation revision method using Multi-Response with MMS achieves the best perfor"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           32,
           "(2) Head-level revision achieves a better balance between safety and helpfulness than layer-level re"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           33,
           "(4) MMS outperforms PWD. When using Multi- Response, MMS outperforms PWD in both head-level and laye"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           34,
           "(1) With the same α, deeper layers lead to a greater im- pact across various tasks. ASR on SafeBench"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           35,
           "(3) Determination of optimal parameters. From Fig- ure 6, it is evident that revising activations at"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           36,
           "Comparing Tables 3(a) and 3(b), we observe that head- level revision typically yields higher composi"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           37,
           "(5) Computational cost. The optimal performance of our method relies on hyperparameter search. Howev"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           38,
           "adhere to principles of respect and dignity for all people. The inclusion of offensive materials, in"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           39,
           "References Azaria, A.; and Mitchell, T. 2023. The Internal State of an LLM Knows When It’s Lying. In"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           40,
           ". arXiv preprint arXiv:2310.09478. Chen, Z.; Wu, J.; Wang, W.; Su, W.; Chen, G.; Xing, S.; In- Zhong"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           41,
           ". Computer Vision Foundation / IEEE. Ji, J.; Qiu, T.; Chen, B.; Zhang, B.; Lou, H.; Wang, K.; Duan, "
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           42,
           "Kim, B.; Wattenberg, M.; Gilmer, J.; Cai, C. J.; Wexler, J.; Vi´egas, F. B.; and Sayres, R. 2018. In"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           43,
           ". In Al-Onaizan, Y.; Bansal, M.; and Chen, Y.-N., eds., Findings of the Association for Com- putatio"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           44,
           ". In European Confer- ence on Computer Vision, 386–403. Springer. Liu, X.; Zhu, Y.; Lan, Y.; Yang, C"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           45,
           ".; Almeida, D.; Wainwright, C.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; Schulman, J"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           46,
           "break Attacks. In Dinkar, T.; Attanasio, G.; Curry, A. C.; Konstas, I.; Hovy, D.; and Rieser, V., ed"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           47,
           ". In Oh, A.; Naumann, T.; Globerson, A.; Saenko, K.; Hardt, M.; and Levine, S., eds., Advances in Ne"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           48,
           ". In Duh, K.; Gomez, H.; and Bethard, S., eds., Proceedings of the 2024 Conference of the North Amer"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           49,
           ". Florence, Italy: Association for Com- putational Linguistics. Turner, A.; Thiergart, L.; Udell, D."
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           50,
           "V. N.; and Garnett, R., eds., Advances in Neural Informa- tion Processing Systems 30: Annual Confere"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           51,
           ".12030. Zhu, D.; Chen, D.; Li, Q.; Chen, Z.; Ma, L.; Grossklags, J.; and Fritz, M. 2024. PoLLMgraph:"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           52,
           "Appendix  Visulization of layer activations We use t-SNE to visualize the extracted layer activation"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           53,
           "Figure 7: 2D layer activations using t-SNE. The blue and red dots represent positive and negative sa"
          ],
          [
           "../data/arxiv/pdfs/2501.16378v1.pdf",
           54,
           "The impact of head utilization ratio on model perfor- mance. Based on the observations in Figure 8, "
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           0,
           "5 2 0 2  b e F 8 2  ] I  A . s c [  1 v 0 8 7 0 2 . 2 0 5 2 : v i X r a  MedHallTune: An Instruction"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           1,
           "Abstract. The increasing use of vision-language models (VLMs) in health- care applications presents "
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           2,
           "Keywords: Medical hallucination · Hallucination mitigation · Large vision-language models.  1  Intro"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           3,
           "and visual data interpretation. While these models demonstrate great potential in various applicatio"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           4,
           "Despite growing attention to hallucinations in VLMs, there is a significant gap in research focusing"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           5,
           "To address these issues, we formulate a medical hallucination tuning dataset, namely MedHallTune, de"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           6,
           "reliability and trustworthiness of VLMs in real-world healthcare applications. To sum up, our contri"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           7,
           "2 MedHallTune  In this section, we present MedHallTune, designed to mitigate and evaluate hallucinat"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           8,
           "4 https://pubmed.ncbi.nlm.nih.gov/  3  4  Y. Qiao et al.  Table 1. Comparison of datasets in medical"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           9,
           "2.1 Data Construction  Hallucination Instruction Generation. Inspired by [16], we classify negative "
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           10,
           "2.2 Quality Control  To prevent the introduction of contradictory descriptive information that may a"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           11,
           "2.3 Data Statistics  We sample over 100,000 annotated images from existing datasets [13,4] sourced f"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           12,
           "This evaluation comprises four critical metrics as detailed in Fig. 4: Clinical Ac- curacy, Clinical"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           13,
           ". For example, in response to the incorrect answer \"Yes, the mass appears to be smooth and well-defi"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           14,
           "3 Experiments  We conduct comprehensive evaluation of state-of-the-art VLVMs, including both those t"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           15,
           "P&N ↑  P&N ↑  GPT-4o MiniGPT-4 Janus-Pro mPLUG-Owl2 Qwen-VL-Chat ⊕MedHallTune InternVL-v1.5 ⊕MedHall"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           16,
           "0.33&0.27 0.88&0.87 7.56&5.67 0.22&0.22 0.85&0.85 5.90&4.15 0.38&0.21 0.91&0.88 7.42&5.21 0.35&0.24 "
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           17,
           "3.91&3.23 4.69& 3.00 4.79&3.12 Med-Flamingo 3.69&4.03 4.17&4.91 3.81&4.41 RadFM 7.08&4.88 6.55&4.49 "
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           18,
           "[21], (9) HuatuoGPT-Vision-7b [4], (10) HealthGPT-M3 [14], and (11) LLaVA- Med-v1.5-7b [13]. Each mo"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           19,
           ". Furthermore, the MedHallTune tuned model achieves the highest Bert Score (0.93&0.90) and METEOR Sc"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           20,
           "Zero-shot Analysis. We evaluate three selected VLMs on several biomedi- cal VQA datasets without fur"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           21,
           "7  8  Y. Qiao et al.  Table 3. Zero-shot performance of different methods on medical VQA datasets.  "
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           22,
           "49.81 52.31 49.49 56.45 47.66 50.82  Fig. 5. Ablation study comparing model performance across train"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           23,
           "Fig. 5(b)(c)(d)(e) illustrate the increasing performance gains achieved by training on 25%, 50%, 75%"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           24,
           "References  1. Agarwal, V., Jin, Y., Chandra, M., Choudhury, M.D., Kumar, S., Sastry, N.R.: Medhalu:"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           25,
           "4. Chen, J., Gui, C., Ouyang, R., Gao, A., Chen, S., Chen, G., Wang, X., Cai, Z., Ji, K., Wan, X., e"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           26,
           "7. Gu, Z., Yin, C., Liu, F., Zhang, P.: Medvh: Towards systematic evalua- tion of hallucination for "
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           27,
           "10. Hu, Y., Li, T., Lu, Q., Shao, W., He, J., Qiao, Y., Luo, P.: Omnimedvqa: A new large-scale compr"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           28,
           "13. Li, C., Wong, C., Zhang, S., Usuyama, N., Liu, H., Yang, J., Naumann, T., Poon, H., Gao, J.: Lla"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           29,
           "14. Lin, T., Zhang, W., Li, S., Yuan, Y., Yu, B., Li, H., He, W., Jiang, H., Li, M., Song, X., Tang,"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           30,
           "16. Liu, F., Lin, K., Li, L., Wang, J., Yacoob, Y., Wang, L.: Mitigating hallucination in large mult"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           31,
           "19. Thirunavukarasu, A.J., Ting, D.S.J., Elangovan, K., Gutierrez, L., Tan, T.F., Ting, D.S.W.: Larg"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           32,
           "22. Xia, P., Chen, Z., Tian, J., Gong, Y., Hou, R., Xu, Y., Wu, Z., Fan, Z., Zhou, Y., Zhu, K., et a"
          ],
          [
           "../data/arxiv/pdfs/2502.20780v1.pdf",
           33,
           "25. Zhang, X., Wu, C., Zhao, Z., Lin, W., Zhang, Y., Wang, Y., Xie, W.: Pmc-vqa: Visual instruction "
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           0,
           "5 2 0 2  b e F 6 2  ]  V C . s c [  2 v 6 1 1 8 1 . 2 0 5 2 : v i X r a  Bayesian Optimization for C"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           1,
           "2University of Manchester  4Tsinghua University  5FancyTech  3University of Washington  6University "
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           2,
           "1  Introduction  In the rapidly evolving field of visual content ma- nipulation, image editing has g"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           3,
           "et al., 2023), have introduced methods to fine-tune and guide transformations in existing images rat"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           4,
           "However, despite these advancements, existing methods face several critical challenges. First, most "
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           5,
           "To address these challenges, we propose BayesGenie: a novel framework that achieves precise localize"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           6,
           "In summary, our contributions are:  We propose BayesGenie, a novel image edit- ing framework that en"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           7,
           "1.1 Related Work  Image-to- Image-to-Image Generation Models image translation models have become in"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           8,
           "The Instruct Pix2Pix framework represents a sig- nificant advancement in the field of image edit- in"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           9,
           "However, a common limitation persists across most state-of-the-art approaches: they typically rely o"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           10,
           "Bayesian learning Black-box functions are fre- quently encountered across various domains, par- ticu"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           11,
           "2 Methodology  Our system architecture integrates LLMs and Bayesian optimization for image editing ("
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           12,
           "2.1 Dynamic Prompt Optimization with  LLMs  In our approach, the prompt is dynamically opti- mized t"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           13,
           "such that, the generated image Igen(s∗ T) pro- duced by the score network ˜eθ(zt,cI,cT) satis- fies "
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           14,
           "˜eθ(zt,cI,cT) = eθ(zt,∅,∅)  + sI · (eθ(zt,cI,∅) − eθ(zt,∅,∅)) + sT · (eθ(zt,cI,cT) − eθ(zt,cI,∅)) (2"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           15,
           "Specifically, the objective function of Bayesian  optimization is  f(s) = f([sI,sT]),  This problem "
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           16,
           "f(s) ∼ GP(µ(s),k(s,s′))  where µ(s) is the mean function and k(s,s′) is the kernel function that def"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           17,
           "5. Iterate: Repeat steps 2-4 until reaching the maximum number of iterations or achieving convergenc"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           18,
           "ExcessiveModificationInsufficientModification Requirement  Normalization  Figure 3: An illustration "
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           19,
           "Optimized Ten Times  Optimized Once  Optimized fifteen Times  Optimized Twenty Times  Original  Opti"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           20,
           "Removing objects from images  Modifying existing objects  LLM Scoring: To assess the nuanced visual "
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           21,
           "3.3 Results  In our experiments, BayesGenie effectively han- dled the three key image editing tasks,"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           22,
           "Similarity Detection For similarity detection, we primarily relied on the CLIP model’s capabil- ity "
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           23,
           "Human Evaluation: We also conducted a hu- man voting phase to gather subjective evaluations. Partici"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           24,
           "In terms of iterations, more iterations represent a finer exploration of the solution space, allowin"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           25,
           "Figure 7: Comparison of CLIP scores, GPT scores, and Human evaluation results across different model"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           26,
           "5 Conclusion  Our work introduces BayesGenie, a novel and model-agnostic framework that combines Bay"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           27,
           "Marios Aristodemou, Xiaolan Liu, Yuan Wang, Kon- stantinos G Kyriakopoulos, Sangarapillai Lam- botha"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           28,
           "Chengkun Cai, Xu Zhao, Yucheng Du, Haoliang Liu, and Lei Li. 2024a. T2 of thoughts: Temperature tree"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           29,
           "Yunchuan Guan, Yu Liu, Ke Zhou, Zhiqi Shen, Serge Belongie, Jenq-Neng Hwang, and Lei Li. 2025. Learn"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           30,
           "Zhongyu Jiang, Zhuoran Zhou, Lei Li, Wenhao Chai, Cheng-Yen Yang, and Jenq-Neng Hwang. 2024. Back to"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           31,
           "Lei Li, Sen Jia, Wang Jianhao, Zhongyu Jiang, Feng Zhou, Ju Dai, Tianfang Zhang, Wu Zongkai, and Jen"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           32,
           "Prasad Narasimha Sarappadi M Shetty K Raghavendra. 2022. Transfer learning with pix2pix gan for gene"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           33,
           "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High- resolu"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           34,
           "10  Aäron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. 2016. Pixel recurrent neural networ"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           35,
           "Replacethewoodenbenchwithtwoelegantwoodenchairsfacingthegarden.Maintainthewindingpath,diverseplants,"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           36,
           "A Examples of Prompt Generation  In this section, we present examples of input im- ages alongside th"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           37,
           "The first one is the original Image.  The second one is the generated Image.  Please evaluate whethe"
          ],
          [
           "../data/arxiv/pdfs/2502.18116v2.pdf",
           38,
           "C Ethics and Participant Consent  This study was conducted following the ethical guidelines establis"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           0,
           "4 2 0 2  t c O 2  ]  V C . s c [  1 v 2 1 9 1 0 . 0 1 4 2 : v i X r a  DnD-Transformer  A SPARK OF V"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           1,
           "1  DnD-Transformer  ABSTRACT  This work tackles the information loss bottleneck of vector-quantizati"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           2,
           "1  INTRODUCTION  The field of autoregressive (AR) image generation is experiencing a resurgence of i"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           3,
           "A review of the development history of AR image generation approaches reveals significant efforts fo"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           4,
           "1) Information loss inherent in the quantization process. Quantization, specifically in VQVAE, intro"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           5,
           "O  …………  28-128-228-(N-1)28-N  2  …  +  28-1  27  28-N  Transformer Block O-2  O  Transformer Block "
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           6,
           "We draw inspiration from the Residual Quantization method (Lee et al., 2022b), which provides a new "
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           7,
           "The remaining problem is how to predict the d times more tokens effectively. We propose the DnD- Tra"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           8,
           "4. A spark of vision-language intelligence for the first time, enabling unconditional rich- text ima"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           9,
           "(H/f) × (W/f) × logN H × W × 3 × log256  =  logN 24f2  A typical configuration (N=8192, f=16) result"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           10,
           "2.2  IMAGES’ 2D DECOMPOSITION AND QUANTIZATION  As pointed out by Equation 1, the information compre"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           11,
           "rd = rd−1 − qd,  r0 = v  Consequently, the sum of the residual codes (cid:80)d i=1 qi is expected to"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           12,
           "ICR(N,f,d) = d ×  (H/f) × (W/f) × logN H × W × 3 × log256  = d ×  logN 24f2  4  (1)  (2)  (3)  DnD-T"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           13,
           "0.11 0.08 0.05 0.04  100% 100% 100% 100%  1 1† 2 8  0.15 0.00 0.50 0.80  0.73 0.00 0.81 0.83  0.14 0"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           14,
           "2.3 RECONSTRUCTION PERFORMANCE  We evaluate the reconstruction performance of our trained visual tok"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           15,
           "5  DnD-Transformer  Layerwise Code Usage of Different Visual Tokenizers  4  0.2  1  Max Depth (d)  0"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           16,
           "2.4 VQVAES CAN PERFECTLY RECONSTRUCT RICH-TEXT IMAGES  A prevalent criticism of VQVAE has been its a"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           17,
           "Experiments and Results. Two rich-text image datasets, Text-Image and arXiv-Image (details in Sectio"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           18,
           "3.1 DND-TRANSFORMER DESIGN  Figure 5 shows DnD-Transformer and its variants: Parallel and Vertical P"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           19,
           "3.2  IMPLEMENTATION DETAILS  As shown in the left part of Figure 5, the increment of DnD-Transformer"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           20,
           "4 EXPERIMENTS AND FINDINGS  4.1 TASKS AND DATASETS  Class-Conditional Image Generation. We conduct s"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           21,
           "1. Pure Text Images (Text-Image). The dataset is automatically rendered from a portion of English wi"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           22,
           "TextarXiv  Figure 6: Data examples in of the collected Text-Image and arXiv-Image image datasets.  M"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           23,
           "DnD-Transformer. We train two size of DnD-Transformers across our experiment, namely DnD- Transforme"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           24,
           "Implemented Baselines for Rich-Text Image Generation. We select multiple diffusion models as the bas"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           25,
           "4.3 RESULTS OF CLASS-CONDITIONAL IMAGE GENERATION  As demonstrated in Table 2, our DnD-Transformer s"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           26,
           "554M 10.94 4.88 400M 3.60 675M 2.27  −  101.0 158.7 247.7 278.2  0.69 − − 0.83  0.63 − − 0.57  AR-Re"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           27,
           "− − 0.51 0.56 0.49 0.54  AR-Implemented  LlamaGen-XXL (cfg=4) LlamaGen-XXL (cfg=2) DnD-Transformer-X"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           28,
           "0.89 0.83 0.89 0.83 0.82 0.80 0.89 0.85 0.83 0.80  0.35 0.49 0.42 0.56 0.56 0.57 0.42 0.54 0.58 0.59"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           29,
           "(b) Sampling PPLocr on Text-Image along training.  Figure 7: Curves during training.  4.4 RESULTS OF"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           30,
           "Generation Results on arXiv-Image. An 8-layer visual tokenizer and corresponding DnD- Transformer tr"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           31,
           "(b) Training Loss when trained on different domain datasets.  Figure 9: Analysis of code depths and "
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           32,
           "4.6 AR TRAINING LOSS FOR DIFFERENT DOMAINS ALIGN WITH INNER RANDOMNESS  Training loss for the same D"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           33,
           "Image Generation with VQVAE. The vector quantization (VQ) method has been pivotal in the development"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           34,
           "., 2024; Wang et al., 2024b) that integrate both understanding and autoregressive image generation c"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           35,
           "Rich-Text Image Generation. Despite recent significant progress in image generation, the task of ric"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           36,
           "., 2023; 2024). The full potential of rich-text image generation remains largely unexplored. Our met"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           37,
           "6 CONCLUSION  This paper investigated the limitations of autoregressive (AR) image generation method"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           38,
           "Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, M"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           39,
           "Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image t"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           40,
           "Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances  in "
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           41,
           "neural information processing systems, 33:6840–6851, 2020.  Jonathan Ho, Chitwan Saharia, William Ch"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           42,
           "13  DnD-Transformer  Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressi"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           43,
           "language, and multi-modal tasks.  Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khos"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           44,
           "tion: Vq-vae made simple. arXiv preprint arXiv:2309.15505, 2023.  OpenAI. Chatgpt. https://openai.co"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           45,
           "models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.  Aditya Ramesh, Mikhail Pavlo"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           46,
           "14  DnD-Transformer  Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Dent"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           47,
           "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee Lacroi"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           48,
           "neural information processing systems, 30, 2017.  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob U"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           49,
           "Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze "
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           50,
           "Wikipedia. URL https://en.wikipedia.org/wiki/Image_compression.  Jinheng Xie, Weijia Mao, Zechen Bai"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           51,
           "15  DnD-Transformer  Lijun Yu, Jos´e Lezama, Nitesh B Gundavarapu, Luca Versari, Kihyuk Sohn, David "
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           52,
           "A.1 STEP1: TRAIN THE VISUAL TOKENIZER AND TOKENIZE THE IMAGES  Images initially exist in the pixel-l"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           53,
           "A.2 STEP2: LEARN THE PRIOR DISTRIBUTION OF IMAGE TOKENS  Having tokenized the source images into dis"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           54,
           "Classifier-Free Guidance As a technique to enhance the visual quality and text-image alignment, clas"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           55,
           "16  (4)  DnD-Transformer  B TRAINING DETAILS OF VISUAL TOKENIZERS  We follow (Lee et al., 2022b) to "
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           56,
           "266.9 232.1 289.7 295.6  0.83 0.79 0.83 0.83  0.49 0.44 0.57 0.56  Table 3: Ablation of DnD-Transfor"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           57,
           "All transformer models were trained using settings similar to LlamaGen (Sun et al., 2024): a base le"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           58,
           "18  DnD-Transformer  Figure 13: Conditional generation comparisons between LlamaGen-XXL and DnD-Tran"
          ],
          [
           "../data/arxiv/pdfs/2410.01912v1.pdf",
           59,
           "Figure 17: Unconditional Generation examples of DnD-Transformer on Image-Text with tempera- ture=0.1"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           0,
           "4 2 0 2  t c O 8 2  ]  G L . s c [  1 v 0 8 4 1 2 . 0 1 4 2 : v i X r a  Preprint. Under review.  AI"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           1,
           "bhr54@cornell.edu, anmol@cs.cornell.edu, felipe.pacheco@cornell.edu {leg86,jy6,amf272}@cornell.edu, "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           2,
           "Trust and interpretability are crucial for the use of Artificial Intelligence (AI) in scientific res"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           3,
           ". Across these datasets, our method outperforms fully supervised models in low and full-labeled data"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           4,
           "1  Preprint. Under review.  1  INTRODUCTION  Until recently, meaningful interactions with AI models "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           5,
           "However, while this transformation has enabled AI to serve as a general-purpose assistant across a w"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           6,
           "Fortunately, the large context windows of LMMs allow for flexible specialization via in-context lear"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           7,
           "To address these challenges, we introduce AISciVision, a framework that adapts general-purpose LMMs "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           8,
           "We evaluate our method on three real-world scientific image classification datasets: detecting aquac"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           9,
           "AISciVision: You have the following tools available for analysis: SupervisedModelTool, ZoomOutTool, "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           10,
           "Figure 1: A schematic of our AISciVision framework, comprising of two components (1) Visual Retrieva"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           11,
           "3  Preprint. Under review.  AI agent. To our knowledge, AISciVision is among the first to apply such"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           12,
           "Multimodal models in low-labeled data regimes Large Multimodal Models (LMMs) like CLIP (Rad- ford et"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           13,
           ". On the other hand, Lu et al. (2022); Mall et al. (2024) find that zero- and few-shot capabilities "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           14,
           "Retrieval-Augmented Generation (RAG) Since Large Language Models (LLMs) suffer from halluci- nations"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           15,
           "Interactive AI Agents and tool-use In recent years, Large Language Models have enabled users to en- "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           16,
           ". Lu et al. (2024) introduce the “AI Scien- tist” that imitates the research process in the Machine "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           17,
           "For scientific research in physical, life, and climate sciences, not only is it important to obtain "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           18,
           "3.1 RETRIEVING RELEVANT IMAGES WITH VISUAL RAG (VISRAG)  To specialize a general-purpose LMM for sci"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           19,
           "On inference, we embed an input test image xtest to get embedding etest = ϕ(xtest). We then retrieve"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           20,
           "sim and x−  sim of the respective embeddings e+  sim and e−  3.2 DOMAIN-SPECIFIC INTERACTIVE TOOLS  "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           21,
           "For each image classification task, we define a set of tools T = {T1,...,TK} and provide their descr"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           22,
           "We design the initial system prompt to reflect the specified domain (see Appendix C for example tran"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           23,
           "4 EXPERIMENTS  We extensively evaluate our AISciVision framework on three image datasets from scient"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           24,
           "2. Eelgrass Wasting Disease Detection. Eelgrass (Zostera marina), essential for coastal ecosystems, "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           25,
           "7  Preprint. Under review.  AISciVision method We use the GPT-4o as the framework’s LMM, and attach "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           26,
           "0.71 0.60 0.82 0.81  0.66 0.17 0.72 0.73  0.79 0.54 0.88 0.89  0.78 0.60 0.80 0.84  0.72 0.17 0.74 0"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           27,
           "0.91 0.65 1.00 1.00  0.95 0.63 0.99 0.97  Table 1: Our AISciVision framework (GPT4o + VisRAG + Tools"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           28,
           "Baselines A key component of AISciVision is VisRAG, which retrieves the most similar positive and ne"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           29,
           "Ablation studies and tool efficacy We conduct ablation studies on the two components of AISciVision:"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           30,
           "2GPT4o generations are inherently random even after setting a seed, system fingerprint, the temperat"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           31,
           "0.83 0.92 0.90 0.95  0.93 0.97 0.96 0.98  0.77 0.89 0.86 0.92  0.99 0.99 1.00 1.00  0.91 0.97 0.96 0"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           32,
           "5 DEPLOYED APPLICATION  AISciVision is not just a conceptual framework, we have deployed it as a ful"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           33,
           "Eelgrass  25  0.00  75  ContrastTools  0.50  100  50  Pandown  0.25  50  Edge  75  0  Panright  0.25"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           34,
           "AUC  0.99 0.99 1.00 1.00  Preprint. Under review.  Figure 4: Screenshot of the AISciVision web appli"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           35,
           "6 DISCUSSION  Our AISciVision framework combines robust prediction capabilities, transparency, and a"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           36,
           "Limitations and Future Work While AISciVision offers significant benefits in providing transparent r"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           37,
           "REFERENCES  Abdelrahman Abdelhamed, Mahmoud Afifi, and Alec Go. What Do You See? Enhancing Zero-Shot"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           38,
           "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           39,
           "Di Chen, Yiwei Bai, Sebastian Ament, Wenting Zhao, Dan Guevarra, Lan Zhou, Bart Selman, R Bruce van "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           40,
           "Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan W"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           41,
           "org/abs/2312.11805.  Laura Greenstreet, Joshua Fan, Felipe Siqueira Pacheco, Yiwei Bai, Marta Eichem"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           42,
           "Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization throug"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           43,
           "Alexandre Lacoste, Nils Lehmann, Pau Rodriguez, Evan David Sherwin, Hannah Kerner, Bj¨orn L¨utjens, "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           44,
           "12  Preprint. Under review.  9459–9474. Curran Associates, Inc., 2020. URL https://proceedings.neuri"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           45,
           "Visual Instruction Tuning.  Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           46,
           "Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan M Krumholz, Jure Leskovec, Eric J T"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           47,
           "nected with Massive APIs. arXiv preprint arXiv:2305.15334, 2023.  Alec Radford, Jong Wook Kim, Chris"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           48,
           "13  Preprint. Under review.  Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lo"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           49,
           "David Soong, Sriram Sridhar, Han Si, Jan-Samuel Wagner, Ana Caroline Costa S´a, Christina Y Yu, Kubr"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           50,
           "Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima A"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           51,
           "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           52,
           "Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Richard James, Jure Leskovec, Percy Liang, Mike Le"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           53,
           "14  Preprint. Under review.  Cyril Zakka, Rohan Shad, Akash Chaurasia, Alex R. Dalal, Jennifer L. Ki"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           54,
           "15  Preprint. Under review.  A DOMAIN-SPECIFIC TOOLS  The tools developed for each dataset are desig"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           55,
           "We detail the full list of tools for each dataset in our AISciVision framework.  A.1 TOOLS FOR AQUAC"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           56,
           "ZoomOutToolRelative: Zooms out from the current view relative to the last image seen. • ZoomOutToolA"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           57,
           "PredictEelgrassWastingDiseaseTool: Predicts the probability of eelgrass wasting disease in the image"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           58,
           "IncreaseContrastTool: Increases the contrast of the image by 50%. This tool can be helpful when the "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           59,
           "0.62 0.00 0.62 0.58 0.58 0.71 0.75  0.61 0.00 0.86 0.80 0.71 0.89 0.94  0.46 0.00 0.50 0.50 0.62 0.6"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           60,
           "0.67 0.47 0.92 1.00 1.00 1.00 0.92  Table 3: We report values of precision and recall when all metho"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           61,
           "18  Preprint. Under review.  C.1 AQUACULTURE  Assistant:In the provided satellite image, several cha"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           62,
           "True Class: 1, LMM Prediction: 1 (Probability: 0.80), Supervised Prediction: 0 (Probability: 25.91)S"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           63,
           ". Use your knowledge and the tools provided to make an accurate classification of whether aquacultur"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           64,
           "19  Preprint. Under review.  Texture: The water surface appears relatively smooth compared to natura"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           65,
           "Assistant:In the provided satellite image, there are no visible aquaculture ponds. Here are the obse"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           66,
           "20  Preprint. Under review.  Here is the satellite image we need to classify as having an aquacultur"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           67,
           "Assistant:In the provided satellite image, we need to determine whether there are aquaculture ponds "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           68,
           ".(cid:127)Example without Aquaculture Pond: The second example showed a natural landscape with dense"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           69,
           "21  Preprint. Under review."
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           70,
           "Conclusion:Based on the geometric patterns, water color, surrounding features, and comparison with k"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           71,
           ". -PanDownToolAbsolute: Pans the view downwards relative to the original starting image. - PanLeftTo"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           72,
           ". Feel free to usemore tools if you need to increase your confidence.If you want to use a tool, expl"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           73,
           ".[PredictAquaculturePondTool]User:The probability of an aquaculture pond being present in this image"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           74,
           "22  Preprint. Under review."
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           75,
           "You have 3 rounds left. You may now choose another tool or indicate you're finished. Respond with ON"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           76,
           ". If you want to reference the last toolyou used, simply write its name without brackets. After choo"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           77,
           "Assistant:[PanDownToolRelative]{yes:60,no:40}I am choosing to pan down relative to the current view "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           78,
           "23  Preprint. Under review.  Assistant:[ZoomInToolRelative]{yes:80,no:20}I am choosing to zoom in fu"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           79,
           "The view has been panned down, revealing more of the area down of the previous view. New coordinates"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           80,
           "User:Based on your analysis and the tools used, please provide your final classification regarding t"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           81,
           ".2.Water Color: The water in these rectangular areas has a distinct color, different from the surrou"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           82,
           "25  Preprint. Under review.  C.2 EELGRASS  Assistant:This image shows eelgrass blades with dark, nec"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           83,
           "True Class: 1, LMM Prediction: 1 (Probability: 0.70), Supervised Prediction: 0 (Probability: 34.51)S"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           84,
           ".Use your knowledge and the tools provided to make an accurate classification.User:I will start by f"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           85,
           "26  Preprint. Under review.  Assistant:This image shows eelgrass blades, one of which has a dark, ne"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           86,
           "Based on the presence of the dark, necrotic lesion on one of the blades in this image, it is likely "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           87,
           ". - AdjustBrightnessTool: Tool Name: 'AdjustBrightnessTool' Description: Adjusts the brightness of t"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           88,
           ". Feel free to usemore tools if you need to increase your confidence.If you want to use a tool, expl"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           89,
           ".You have 3 rounds left. You may now choose another tool or indicate you're finished. Respond with O"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           90,
           "28  Preprint. Under review.  Assistant:  Assistant:{yes:80,no:20}The increased contrast has made the"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           91,
           "{yes:85,no:15}The sharpening has further clarified the edges and details of the dark lesion, making "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           92,
           ". Then, explain why you are picking this tool orfinishing, and how it relates to your current assess"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           93,
           "30  Preprint. Under review."
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           94,
           "User:Based on your analysis and the tools used, please provide your final classification regarding t"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           95,
           ".2.IncreaseContrastTool: This tool enhanced the visibility of the lesion, making it more prominent a"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           96,
           "31  Preprint. Under review.  C.3 SOLAR  Assistant:In the provided satellite image, several character"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           97,
           "True Class: 1, LMM Prediction: 1 (Probability: 0.99), Supervised Prediction: 1 (Probability: 98.95)S"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           98,
           "32  Preprint. Under review.  User:This is an example of a satellite image without solar panels. Desc"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           99,
           "Assistant:In the provided satellite image, several observations indicate the absence of solar panels"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           100,
           ". Describe what you see and compare and contrast it withthe previous two known examples. Explain you"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           101,
           "33  Preprint. Under review."
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           102,
           "Assistant:In the provided satellite image, several characteristics suggest the presence of solar pan"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           103,
           ".19.First Example: Panels also cover a significant portion of the rooftop.20.Second Example: Objects"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           104,
           "34  Preprint. Under review."
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           105,
           "Based on these observations, I classify the current image as containing solar panels.User:You have a"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           106,
           ". This can help in identifying the outlines of solar panels and potentialdefects or anomalies. - Inc"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           107,
           ".Remember, your goal is to accurately classify whether the image shows solar panels or not. If you d"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           108,
           ".You have 3 rounds left. You may now choose another tool or indicate you're finished. Respond with O"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           109,
           "35  Preprint. Under review.  Assistant:[EdgeDetectionTool] {yes:98.95,no:1.05}User:Edge detection ha"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           110,
           "how it relates to your current assessment of the image.Assistant:[SharpenTool] {yes:98.95,no:1.05}Us"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           111,
           "36  Preprint. Under review.  Assistant:[HistogramEqualizationTool] {yes:98.95,no:1.05}User:Histogram"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           112,
           "User:Based on your analysis and the tools used, please provide your final classification regarding t"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           113,
           "percentages must sum to 100%. To break ties, always assign at least 51% to one category. For example"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           114,
           ".95%) of solar panels being present, which aligns with the visual analysis.Final Classification: [Ye"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           115,
           "38  Preprint. Under review.  D ABLATION FAILURE CASE  In our analysis of the Eelgrass dataset, we no"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           116,
           "True Class: 1, LMM Prediction: 1 (Probability: 0.85), Supervised Prediction: 0 (Probability: 7.13)Sy"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           117,
           ".Use your knowledge and the tools provided to make an accurate classification.User:I will start by f"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           118,
           "Assistant:This image shows eelgrass blades with dark, necrotic lesions, which are indicative of eelg"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           119,
           "Assistant:In this image, the eelgrass blades exhibit some dark spots and streaks, particularly on th"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           120,
           "User:Based on your analysis and the tools used, please provide your final classification regarding t"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           121,
           ". The presence of these lesions, even though not as extensive as in the first example, suggests the "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           122,
           "42  Preprint. Under review.  D.2 GPT4O + TOOLS (INCORRECT PREDICTION)  Assistant:To classify this im"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           123,
           "True Class: 1, LMM Prediction: 0 (Probability: 0.20), Supervised Prediction: 0 (Probability: 7.13)Sy"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           124,
           ".Use your knowledge and the tools provided to make an accurate classification.User:Here is the image"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           125,
           "43  Preprint. Under review."
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           126,
           "Blotching: The dark streak on the left blade does not appear to have merged into larger blotches, bu"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           127,
           ". This tool can beuseful when the image appears too harsh or when you want to reduce the intensity o"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           128,
           ". This can help in makingfeatures more distinguishable, which is beneficial for detecting eelgrass w"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           129,
           ". Note that this is not your final prediction.Then you will be asked again if you want to use a tool"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           130,
           "44  Preprint. Under review.  [IncreaseContrastTool]User:Image contrast has been increased by 50%.You"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           131,
           "Assistant:[SharpenTool]{yes:60,no:40}I am using the SharpenTool to enhance the edges and details of "
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           132,
           "Assistant:[PredictEelgrassWastingDiseaseTool]{yes:65,no:35}I am using the PredictEelgrassWastingDise"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           133,
           ". Then, explain why you are picking this tool orfinishing, and how it relates to your current assess"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           134,
           "46  Preprint. Under review.  You have 0 rounds left. You may now choose another tool or indicate you"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           135,
           "User:Based on your analysis and the tools used, please provide your final classification regarding t"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           136,
           ".3.SharpenTool: Further enhanced the edges and details, confirming the presence of the dark streak b"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           137,
           "47  Preprint. Under review.  The combination of visual inspection and tool results suggests that whi"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           138,
           "True Class: 1, LMM Prediction: 0 (Probability: 0.40), Supervised Prediction: 0 (Probability: 7.13)Sy"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           139,
           ".Use your knowledge and the tools provided to make an accurate classification.User:I will start by f"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           140,
           "Assistant:This image shows eelgrass blades with dark, necrotic lesions, which are indicative of eelg"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           141,
           "50  Preprint. Under review."
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           142,
           "Based on the presence of these dark, necrotic lesions and the pattern of spotting and streaking, I c"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           143,
           ". I reccomend you usethis tool. - AdjustBrightnessTool: Tool Name: 'AdjustBrightnessTool' Descriptio"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           144,
           ". Feel free to usemore tools if you need to increase your confidence.If you want to use a tool, expl"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           145,
           ".[IncreaseContrastTool]User:Image contrast has been increased by 50%.You have 3 rounds left. You may"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           146,
           "51  Preprint. Under review.  Assistant:[SharpenTool]{Yes:75,No:25}I am using the SharpenTool to enha"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           147,
           "Assistant:[PredictEelgrassWastingDiseaseTool]{Yes:80,No:20}I am using the PredictEelgrassWastingDise"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           148,
           ". Then, explain why you are picking this tool orfinishing, and how it relates to your current assess"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           149,
           "53  Preprint. Under review."
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           150,
           "User:Based on your analysis and the tools used, please provide your final classification regarding t"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           151,
           ". Thisreinforced the initial suspicion of eelgrass wasting disease.SharpenTool: This tool further en"
          ],
          [
           "../data/arxiv/pdfs/2410.21480v1.pdf",
           152,
           "54"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           0,
           "4 2 0 2  v o N 8  ] L C . s c [  1 v 3 2 4 5 0 . 1 1 4 2 : v i X r a  Proceedings of Machine Learnin"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           1,
           "milhaud1201@gmail.com  Editor: Jeongwoo Lee, Kwangsuk Park, Jihyeon Park  Abstract Generating accura"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           2,
           "1. Introduction  Visual aids are critical components of mathematics education, enabling students to "
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           3,
           "© 2024 J. Lee, K. Park & J. Park.  J. Lee, K. Park, J. Park  creating problems requiring precise vis"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           4,
           "Generative AI, particularly LLMs, holds significant promise for addressing challenges in mathematics"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           5,
           "In this paper, we introduce a multi-agent framework for generating and visualizing math problems, le"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           6,
           "This framework introduces a novel workflow where LLMs generate both mathemati- cal problems and thei"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           7,
           "By leveraging this integrated approach, we aim to enhance the overall quality of math education, mak"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           8,
           "In this study, we utilized Claude 3.5 Sonnet as the core model for our framework. A critical aspect "
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           9,
           "3  J. Lee, K. Park, J. Park  Figure 1: Overview of the multi-agent system for generating math proble"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           10,
           "4  VISTA: Visual Integrated System for Tailored Automation in Math Problem Generation Using LLM  cul"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           11,
           "Function Validator. The Function Validator plays a complementary role by analyzing and verifying geo"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           12,
           "Code Executor and Summarizer. The Code Executor is responsible for executing the code generated by t"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           13,
           "3. Evaluation  3.1. Datasets  The study focuses on two key areas of mathematics: Geometry and Functi"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           14,
           "Within our framework, Claude 3.5 Sonnet was used to generate complex figures and math- ematical func"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           15,
           ". For instance, subtle variations in the size, position, or proportions of geometric figures can dis"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           16,
           "2. https://huggingface.co/spaces/opencompass/open_vlm_leaderboard  6  VISTA: Visual Integrated Syste"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           17,
           "To address the shortcomings of conventional visual similarity metrics, we developed an enhanced eval"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           18,
           "4. Results  4.1. Geometry  In Figure 2, the performance of VISTA and the baseline system is evaluate"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           19,
           "In the Length and Applied subtypes, the pattern remains consistent, with VISTA demon- strating stron"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           20,
           "4.2. Function  In Figure 3, the evaluation of function-related problems, which includes the subtypes"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           21,
           "While image similarity remains low across all subtypes, VISTA’s consistently high scores in the text"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           22,
           "10  VISTA: Visual Integrated System for Tailored Automation in Math Problem Generation Using LLM  Fi"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           23,
           "5. Discussion  This study has demonstrated the potential of LLMs as powerful tools for generating vi"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           24,
           "11  J. Lee, K. Park, J. Park  stance, when certain critical details were omitted from a problem, the"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           25,
           "While our system has shown considerable potential, it is not yet perfect, and several avenues for im"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           26,
           "Journal of Research & Practice, 25(5-6):355–360, 2001.  Arya Bulusu, Brandon Man, Ashish Jagmohan, A"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           27,
           "12  VISTA: Visual Integrated System for Tailored Automation in Math Problem Generation Using LLM  Qi"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           28,
           "Property Expression Coordinate Applied  71 32 16 40  Total  159  A.2. Agent Prompts  Figure 7: Promp"
          ],
          [
           "../data/arxiv/pdfs/2411.05423v1.pdf",
           29,
           "Figure 18: Math problem generation samples using VISTA  20  VISTA: Visual Integrated System for Tail"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           0,
           "5 2 0 2  b e F 4  ] L C . s c [  1 v 9 5 3 4 0 . 2 0 5 2 : v i X r a  Exploring Spatial Language Gro"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           1,
           "Abstract  Spatial Reasoning is an important component of human cognition and is an area in which the"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           2,
           "ther to analyze the comparative performance of these models for spatial categories that represent di"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           3,
           "We test two popular VLMs - LLaVA [Liu et al., 2024] and Grounding DINO [Liu et al., 2023b]. We also "
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           4,
           "1 Introduction Vision-Language model (VLM) research has boomed in the recent past, owing to the enha"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           5,
           "Some of our important findings are as follows:  (1) Spatial relations contribute to more accurate gr"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           6,
           "(c) The brown table that is to the left of the black cell phone  (d) The sandy shore that is near th"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           7,
           "[R¨osch and Libovick`y, 2023; Subramanian et al., 2022] focus solely on spatial analysis of VLMs. [W"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           8,
           "., 2021] focus on probing pre- trained LLMs or VLMs with text-only questions for spatial analysis. H"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           9,
           "Other closely aligned work includes Embodied Spatial Analysis which focuses on the effects of differ"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           10,
           "3 Dataset Table 1 shows the key characteristics of some popular REC datasets. We chose CopsRef over "
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           11,
           "Category  Number  Spatial Relations  Absolute Adjacency Directional Orientation Projective  Proximit"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           12,
           "Table 2: Category-wise relation split and number of referring expressions in the CopsRef test set wi"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           13,
           "To answer these questions, we explain our research  None One Two-chained Two-and Two-or Three  0 1 2"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           14,
           "[Zheng et al., 2020] is an REC task-specific MGA-Net. model whose compositional learning architectur"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           15,
           "limit our training to ten epochs due to computational con- straints. [Liu et al., 2023b] It is an op"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           16,
           ". Both prompts have a similar structure, but the second prompt is longer. [Minderer et al., 2022] is"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           17,
           ". This is unlike MGA-Net, which has dedicated trans- former architecture modules for visual, linguis"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           18,
           "4.2 Experimental Setting and Evaluation We create the following dataset test splits for evaluation a"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           19,
           "Visual Complexity Split To observe the effect of visual complexity on model perfor- mance, we split "
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           20,
           "5 Results  Model  Accuracy (%)  MGA-Net (Partial CNN) Grounding DINO LLaVA - Short Prompt MGA-Net (F"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           21,
           "Absolute Adjacency Directional Projective Proximity Topological Unallocated None Two-chained Two-and"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           22,
           "4 1 12 6 3 2 7 5 9 8 10 11  Table 5: Category-wise accuracy and ranking  Evaluation Metrics. We eval"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           23,
           "with a single spatial relation. Among those, all 3 models per- form well for the Topological and Abs"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           24,
           "Since we trained/tested each model for three runs, we re- port the average accuracy of the three run"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           25,
           "Among spatial categories of MGA-Net and VLMs, the ma- jor difference occurs with the Proximity and P"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           26,
           "42.89 ± 0.17 40.33 ± 0.01 31.05 ± 0.06 30.19 ± 0.26  67.8 ± 0 60.5 ± 0 52.39 ± 0 55 ± 0  Table 7: Re"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           27,
           "Model  Accuracy Single(%) Accuracy Multi(%)  64.91 ± 0.15 72.54 ± 0.01 37.69 ± 0.01  MGA-Net G-DINO "
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           28,
           "Now, to answer RQ3, we observe in Table 5 that among the seven categories of single spatial relation"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           29,
           "36 24 23 17 20  Table 9: Results for negations in expressions.  We obtained 36 expressions with 1 ‘n"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           30,
           "Another interesting observation was for the outputs of MGA-Net and LLaVA models when they came close"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           31,
           "6.2 Projective and Proximity Relations Figure 1c shows an example of Projective relations (‘to the l"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           32,
           "the model might pay attention to only one spatial clause. Consequently, it returns an object satisfy"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           33,
           "7 Conclusion  Spatial reasoning and understanding is an area in which the latest VLMs have shown sig"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           34,
           "8 Future Directions  We observed that MGA-Net handles expressions with vary- ing spatial complexity "
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           35,
           "Another issue to address is the VLMs’ inability to com- prehend negations. MGA-Net’s improved perfor"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           36,
           "[Cohn and Hernandez-Orallo, 2023] Anthony G Cohn and Jose Hernandez-Orallo. language model evaluatio"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           37,
           "[Gokhale et al., 2022] Tejas Gokhale, Hamid Palangi, Be- smira Nushi, Vibhav Vineet, Eric Horvitz, E"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           38,
           "[Kamali et al., 2024] Danial Kamali, Elham J Barezi, and Parisa Kordjamshidi. NeSyCoCo: A Neuro-Symb"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           39,
           "[Krishna et al., 2017] Ranjay Krishna, Yuke Zhu, Oliver Groth, Joshua Kravitz, Stephanie Chen, Yanni"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           40,
           "[Li et al., 2023] Bohao Li, Rui Wang, Guangzhi Wang, Yuy- ing Ge, Yixiao Ge, and Ying Shan. SEED-Ben"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           41,
           "[Liu et al., 2023b] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan L"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           42,
           "language.  [Minderer et al., 2022] Matthias Minderer, Alexey Grit- senko, Austin Stone, Maxim Neuman"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           43,
           "Vicuna:  an Open-Source Large https://blogs.novita.ai/  Language Model for Chatbots. vicuna-an-open-"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           44,
           "[Ren et al., 2016] Shaoqing Ren, Kaiming He, Ross Gir- shick, and Jian Sun. Faster R-CNN: Towards Re"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           45,
           "[Subramanian et al., 2022] Sanjay Subramanian, William Merrill, Trevor Darrell, Matt Gardner, Sameer"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           46,
           "[Wang et al., 2024] Jiayu Wang, Yifei Ming, Zhenmei Shi, Vibhav Vineet, Xin Wang, Yixuan Li, and Nee"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           47,
           "[Zheng et al., 2020] Yihan Zheng, Zhiquan Wen, Mingkui Tan, Runhao Zeng, Qi Chen, Yaowei Wang, and Q"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           48,
           "4. Orientation: Consists of relations which describe the orientation of an object w.r.t another obje"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           49,
           "8. Unallocated: Consists of relations that cannot be allo- cated to any of the above categories.  A."
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           50,
           "3. Provide the bounding box coordinates for: ”Refexp”  In both prompts, the ‘bounding box list’ cons"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           51,
           "{‘1’: [0.16, 0.55], ‘2’: [0.32, 0.47], ‘3’: [0.55, 0.6], ‘4’: [0.21, 0.06],...  0.9, 0.53, 0.93, 0.5"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           52,
           "output  format:  Query format: <image>Bounding Boxes:bounding box list; Expression: Refexp; Correct "
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           53,
           "A.3 Related Works Data Table 11 provides the evaluation task, list of evaluated mod- els, the benchm"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           54,
           "New: MM- Bench  Image-caption agreement Multi-Choice VQA  New: SEED- Bench  Multi-Choice VQA  New: M"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           55,
           "[R¨osch and  Libovick`y, 2023]  [Kamath et  al., 2023]  [Gokhale et al., 2022]  [Lewis et al., 2022]"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           56,
           "Template annotation  Template annotation  Human and template an- notation Human-annotation with cons"
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           57,
           "[Yu et al.,  2023]  [R¨osch and  Libovick`y, 2023]  [Kamath et  al., 2023]  [Gokhale et  Real-world "
          ],
          [
           "../data/arxiv/pdfs/2502.04359v1.pdf",
           58,
           "CopsRef Dataset  Captions - Simple  Referring expres- sions - Complex  8 + their superla- tives 51  "
          ]
         ],
         "hovertemplate": "x=%{x}<br>y=%{y}<br>document=%{customdata[0]}<br>chunk=%{customdata[1]}<br>preview=%{customdata[2]}<br>color_id=%{marker.color}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": {
           "bdata": "AwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwc=",
           "dtype": "i1"
          },
          "coloraxis": "coloraxis",
          "opacity": 0.6,
          "size": 6,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "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",
          "dtype": "f8"
         },
         "xaxis": "x",
         "y": {
          "bdata": "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",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "color_id"
          }
         },
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "legend": {
         "tracegroupgap": 0
        },
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Interactive PCA Projection of Chunks"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "x"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "y"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    color=\"color_id\",\n",
    "    hover_data=[\"document\", \"chunk\", \"preview\"],\n",
    "    title=\"Interactive PCA Projection of Chunks\"\n",
    ")\n",
    "fig.update_traces(marker=dict(size=6, opacity=0.6))\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4cee14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ScientificPapersRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
